[
{
	"permalink": "https://findstar.pe.kr/tags/dependency-management/",
	"title": "Dependency Management",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/ioc/",
	"title": "Ioc",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/posts/",
	"title": "Posts",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/python-framework/",
	"title": "Python Framework",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/",
	"title": "Soo Story",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2025/02/02/how-to-manage-dependencies-while-using-python-frameworks/",
	"title": "파이썬 프레임워크를 사용하면서 의존성을 관리하는 방법",
	"tags": ["ioc", "python framework", "dependency management"],
	"description": "",
	"type": "post",
	"contents": "의문점 - 파이썬에서는 왜 IoC가 일반적이지 않을까? 나의 경우 경력의 많은 시간동안 Java와 Spring 을 주력 언어로 사용해왔기 때문에, IoC(Inversion of Control)와 DI(Dependency Injection)를 기반으로 하는 애플리케이션 설계 방식이 익숙했다. 스프링에서는 프레임워크레벨에서 애플리케이션 작성에 필요한 전반적인 의존성 관리를 손쉽게 등록할 수 있고, 제일 처음 배우는 것들이 이러한 의존성을 등록하는 방법인 만큼, IoC 와 DI는 가장 익숙한 개념이었다.\n애플리케이션의 코드가 늘어나면서 필요한 의존성을 체계적으로 관리하고 싶어졌는데, 이런 필요에 비해서 python 프레임워크 레벨에서는 IoC 컨테이너를 제공해주지 않아 불편함을 느꼈었다.\n그러던 중 스택오버플로우에서 \u0026ldquo;왜 파이썬에서는 IoC가 일반적이지 않느냐\u0026rdquo;는 오래된 글을 읽게되었고, 이를 통해서 언어의 차이를 좀 더 명확하게 이해할 수 있게 되어 의존성 관리 방식을 개선해나가게 되었다.\nFastAPI, Litestar 프레임워크들의 IoC 컨테이너 제작년 부터 python 기반으로 애플리케이션을 개발하면서 FastAPI, Litestar 와 같은 파이썬 프레임워크를 사용하고 있다. FastAPI는 빠르게 시작하기 쉬워서, Litestar는 빠르고 현대적인 웹 프레임워크로, 비동기 처리와 간결한 API 설계에 강점을 가지고 있다고 느끼고 있어서 적극적으로 사용하고 있다.\n하지만 이 두 개의 프레임워크에서는 IoC 컨테이너와 같은 기능을 내장하고 있지 않기 때문에, 애플리케이션 전반의 의존성 관리를 위해서는 개발자가 스스로 해결책을 찾아야 하는 상황이 발생한다. 이 프레임워크들이 내가 이전에 경험했던 스프링과 같은 \u0026lsquo;애플리케이션의 모든 영역을 커버하는 컨셉\u0026rsquo;이 아닌 일종의 Micro 프레임워크라서 차이가 있음을 이해하고는 있었지만, 그럼에도 IoC 컨테이너와 같은 중앙 집중식 의존성 관리 기능이 제공되지 않아 불편함을 느꼈었다.\n처음에는 조직원들과 함께 프레임워크 자체적인 의존성 관리방식만을 사용하거나 Java/Spring 스타일의 중앙 집중식 IoC 컨테이너를 직접 구현해서 사용해보고 이를 프레임워크에 접목시켜보고자 노력했었다. 하지만 결국 파이썬의 설계 철학을 고려할 때 이런 접근 방법은 오히려 복잡하다는 의견을 확인했다.\n# 초기에는 애플리케이션 bootstrapping 시점에 도메인별 ServiceProvider 를 사용하여 dependency 를 등록하고 사용했었었다. class MyServiceProvider(ServiceProvider): # 의존성을 정의하는 영역입니다. def _dependencies(self) -\u0026gt; dict[str, Provide | AnyCallable] : dependencies: dict[str, Provide | AnyCallable] = { \u0026#34;my_domain_store_output\u0026#34;: MyProvide( self.provide_my_domain_store_output_port ), \u0026#34;my_domain_input\u0026#34;: MyProvide(self.provide_my_domain_input_port), \u0026#34;my_domain_query_in\u0026#34;: MyProvide(self.provide_my_domain_query_input_port), \u0026#34;_my_domain_service\u0026#34;: MyProvide(self._provide_my_domain_service), } return dependencies @staticmethod def provide_my_domain_input_port( _my_domain_service: MyDomainService, ) -\u0026gt; MyDomainInputPort: return _my_domain_service @staticmethod def provide_my_domain_store_output_port( db_engine: AsyncEngine, ) -\u0026gt; MyDomainStoreOutputPort: return MyDomainStoreOutputAdaptor(engine=async_engine) ... 파이썬의 유연성과 설계 철학의 차이 먼저 파이썬은 동적 타이핑과 간결한 문법 덕분에, 객체 생성 및 의존성 관리가 매우 자유롭다. Java 에서는 implement 키워드를 필수로 사용하는 방식이지만, Python 에서는 Protocol을 사용할 수도 있다.\n또 파이썬에서는 IoC 컨테이너를 따로 두기 보다 직접 의존성을 주입하는 방식을 선호하고 있고 이 방식이 오히려 가독성을 높이게끔 유도하고 있다고 느껴졌다.\n이러한 차이가 파이썬답게 “파이써닉하다”라는 표현이 딱 맞는, 단순하고 직관적인 접근 방식으로 이해되었다.\nIoC 대신 직접 주입과 모듈 사용 그럼 의존성 관리를 어떻게 하는 것이 좋을까? 이부분은 팀원들과 함께 여러차례 아키텍처 논의를 하면서 우리만의 자체적인 대안을 마련하게 되었다.\n먼저 Java/Spring 에서는 정적 타이핑과 엄격한 구조로 인해 IoC 컨테이너를 통한 의존성 관리가 필수적이었지만, 파이썬에서는 그런 접근 보다는 직접 의존성을 주입하거나 모듈로써 import 시점에 생성된 객체를 직접 사용하는 방식을 선호했다. 그래서 우리도 파이썬 스타일을 유지하는 방향으로 접근하였다.\n생각해보면, 스프링에서의 대부분의 의존성은 싱글톤(singleton)으로 정의되는데(물론 아닌 경우도 있지만), 애플리케이션에서 필요로 하는 의존성이 싱글톤뿐이라면 애플리케이션 bootstrapping 시점에 이를 미리 생성해두고 연결해서 사용하는 방식도 충분히 합리적이라고 생각되었다.\njava spring 1. 프레임워크 booting 2. component scan 을 통한 의존성 등록 3. 의존 객체 생성 후 주입 python 1. 필요한 의존성 미리 생성 2. 프레임워크 booting (w/의존객체 주입) 따라서 이렇게 되면 프레임워크가 구동되는 라이프사이클이 애플리케이션 전체를 커버하는 것이 아니라 특정 영역(restapi backend)만 담당하는 방식으로 동작하기 때문에 의존성에 대한 관리는 개발자가 직접 처리하면 된다.\n그리고 아예 별도 모듈로 정의해서 import 하는 순간 모듈 자체에서 제공하는 의존성을 바로 가져다가 사용하도록 코드를 구성할 수도 있다.\n현재 사용하는 방식 Litestar와 같이 프레임워크 체제 하에서 의존성 관리를 아예 맡겨버리는 방식도 사용해보았지만, 한계점들을 마주한 뒤로 프레임워크와는 별개로 직접 의존 성을 관리하고 이를 직접 주입하는 방식으로 방식을 변경하였다.\n대신 Litestar와 같은 프레임워크가 제공하는 의존성 등록의 연결고리를 두어서 프레임워크의 라이프사이클에서도 의존성이 필요한 경우 손쉽게 사용할 수 있도록 일종의 \u0026ldquo;브릿지\u0026quot;를 제공하는 방식으로 변경하였다.\n# 1. 자체적으로 의존 객체 생성 관리 my_domain_store_output: MyDomainStoreOutputPort = MyDomainStoreOutputAdaptor( async_session_factory=async_session_factory ) my_domain_input: MyDomainInputPort = MyDomainService( store_output=my_domain_store_output ) ... # 2. 프레임워크와 연결고리 생성 def litestar_provide(dependencies: Depedencies) -\u0026gt; dict[str, Provide]: def bind(val) -\u0026gt; Callable[[], Any]: return lambda: val res: dict[str, Provide] = { key: Provide(bind(val), use_cache=True, sync_to_thread=True) for key, val in dependencies.items() } return res ... # 3. 필요한 부분에서는 자유롭게 사용 class MyDomainController(Controller): @get(\u0026#34;/v1/my-domain/\u0026#34;, tags=[\u0026#34;My Domain\u0026#34;], summary=\u0026#34;Item 목록 반환\u0026#34;, ) async def list_my_domain_items( self, my_domain_input: MyDomainInputPort, ) -\u0026gt; list[ListMyDomainResponse]: item_list = await my_domain_input.list_items() return [ListMyDomainResponse.from_domain(item) for item in item_list] ... 이렇게 함으로써 파이썬 언어의 방식에 좀 더 익숙해지고, 나아가 프레임워크의 유용성은 살리면서도 직접 의존성을 관리하는 방식의 간결함도 가져올 수 있었다. 물론 이 과정에서 어려움이 없었던 것은 아니다. (아주 많은 팀원들과의 아키텍처 설계 논의들 \u0026hellip;)\n결국 자체적인 의존성 관리를 직접 수행하고 이를 프레임워크와 연결하는 방식을 최종 선택해서 사용하고 있다.\n결론: 언어의 철학을 이해하고 접근하자. 결국, 각 언어가 추구하는 철학과 설계 방향을 이해하는 것이 중요하다는 것을 다시금 깨달았다. Java에서는 정적 타입과 대규모 애플리케이션을 위한 복잡한 IoC 컨테이너가 필요하지만, 파이썬은 그 자체의 유연성과 단순함, 그리고 파이써닉한 철학을 기반으로 명시적이고 간결한 의존성 관리를 선호한다.\nLitestar과 같은 파이썬 프레임워크를 사용할 때는, 굳이 Java/Srping 스타일의 중앙 집중식 IoC 컨테이너를 구현하기보다는 파이썬이 제공하는 직관적인 패턴과 기능들을 최대한 활용하는 것이 현명한 방법이다. 이를 통해 불필요한 복잡도를 피하고, 보다 파이써닉한 방식으로 문제를 해결할 수 있다.\n여러차례 고민하면서 논의를 거듭해서 현재의 방식을 선택했지만, 문제가 아예 없는 것은 아니다. 앞으로 또 여러차례 고민하고 논의하면서 우리만의 좋은 설계에 대한 고민을 이어나갈것 같다.\n어찌보면, 틀이 정해져 있지 않아서 어떻게 해야할지 모르겠다는 느낌도 들지만, 오히려 그 안에서 팀원들과 논의하면서 더 단단해지는 방향으로 나아가고 있는게 아닐까?\n참고자료 https://stackoverflow.com/questions/2461702/why-is-ioc-di-not-common-in-python https://drive.google.com/file/d/1u5nF6vmCgZGE3rnoGQlGYVks9QFJoIDF/view "
},
{
	"permalink": "https://findstar.pe.kr/tags/2024/",
	"title": "2024",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2025/01/13/2024-year-review/",
	"title": "2024 회고 - 건강, 기록, 행동",
	"tags": ["year review", "2024", "3 keywords"],
	"description": "",
	"type": "post",
	"contents": "2024년 회고 2024 한 해가 저물었다. 유달리 2024년 한 해는 나 개인에게 있어서 많은 변화와 도전이 있었던 한 해였다. 시작과 함께 건강 이슈로 병원에 다녀야만 했고, 그 와 동시에 회사에서는 조직의 리더가 되었다. 눈앞에 있는 일을 해내는것 자체가 익숙하지 않은 일들의 연속이었고, 그 때문인지 어떻게 지나갔는지도 모를만큼 시간이 빠르게 흘러갔다. (이건 왠지 나이가 들수록 시간흐름이 더 빨리지는것 같지만.)\n기억이 휘발되기 전에 2024년의 회고를 진행해보았다.\n집중하자고 마음먹었던 세 가지 키워드들 언제부턴가 한 해가 시작하면서 매년 3가지 키워드를 꼽아보고 있다. 2024년에는 건강, 기록, 행동 세 가지를 꼽았었다. 각 키워드에 대해서 소감을 정리해보았다.\n건강 매해 빠지지 않는 키워드가 있다면 건강인데 3년전부터 목 디스크를 비롯해서 병원 다닐일이 많아졌다. 그래서 이제는 살기 위해서 운동해야한다. (생존운동) 연 초에 목디스크 악화로 다시 병원에 가야되면서 더 중요하게 신경쓰게되었다.\n한 해 동안 헬스와 PT를 받으면서 기초 체력과 유연성을 개선하려고 노력했고, 어느 순간 꾸준한 스트레칭이 효과를 발휘하는 것을 체감할 수 있었다. (처음에는 허리를 숙이고 양말 신는 자세가 잘 안되었을 정도였다.)\n하루 잠자고 밥먹는 시간을 제외하면 거의 대다수의 시간을 컴퓨터 책상 앞에 있다보니 바른자세도 신경을 쓰게되었는데, 얼마전 회사 동료분이 앉은 자세가 좋다는 칭찬(?!)을 해주셔서 감동이었다. 사실.. 안아프려고 그런거예요. 살려고 ㅠㅠ\n연말에 춥고 바쁘다고 운동 잘 안했는데 반성하고 다시 힘 내보아야겠다.\n기록 두 번째 키워드는 기록 이다. 매해 잘 해봐야지 하고 마음먹는 키워드 중 하나이다. 그만큼 실천이 잘 안되던 영역인데 2024년에는 \u0026ldquo;기록 잘 해놓자\u0026rdquo; 라고 굳게 다짐하고 새로운 접근을 해보았다.\n기존에는 디지털 방식으로 기록하는 것에 집중을 했었다. 매일 마크다운으로 daily log 를 남긴다던지, GTD 방식으로 할일 관리를 하면서 결과를 정리한다던지 하는 방식이었다. 그러다가 이번에는 아날로그 방식으로 접근을 시도했는데 효과가 너무 좋았다. 사실 아내가 매번 다이어리를 꾸준히 작성하는 걸 보고 나도 한번 해볼까 하는 마음에 시작한 것인데, 디지털 방식보다 아날로그가 더 좋은 효과가 있어서 놀라웠다. 아무래도 내가 하는 일이 개발이다보니, 디지털 방식은 업무나 다른 일들과 섞이기 쉬워서 아예 분리하는게 효과가 있었을지도 모르겠다.\n아무튼 다이어리를 통해 일상과 중요한 순간을 기록하며 차곡차곡 페이지를 채워났고, 이 때문인지 이런 기록하고 정리하는 부분들이 업무에도 큰 도움을 주었다. 그날그날의 일을 기록하면서 자연스럽게 다음날의 할일들과 우선순위를 생각하게 되었는데 이 덕분에 매일 아침 업무 시작을 잘 할 수 있게 되었다. 다시한번 기록의 중요성을 깨달았다. 다만 나에게는 \u0026lsquo;디지털 보다 아날로그\u0026rsquo;가 적합하다는 걸 알게되었다. (내가 아날로그 친화적 인간이라니..)\n행동 세 번째 키워드는 행동이다. 어떤 일을 진행하는데 있어서 잘 하고 싶은 마음에 완벽을 추구하다가 시작도 못하고 끝나버린 일들이 많아서, 너무 잘 하려고 하기 보다 발빠르게 움직이자는 의미로 꼽아보았었다.\n그런의미에서 시도해보고 싶었던 \u0026ldquo;영어공부\u0026rdquo;, \u0026ldquo;그림그리기\u0026rdquo; 모두 제대로 시작도 못하고 한 해가 끝나버렸다. 이부분은 다른 방식으로 다시 생각을 해봐야겠다.\n각각의 키워드에 점수를 매기자면 10점 만점에 건강 8점 , 기록 8점, 행동 3점을 줄 수 있다. 그래서 2025년 한해에는 점수가 낮은 키워드는 빼고, 아예 하나의 영역만 파보기로 했다. (그것은 바로 영어공부..)\n리더가 되었다. 새 해가 시작되면서 회사에서의 역할도 변화가 있었다. 공식적으로 리더 직함을 부여받았다. AI 라는 새로운 분야에 구성된지 얼마 되지 않은 조직을 맡게 되었는데, 이 때문에 2024년 한 해가 무척이나 바쁜한해였다.\n기존에는 시니어 엔지니어로써 역할에 집중해왔고 이 부분은 어느정도 익숙한 영역이었는데, 리더 라는 역할은 내가 익숙하지 않은 전혀 새로운 고민들이 이어져서 정신없이 눈앞의 일들로 허덕이는 한해를 보냈다. 업무보다는 사람을, 기술보다는 관계를 더 중요하게 생각했었야 했고 내가 생각하는 사고의 프레임이 완전히 새롭게 정의되어야 했었는데 처음에는 그러지 못해서 많은 어려움이 있었다.\n처음의 생각과 한 해가 지나는 시점에서 생각이 많이 달라진 부분도 있었다. 대표적으로 처음에는 \u0026ldquo;어떻게 하면 조직원들의 만족도를 높이는 리더가 될 수 있을까?\u0026ldquo;라는 생각을 했었는데, 지금은 이게 바뀌어서 \u0026ldquo;어떻게 하면 조직의 성과를 내어 만족도를 높이는 리더가 될 수 있을까?\u0026ldquo;로 변경되었다. 결국 조직의 리더는 성과를 통해서 조직의 역랑과 개인의 성장을 뒷받침 해야한다는 생각이 들었다.\n얼마전 한 해를 마무리 하면서 팀원들로부터 받은 감사인사를 받게 되었는데, 부족한 리더임에도 큰 힘이 되었었다. 새로운 해에도 더 나은 리더가 되고자 다짐하게 되었다. 리더쉽 공부 많이 해야겠다.\n마무리 하며 빠르게 변화하는 세상 속에서 나 자신을 유지하는 것 조차도 점점 더 많은 에너지를 필요로 한다는 것을 느낀다. 경력이 쌓이면서 자연스럽게 역할의 변화가 기대되고, 여러 경험의 폭이 넓어지면서 내가 보는 시야의 폭이 넓어짐을 느낀다. 나 자신 스스로를 돌아보고 내가 더 나은 사람이 되지 않으면 내 삶을 온전히 살아가는 것도 어렵지 않을까 하는 생각이 드는 요즘이다.\n2024년 한 해 동안 새로운 역할을 수행하게 되면서 기술적으로 새로운 것을 배우고 적용하는 데 한 발짝 뒤처지는 듯한 느낌이 들기도 했지만, 이를 통해 사람과의 관계와 나 자신을 돌아보는 것에 대한 중요성을 다시 한번 깨닫게 되었다.\n앞으로도 건강관리 잘 하고, 기록하는 습관을 잘 유지하고, 나 자신을 잘 이해하고 돌아보는 여유를 가지기를 바라면서 2025년도 잘 맞이했으면 좋겠다.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/3-keywords/",
	"title": "3 Keywords",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/year-review/",
	"title": "Year Review",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/k8s/",
	"title": "K8s",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/node-drain/",
	"title": "Node Drain",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2024/12/22/pod-disruption-budget-on-k8s-node-drain/",
	"title": "Node Drain 작업시 주의사항",
	"tags": ["k8s", "node drain", "PodDisruptionBudget"],
	"description": "",
	"type": "post",
	"contents": "개요 k8s 클러스터를 운영하다보면, 종종 Node Drain 작업을 수행해야하는 경우가 발생한다. 나의 경우에는 GPU 기반의 AI Workload를 운영하는 플랫폼을 개발하고 있는데 특정 Node 의 GPU에 장애가 발생하는 경우에 이 Node 에서 동작중인 pod 을 다른 노드로 옮긴 뒤에 노드를 재부팅하거나 물리적으로 교체하는 작업을 수행했었다. 이 Node Drain 작업시에 종종 Pod 의 위치에 따라 Service 가 불가능한 상황이 발생하여 이를 제어하기 위해서 PodDisruptionBudget을 알아보았다.\nNode Drain? 노드 드레인이란 특정 노드에서 동작중인 pod를 다른 노드로 이동시키고, 해당 노드를 스케줄링에서 제외 시켜 새로운 Pod 이 배포되지 못하도록 하는 작업이다. 일반적으로 노드의 메인터넌스 작업이 필요한 경우에 수행하는 명령어라고 할 수 있다. Drain 이란 단어가 \u0026lsquo;물을 빼다\u0026rsquo; 라는 의미가 있는데 찰랑거리는 욕조에 마개를 열어 물을 빼는 상황을 생각해보면 이해가 쉽다. 주로 노드의 OS 업데이트, 하드웨어 장애시에 많이 사용하게 된다.\n나의 경우에는 AI 플랫폼을 운영하기 위한 GPU worker를 사용하고 있어서, 특정 GPU Ecc 에러가 감지되어 재부팅 해야하거나, 아예 완전히 GPU를 교체해야하는 하드웨어 장애가 발생하는 경우 많이 사용하게 되었다.\n명령어는 다음과 같이 노드의 이름을 입력하면 된다.\nkubectl drain \u0026lt;NODE_NAME\u0026gt; 드레인 명령어를 입력하면 해당 노드에서 수행중인 pod 이 삭제되어 다른 노드에 다시 스케줄링 된다. 이렇게 노드가 삭제되면 eviction(축출) 되었다고 표현한다 다만 drain 명령어를 실행해도 daemonset 과 같은 pod 는 삭제되지 않는다. 만약 daemonset pod 도 함께 삭제해야한다면 다음과 같이 명령어를 입력하면 된다.\nkubectl drain \u0026lt;NODE_NAME\u0026gt; --ignore-daemonsets=false Node Drain 시에 Service 가 단절되는 상황 pod 이 스케줄링 되어 있는 상황에 따라서 Service 가 단절되는 상황이 발생한다. 다음의 경우를 생각해보자.\ndrain 작업을 수행하려는 node 에 pod 가 몰려 있는 경우 예를 들어 drain 작업을 수행하려는 node 에 replica size 가 2인 pod 가 모두 스케줄링 되어 있는 경우 drain 작업을 수행하면 서비스가 단절된다. K8S Node Drain 왜 그러한지는 pod 의 eviction 과정을 살펴보면 이해할 수 있다.\nPod Eviction 드레인 명령어가 입력되면 먼저 노드는 스케줄링에서 제외어 새로운 pod 이 배치되지 못하도록 한다. 그 다음 pod 가 eviction 된다. pod 는 terminating 상태가 된다. 참고로 eviction 은 pod의 TerminationGracePeriodSeconds 설정에 따라 종료과정을 거친다. preStop 이 설정되어 있다면 이 hook 을 먼저 실행하고 hook 완료시 TERM 시그널을 main container 에 전달한다. TerminationGracePeriodSeconds 안에 완료되지 못하면 SIGKILL 시그널이 main container 에 전달된다. pod 가 terminating 상태임을 감지한 클러스터는 replica 를 유지하기 위해서 신규 pod 를 생성하는데 이미 drain 대상이 된 노드는 스케줄링 제외 되어 있기 때문에 다른 노드에 배치된다. PodDisruptionBudget 를 사용하여 MinAvailable 조정하기 노드 드레인 작업시 서비스를 유지하기 위해 최소한의 pod 수를 유지해야한다. replica size 가 2라면 1 개의 pod 씩 옮겨야 서비스가 유지된다. 이럴 때는 PodDisruptionBudget 의 MinAvailable 을 조정하면 된다.\napiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: my-pdb spec: minAvailable: 1 selector: matchLabels: app: my-gpu-app 최소한 하나의 replica 는 유지하도록 설정하였다.\n이렇게 되면 최소한 하나의 pod replica 는 유지되기 때문에 서비스가 단절되는 상황을 방지할 수 있다. replica 2 의 pod 이라면 1 개의 pod 이 유지되고 나머지 pod 이 다른 노드에 스케줄링되기 때문이다.\n이와 반대로 MaxUnavailable 을 지정할 수도 있다.\napiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: my-pdb spec: maxUnavailable: 1 selector: matchLabels: app: my-gpu-app 이 경우에는 하나씩 다른 노드로 새로운 pod 을 스케줄링 하게 된다. 다만 replica size 가 크다면 하나씩 이동시키는 건 시간이 오래걸리기 때문에 MaxUnavailable 값은 25%와 같이 퍼센트 값을 지정할 수도 있다.\n만약 MaxUnavailable 이 0 이라면? pod 의 eviction 을 생각할 때 MaxUnavailable 이 0 이라면 eviction 이 차단된다. 따라서 노드 드레인 작업이 완료되지 않는다. 따라서 해당 노드에 위치한 pod 를 옮기기 위해서 다른 방법을 사용해야한다.\n노드에서 pod 를 제거하는 다른 방법 PodDisruptionBudget을 생각하지 않고 특정 노드에 스케줄링되어 동작중인 pod 를 다른 노드로 옮기는 다른 방법은 해당 노드를 먼저 스케줄링에서 제외하고 동작중인 pod 가 포함된 deployment 를 rollout restart 하면 된다. 이 방법도 많이 사용한다. 다만 이렇게 하면 deployment 의 replica count 가 많은 경우 다른 노드에 스케줄링된 모든 pod 이 새롭게 스케줄링 되어야 하므로 node drain 보다 시간이 오래걸리고, node 에 위치한 여러 pod 모두 작업을 해주어야 해서 번거롭다.\n상황에 맞게 선택해서 사용하도록 하자.\n결론 k8s 클러스터를 운영할 때 node drain 과정에서 replica 에 따라서 서비스가 중단되는 경우가 발생할 수 있다. 이를 방지하기 위해서 PodDisruptionBudget 를 설정할 수 있고, 상황에 따라 MinAvailable 또는 MaxUnavailable 를 설정해서 사용하면 된다. Pod Eviction 과정에 대해서 이해하는 것이 좋다. 상황에 따라 노드를 스케줄링 제외하고 deployment rollout restart 로 pod 를 다른 노드에 이동시키기도 한다. 참고자료 https://kubernetes.io/docs/tasks/run-application/configure-pdb. https://blog.gruntwork.io/gracefully-shutting-down-pods-in-a-kubernetes-cluster-328aecec90d https://stackoverflow.com/questions/75385965/when-using-kubectl-drain-node-node-pods-it-doesnt-wait-for-new-pods-to-get-h "
},
{
	"permalink": "https://findstar.pe.kr/tags/poddisruptionbudget/",
	"title": "PodDisruptionBudget",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/adaptive-growth/",
	"title": "Adaptive Growth",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/hedgehog-sharp/",
	"title": "Hedgehog Sharp",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/portfolio-expansion/",
	"title": "Portfolio Expansion",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/product-strategy/",
	"title": "Product Strategy",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2024/11/10/personal-thoughts-on-the-criteria-for-engineering-levels/",
	"title": "내맘대로 나눠본 엔지니어 레벨의 기준",
	"tags": ["product strategy", "hedgehog sharp", "portfolio expansion", "adaptive growth"],
	"description": "",
	"type": "post",
	"contents": "소프트웨어 엔지니어링 레벨 구글에서는 소프트웨어 엔지니어링 레벨일 L3 부터 L10까지 나눈다고 한다. 비슷하게 아마존도 L3 ~ L10, 메타는 E3 ~ E9, MS는 59 ~ 80 의 직급체계를 가지고 있다고 한다. 이런 직급 체계를 가지고 있는 것은 각 단계별로 기대되는 역할과 책임을 명확하게 하기 위함이고, 그에 따라 개인입장에서는 경력 개발을 하기 위한 목표를 뚜렷하게 인식할 수 있다.\n종종 주니어, 시니어에 대한 기준에 대한 이야기를 듣고는 하는데, 최근에 커피챗을 진행하면서 이 레벨에 대한 이야기를 하게 되었다. 그러다가, \u0026lsquo;각각의 단계를 나누는 하나의 기준이 있다면 무엇일까?\u0026rsquo; 라는 생각을 하게 되어 \u0026lsquo;내가 생각하는 단계별 기준\u0026rsquo; 을 정리해 보았다.\n레벨 구분해보기 먼저 엔지니어링 레벨을 구분해보았다. 그냥 단순하게 주니어 / 시니어를 구분해서는 의미가 없으니, 좀 더 세세하게 나누어서 생각해 보았다.\n신입 주니어 미들 시니어 그 이상(?) 신입과 주니어의 차이 신입\n신입은 막 학교를 졸업하고 회사에서 업무를 시작한 단계라고 정의해봤다. 기본적인 코딩 능력과 개발 스킬을 가지고 있다. 하지만 업무 경험이 없기 때문에 회사에서 어떻게 일이 진행되는지 정확하게 알지 못한다. 업무 프로세스에 대해서 익숙해져야 하고, 자신이 어떤 과제를 직접 시작하기 어렵고, 대부분 주어진 업무를 수행하게 된다. 역량을 성장시키는데 있어서 팀에서 지원을 받는 것이 일반적이다. 주니어\n신입에 비해서 상대적인 경험치가 더 많다. 팀에서 업무가 어떻게 진행되는지 이해하고 있다. 다만 큰 레벨에서의 전략적인 부분까지 모두 이해하기는 어렵다. 본인이 처리하는 업무에 대해서 이슈를 제기하고, 개선방향을 제시 할 수 있다. 일을 통해서 알아가는 것들이 늘어나는 단계이다. 이 두 단계를 나누는 기준을 생각했을 때 나의 기준은 다음과 같다.\n신입은 뭘 모르는지 모르고, 주니어는 뭘 모르는지 알기 때문에 질문을 잘 하게 되면 주니어. 질문을 잘 할 수 있게 되려면, 나 자신에게 요구되는 기대치를 이해하고 있어야하는데 신입시절에는 그게 잘 안된다.\n생각해보자면, 신입과 주니어 모두 주어진 업무를 무사히(!!!) 수행하는 것 자체가 기대치인데, 신입시절에는 항상 나 자신에 대한 증명(?)을 하려고 애쓰는 모습들이 있어서 질문을 잘 하지 않았던 것 같다. 보는 사람 입장에서는 애가탄다고 할까..\n그래서 이걸 물어봐도 되는지, 물어보면 안되는지 고민하는 모습이 반복되면 아직 신입, 그에 대한 감이 있다면 주니어라고 할 수 있다.\n만약 누군가가 신입인데 질문을 잘 하기 어렵다고 한다면, 나의 조언은 \u0026ldquo;그냥 일단 다 물어보고, 엉뚱한걸 물어보더라도 빠르게 깨지고, 틀리고 하면서 배우는게 더 낫다\u0026rdquo; 이다. 빠르게 감잡으면 성공!\n몰라도 계속 물어보자. 계속 물어보자 주니어와 미들의 차이 미들 주니어에 비해서 상대적으로 경험치가 더 많다. 빠르게는 3년차로 보기도 하지만 주로 5년차~7,8 년차 길게는 10년차 정도 된다고 보인다. 조직 내에서 가장 많은 코드를 생산하고, 일을 끝까지 완료할 수 있는 능력을 가지고 있다. 본인이 관심을 가지는 분야에 대해서 스스로 탐구하고 연구를 통해서 역량을 성장시킨다. 시장에서 가장 많이 선호하는 레벨이라고 생각한다. (그만큼 이직하기 좋은 레벨?!) 주니어와 미들을 나누는 나만의 기준은 다음과 같다.\n주니어는 아직 누군가에게 도움을 받아야 하고 미들은 스스로 일을 완료 할 수 있기 때문에, 마감을 잘 할 수 있게 되면 미들 주니어 레벨에서는 주어진 일을 완료할 때 완료의 범위가 좁다. 특히 어떤 이슈를 해결할 때 주어진 일을 해결한 뒤에 주변에서 크로스 체크를 해준다던가, 후속작업은 다른 사람이 진행하게 되는 경우가 많다. 왜냐하면 아직은 팀에서 누군가가 같이 커버를 해주어야만 전체 일이 완료되기 때문이다.\n그에 반해서 미들 레벨에서는 일을 완료하는 범위가 상대적으로 넓다. 이슈를 해결할 때 이어질 후속 작업에 대해서 인지하고 있으면서, 동료에게 잘 전달될 수 있는 형태로 마무리 하는 능력을 갖추고 있다.\n비유하자면,\n주니어에게는 코드를 작성해서 PR을 생성하는게 완료의 범위라면, 미들에게는 해당 이슈가 머지되고, 배포되어 사용자에게 완료되었다는 피드백을 확인하는 것까지가 업무 범위가 된다. 따라서 내가 하는 일이 단순히 특정 Task를 완료하는 것이 아니라, 이 일이 제품에 또는 고객에 어떻게 연결되는지 인지하는 시야가 필요하다. 그래서 미들레벨로 갈 수록 전체 시야가 넓어지고 협업 및 커뮤니케이션에 대한 요구사항이 높아진다. 왜냐하면 그렇게 해야만 전체 일이 마감될 수 있기 때문이다.\n이 시기가 가장 많은 코드를 작성하는 시기인것 같다. 미들과 시니어의 차이 시니어 미들에 비해서 상대적으로 경험치가 더 많다. 팀 내에서 중요한 기술적 리더십을 가지고 있다. 팀장 / 매니저를 겸하는 경우도 많다. 보다 큰 규모의 프로젝트를 주도하고, 복잡한 시스템에 대한 설계를 처리할 수 있다. 또한 팀원들에 대한 지원 및 멘토링, 교육과 같은 미션도 주어진다. 미들과 시니어를 나누는 나만의 기준은 다음과 같다.\n혼자 잘 하는 것을 넘어서 조직이 함께 잘 하게 만들 수 있게 되면 시니어 미들 레벨에서 일을 잘 마무리하는 역량까지 갖추기 되면, 이제 본격적으로 더 넓은 시야를 가지는게 필요하다. 이 때부터는 자신에게 주어진 일을 넘어서 조직에서 성과를 내게 만드는 것이 중요해지는데, 이게 시니어의 어려운 점이라고 생각한다. 이 때부터는 혼자 잘 하는 것을 넘어서 조직이 함께 잘 하게 만들 수 있어야 시니어라고 할 수 있다.\n이 때문에 다른 구성원들이 선호하지 않는 일을 맡게 되기도 하고(시니어가 뒤치닥거리를 하게 되는 이유) 그런 부분에 있어서 비효율을 개선하는 업무를 진행하게 되기도 한다. 그리고 시니어 레벨 부터, 조직적인 역량을 높이는 활동들(멘토링, 교육, 문서화)에 대한 기대가 높이지기 시작한다.\n여러방면에서 요청받는 사항들의 폭이 넓어지는데, 그 이유가 바로 조직이 함께 잘 해서 결과를 만들어 내기를 기대받기 때문이라고 생각한다.\n팀이 같이 잘 하게 하기 위해서 노력해야하는 시니어 시니어 그 이상(?) 시니어 그 이상은 뭘까? Staff Engineer / Principal Engineer / Distinguished Engineer 여러가지 레벨이 있다고 하지만, 시니어 이상의 역할과 책임에서는 회사 차원에서 영향력이 더 커지는 것으로 보인다. 그만큼 정치력(?)이 필요해보이기도 하고, 어려운 자리로 보인다.\n이부분에 대한 생각은 나도 아직 정리가 안되어 있어서 뭐라고 작성하기가 어렵다. 나중에(?) 경험이 쌓이면 기록해봐야겠다.\n결론 각각의 엔지니어링 레벨을 나누어 보고, 그 경계를 가르는 기준을 정리해보았다. 물론 나의 주관적인 경험을 기반으로 했기 때문에, 꼭 이게 정답이다라고 말할 수는 없다. 참고만 하자.\n구분 설명 신입 학업을 막 마치고 산업에 들어온 엔지니어로, 기본적인 코딩 능력과 소프트웨어 개발의 기초 지식을 가지고 있다. 주로 할당된 업무를 수행하고, 팀원의 지원을 받으며 역량을 성장시킨다. 주니어 약간의 실무 경험을 가진 엔지니어로, 스스로 작업을 시작하고 문제를 해결할 수 있으며, 필요할 때 질문을 하고 이를 통해 능력을 향상시킨다. 미들 작업을 처음부터 끝까지 완수할 수 있는 능력을 갖추고 있으며, 보다 복잡한 문제를 다루고, 다른 팀원에게 피드백을 줄 수 있다. 시니어 팀 내에서 중요한 기술적 리더십을 발휘하고, 대규모 프로젝트를 주도하며, 복잡한 시스템 설계를 처리할 수 있는 능력을 보유한다. 또한, 팀 내 멘토링 및 교육 역할을 담당한다. 단계별 기준 신입 =\u0026gt; 주니어 : 질문을 잘 하게 되면 주니어 / 이를 위해서는 틀려도, 깨져도 계속 물어보는 것이 중요함 주니어 =\u0026gt; 미들 : 마감을 잘 할 수 있게 되면 미들 / 내가 하는 업무의 완료의 의미가 무엇인지 생각해보고 이를 확장해나가는 것이 중요함 미들 =\u0026gt; 시니어 : 혼자 잘 하는 것을 넘어서 같이 잘 하게 만들 수 있으면 시니어 / 같이 잘 하게 하기 위해서 조직에서의 영향력을 키우고 이를 위해 커뮤니케이션 및 다양한 역량 확장이 필요함 마치며 현재 조직에서 시니어 엔지니어 역할을 맡고 있어서 멘토링 및 교육활동을 많이 하게 된다. 각 단계에 대한 나름대로의 기준을 정해놓았더니, 고민하는 주니어분들께 멘토링에서 어떤 가이드를 해주면 좋을지 생각이 정리되어서 좋았다. 혹시나 각 레벨에서 어떤 역량을 높여야 할지 고민되시는 분들이라면 엔지니어 레벨의 구분과 각 레벨의 주요 기준을 정의해보면서 자신이 현재 어느 위치에 있는지, 앞으로 어떤 방향으로 나아가야 하는지를 명확히 하는 데 도움이 되기를 바란다.\n"
},
{
	"permalink": "https://findstar.pe.kr/2024/10/25/hedgehog-style-expansion/",
	"title": "고슴도치의 가시처럼 뾰족하게 나아가기",
	"tags": ["product strategy", "hedgehog sharp", "portfolio expansion", "adaptive growth"],
	"description": "",
	"type": "post",
	"contents": "제품 확장 전략: 고슴도치의 가시처럼 뾰족하게 나아가기 새로운 제품을 만드는 일은 언제나 흥미롭고 도전적인 과정이다. 특히나 시장이 빠르게 변화하는 이 IT업계의 특성상 제품의 가치를 높이기 위해서는, 다양한 실험을 통해서 가설을 검증하고 기능을 확장하는 일이 중요하다고 생각한다.\n작년부터 신규 팀의 리더가 되어서 팀원들과 함께 제품을 성장시키기 위해서 많은 고민을 했었다. 그 중 하나가 어떻게 제품의 기능을 확장해 나갈 것인가 였다. 이 글에서는 우리 팀이 새로운 프로덕트 개발을 위해 설정한 전략과 접근 방식을 소개하고자 한다.\n우리는 이 전략을 \u0026lsquo;고슴도치의 가시처럼 뾰족하게 나아가기\u0026rsquo;라고 지칭하고 기능을 확장하는 기본 컨셉으로 고려하고 있다.\n기본 전제 정의하기 : 제품의 영역을 확장에 대한 생각 우리팀이 개발하는 제품은 최근의 가장 주목받는 LLM을 기반으로한 MLOps 영역이다. 신기술이 매우 빠르게 발표되고 있기 때문에, 이에 대응되는 사용자들의 요구사항도 다양함과 동시에 변경이 잦다. 이런 상황속에서 제품의 기능을 개발하는데 변화가 발생할 때마다 대응하기 보다는 어느정도 정돈된 전략이 필요함을 느꼈다. 그래서 시장에서의 제품의 포지션을 설정하고 커버리지를 높이기 위해서 다음과 같이 생각을 정리해보았다.\n\u0026ldquo;제품의 커버리지를 높이는 것 = 땅따먹기\u0026rdquo;\n우리가 개발하는 제품이 시장에서 차지하고 있는 시작 포지션은 작은 영역이고, 이를 확장하는 것은 땅따먹기 게임 (paper.io) 에 비유할 수 있다. 그리고 이런 게임에서는 한번에 한 방향으로만 영역을 확장할 수 있다. 한번에 모든 부분을 전방위적으로 확장하는 것은 불가능하다.\n땅따먹기 게임에서는 한번에 모든 영역을 확장할 수 없다. 제품의 영역을 확장하기 위한 기본 전제를 정의한 다음 이를 확장하기 위해서는 다음과 같이 나아가야겠다고 정리했다.\n제품의 커버리지를 넓히기 위한 전략 수립 확장하려는 영역을 지정하고 이를 바탕으로 추가하려는 기능 영역을 정한다. 해당 영역에 대한 핵심적인 목적을 정의하고 검증하려는 가설을 뾰족하게 다듬는다. 최소한의 리소스로 가설을 검증하여 워킹하는 scene 을 만들어본다. 사용자들의 피드백을 통해서 가설이 검증되었는지 확인한다. 피드백에서 기능이 가치가 있다고 판단된다면, 뾰족한 가시위에 연결된 영역을 보강하여 두텁게 만들어서 안정적으로 개선한다. 그렇게 되면 최종적으로 목표로 했던 영역을 커버하는 피처를 제품에 추가할 수 있게된다. 우리는 이런 전략을 \u0026lsquo;고슴도치 가시처럼 뾰족하게 나아가기\u0026lsquo;라고 지칭했다.\n너무나 당연한 소리로 들릴 수 있지만, 신규 피처를 추가하는데 있어서 많은 과제를 동시에 진행하게 되면 다음 문제가 쉽게 발생하고는 했다.\n구성원들 사이의 정보의 불일치 원활하지 않은 커뮤니케이션 협업하기 어려운 구조 고슴도치의 가시처럼 뾰족하게 확장하기: 철저한 검증과 신속한 피봇 이 전략의 장점은, 시장의 요구와 환경 변화에 즉각적으로 대응할 수 있는 힘을 키울 수 있다는 점이다. 변화와 불확실성이 존재하는 상황 속에서 한 번에 큰 도전을 감행하기보다는, 작은 영역을 하나씩 확보해나가며 단계를 밟아가면서 가치를 추가할 수 있다.\n각 영역의 피처를 개별적으로 실험하며 점진적으로 확장을 시도하는 PoC(Proof oc Concept) 단계를 거치면, 투입되는 리소스를 최소화 할 수 있게 되고 아낀 리소스는 이전 단계에서 검증한 기능을 탄탄하고 안정적이게 만드는데 사용할 수 있다.\n또한 검증 과정에서 불필요한 자원 낭비를 줄이고 결과에 따라 신속히 확장하거나 조정할 수 있는 장점이 있다.\n고슴도치가 가시 하나씩 각각 영역을 탭핑한다. 하지만 이런 전략을 사용할 때 주의할점도 있다.\n\u0026lsquo;최첨단 수동\u0026rsquo; 작업이 중요함 고슴도치 전략을 실현하기 위해서는 최소한의 리소스로 최대의 검증 결과를 얻는 것이 중요하다. 이를 위해 각 피처가 동작하는지 확인할 때, 완벽한 자동화나 대규모 시스템을 구축하기보다는 “최첨단 수동” 작업을 활용하게 된다. (\u0026rsquo;최첨단 수동\u0026rsquo; 이라는 말은 우리팀의 누군가가 먼저 이야기 했는데 너무나 찰떡같이 알아듣기 쉬운 말이라 모두가 자주 사용하고 있는 단어이다.)\n이 “최첨단 수동” 방식은 기능을 검증하되 PoC 과정에서는 기능이 유지되는데 필요한 자동화 과정을 과감하게 생략하는 것이다. 예를 들어, 신규 기능에 대한 통계라던지, 운영기능을 위한 \u0026ldquo;백오피스-운영툴\u0026rdquo; 은 생략하고 기능을 출시한다던지 하는 방식이다.\n기능이 워킹하는지 확인하기 위한 최소한의 장치는 필요하지만, 이 기능이 안정적으로 돌아가기 위한 운영기능등은 우선순위를 미뤄두고 제품의 기능을 출시하는 것이다.\n실제로 큰 기능을 개발하면 운영을 위한 작업들을 개발하는데 시간과 에너지가 많이 들게 되는데, 막상 기능 출시 이후에 사용자의 반응이 좋지 않다던가, 전혀 다른 니즈가 발생해서 기능을 피벗해야하는경우가 발생한다. 그렇게 되면 그 동안 공들인 노력이 빛을 보지 못하는 경우가 발생하는데 이런 것도 하나의 리소스 낭비라고 보았다.\n수면아래에서 최첨단 수동으로 운영하는 신규 피처 팀워크와 피드백 고슴도치 전략을 성공시키기 위해서 중요한 또다른 요소는 팀워크와 피드백이다. 뾰족하게 하나의 PoC를 만드는 과정이 반복되는 데, 이 과정에서 구성원들과 사용자의 피드백을 통해서 아이디어를 가다듬고 기능이 가치있는지 판단하려면 구성원들 모두가 시장에 대한 높은 이해도와 자신의 역할에 대한 충실한 수행이 중요하다. 이를 가능하게 하는 것은 팀이 하나의 방향으로 뾰족하게 찌르기 위해서 이를 뒷받침해줄 팀워크이다.\n하나의 팀으로 움직이는 것, 그리고 구성원들의 아이디어와 사용자의 피드백을 열린 자세로 받아들이는 것 모두 쉽지 않은 일이지만 계속해서 의식적으로 리마인드 하면서 나아가고 있다.\n그래서 하나의 PoC를 시작하기 위한 과제 선정에는 구성원 모두가 반드시 참여하여 더 이상 문제에 대한 의문점이 없을 때까지 의견을 교환하는 자리를 가진다.\n흔들릴 마음가짐 우리의 전략은 빠르게 변화하는 환경 속에서 유연하게 대응하고, 새로운 기회를 모색하면서도, 각자의 위치에서 최선을 다하기 위한 것이다. 시장과 상황의 변화는 너무나 자연스럽기 때문에 변화에 적응해야하는 일은 피할 수 없지만, 개인의 입장에서 이러한 변화를 마주하는 것은 쉽지 않은 일이다.\n열심히 개발한 신규 기능이 생각보다 가치가 없다고 판단될 때, 빠르게 기능을 종료하거나 피벗하자는 이야기는 노력에 공을들인 당사자 입장에서는 납득하기 어려운 일일 수도 있다.\n그래서 이러한 변화를 마주할 때 필요한 것은 고정된 사고가 아닌 ‘흔들릴 마음가짐’이라는 자세이다.\n제품의 가치를 더 하는 일이 바다를 항해하는 일이라고 비유한다면, 파도에 흔들리는 일은 피할 수 없고, 그렇다면 오히려 마음껏 흔들릴 각오가 필요하다.\n\u0026ldquo;흔들릴 마음가짐\u0026rdquo; 이라는 키워드는 팀에 누군가 합류하면 꼭 거치는 온보딩의 마지막에 있는 \u0026ldquo;메세지\u0026quot;로 우리팀에서 추구하는 \u0026ldquo;변화에 유연하게 대응하는 자세\u0026quot;이다.\n앞서 이야기한 제품의 확장전략을 땅따먹기와 같이 이해하고, 고슴도치의 가시 같은 전략이 성공하기 위해서는 이러한 열린 사고가 필수적이다.\n어차피 바다에서 파도는 피할 수 없다. 마무리하며 우리의 전략은 빠르게 변화하는 환경 속에서 유연하게 대응하면서도, 뚜렷한 목표를 가지고 단단히 자리 잡으려는 노력이다.\n땅따먹기와 고슴도치의 가시처럼 뾰족하게 확장하는 방식을 통해서, 우리는 하나하나 영역을 확장하며 새로운 제품을 만들어 나가고 있다.\n물론 매 순간이 쉽지 않고 다양한 문제들이 쏟아지지만 이 전략이 제품 개발과정에서 든든한 나침반 역할이 되기를 바라며 팀원들과 의지를 다진다.\n앞으로도 여전히 흔들리겠지만, 그래도 뾰족하게 가시를 세워보자.\n참고사항 https://www.rhythmsystems.com/blog/real-example-of-how-a-hedgehog-supports-a-bhag "
},
{
	"permalink": "https://findstar.pe.kr/tags/asgi/",
	"title": "ASGI",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/framework/",
	"title": "Framework",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/litestar/",
	"title": "Litestar",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2024/10/13/introduce-litestar-python-framework/",
	"title": "Litestar: 새로운 Python 웹 프레임워크",
	"tags": ["python", "framework", "litestar", "ASGI"],
	"description": "",
	"type": "post",
	"contents": "Litestar: 새로운 파이썬 프레임워크 소개 웹 개발을 위한 파이썬 웹 프레임워크는 많다. Django, Flask 부터 최근에 부쩍(?) 많이 보이는 FastAPI 까지 다양한 선택지가 있다. 나 또한 1년전부터 파이썬으로 애플리케이션을 개발하면서 FastAPI를 업무에 사용하고 있다.\n하지만 FastAPI를 사용하면서 몇 가지 불편함이 있어서 새로운 프레임워클 찾던 중 Litestar 라는 새로운 프레임워크를 발견했다. 가벼우면서도 확장가능한 구조가 마음에 들어 간단하게 사용해본 소감을 정리해보았다.\n이 글에서는 Litestar의 특징과 간단한 사용법, 그리고 사용하면서 느낀 장단점을 공유하고자 한다.\n0. 새로운 프레임워크를 찾게된 이유 작년 하반기 부터 python 을 사용하여 웹 애플리케이션을 개발하고 있다. 신규 프로젝트를 구성하는데 있어서 어떤 프레임워크를 사용할지 고민이 있었는데, 당시에 주변에서 FastAPI 를 많이 추천받았다. 개발하려던 애플리케이션이 AI Model 의 Serving을 지원하는 용도인데, 이쪽 분야에서는 대부분이 FastAPI를 사용하고 있어서 업계의 트렌드를 따르고자(?) FastAPI를 선택했다.\n다만 시간이 지날 수록 FastAPI의 단점을 느끼게 되었는데, 대표적으로 다음과 같이 정리해볼 수 있다.\n기능 부족 먼저 FastAPI 프레임워크 레벨에서 제공되는 기능이 부족하다는 아쉬움이 있었다. 예를 들면 특정 Route 에서만 동작하는 미들웨어를 설정하고 싶었는데 FastAPI는 글로벌 미들웨어만 사용해서 특정 path 에서만 동작하는 미들웨어를 적용하기 어려웠다. 프레임워크 레벨에서 Event Listener 기능을 제공하지 않아 도메인영역별 관심사 분리하려고 할 때 별도의 라이브러리를 연결해야했다. 또한 Depends 에 의존하는 route 정의 방식은 HTTP 테스트를 할 때 Mocking 을 어렵게해 테스트 작성하기 어렵다고 느껴졌다. Cache 에 대한 지원이 부족했다. 방향성의 차이 개발하면서 느낀 점은 FastAPI는 말 그대로 Fast 하게 API를 작성하는데 초점을 두고, 나머지 필요한 기능은 직접 구현하거나 다른 라이브러리를 사용하는 것이 일반적이라는 것이다. 하지만 나의 경우에는 시간도 촉박하고 팀 구성원중에 Python이 익숙하신 분이 한 분 밖에 없어 추가 기능 구현을 위한 리서치에 시간을 들이기 어려웠다. 검색해보면 FastAPI 레퍼런스를 많이 찾을 수 있었지만, 이 대안이 우리에게 적절한 해법이 될지 검토하는데 시간이 많이 들었다. 1인 개발 거버넌스 우려 FastAPI의 개발이 tiangolo에게 의존적이라는 점도 우려스러운 부분이었다. (1.0 은 언제 나오는 것인가..) 물론 최근에는 팀의 지원을 받고 있고 스폰서쉽도 활발히 진행되는 것을 보면 프레임워크 발전이 더디다고 보기는 어렵다. 하지만 대부분의 기능과 관련된 PR은 creator 인 tiangolo 가 진행하고 있는 것으로 보인다. 한 사람이 코드를 주로 관리하는 상황자체가 꼭 문제가 있는 것은 아니지만 이 때문에 PR 머지가 느려지거나 중요한 버그 픽스가 지연되는 것에 대한 불만글을 보게 되어 신경이 쓰였다. 이런 단점들 때문에 다른 대안을 찾았고, 그 중에 Litestar 프레임워크를 알게되었다.\n1. Litestar 프레임워크 소개 Litestar는 ASGI를 잘 지원하고, FastAPI와 유사하게 타입 힌트를 적극적으로 활용하고 있다. 스타일을 따져보면 FastAPI와 유사한 부분이 많아 FastAPI 사용자는 큰 어려움 없이 적응할 수 있다. 2021년 12월 7일 시작했고 1.0 버전까지는 \u0026ldquo;Starlite\u0026quot;라는 이름으로 릴리즈 하다가, Starlette과 혼동된다는 피드백을 수용해서 지금의 Litestar로 이름을 변경했다. 2024년 10월 현재는 버전 2.12.1 이 최신버전이다.\nMaintainer 는 총 5명으로 Litestar Org 에서 관리하고 있다.\n포지션으로만 보면 Django와 FastAPI의 중간정도에 위치한게 아닌가 라는 생각이 드는데, Django와 같이 전체 기능을 모두 아우르는 에코시스템을 추구하지 않으면서도 FastAPI와 같은 단순함을 가지고 지향하고 있다. 사용해보면서 프레임워크의 기능들에 대한 커스터마이징 하기 수월하도록 많은 신경을 쓰고 있구나 라고 느껴졌다.\n2. 설치 및 기본 사용법 다음과 같이 설치하면 된다.\n$ pip install \u0026#34;litestar[standard]\u0026#34; 간단하게 \u0026ldquo;Hello world\u0026rdquo; 를 출력하는 웹 API를 작성해보면 다음과 같다.\n# app.py from litestar import Litestar, get @get(\u0026#34;/\u0026#34;) async def index() -\u0026gt; str: return \u0026#34;Hello, world!\u0026#34; app = Litestar([index]) 그리고 cli 에서 litestar run 명령어를 입력하면 다음과 같은 화면을 확인할 수 있다.\n$ litestar run --reload Using Litestar app from app:app Starting server process ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ┌──────────────────────────────┬──────────────────────┐ │ Litestar version │ 2.12.1 │ │ Debug mode │ Disabled │ │ Python Debugger on exception │ Disabled │ │ CORS │ Disabled │ │ CSRF │ Disabled │ │ OpenAPI │ Enabled path=/schema │ │ Compression │ Disabled │ └──────────────────────────────┴──────────────────────┘ INFO: Started server process [9697] INFO: Waiting for application startup. INFO: Application startup complete. INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit) Route 를 등록할 때 함수를 등록할 수도 있지만, Controller 클래스 기반으로 등록할 수도 있다.\n# controller.py from litestar import Controller, get from litestar.exceptions import NotFoundException from dataclasses import dataclass @dataclass class Todo: id: int title: str content: str todos = [ Todo(1, \u0026#34;마트 장보기\u0026#34;, \u0026#34;계란 2개, 우유 1개, 당근 1개 사기\u0026#34;), Todo(2, \u0026#34;도서관 책 반납하기\u0026#34;, \u0026#34;계란 2개, 우유 1개, 당근 1개 사기\u0026#34;), ] class TodoController(Controller): path = \u0026#34;/todos\u0026#34; @get() async def list_todos(self) -\u0026gt; list[Todo]: return todos @get(\u0026#34;/{todo_id:int}\u0026#34;) async def get_todo(self, todo_id: int) -\u0026gt; Todo: # todo_id에 해당하는 Todo 객체를 찾기 for todo in todos: if todo.id == todo_id: return todo # 해당하는 Todo가 없으면 404 에러 반환 raise NotFoundException(f\u0026#34;Todo id {todo_id} 를 찾을 수 없습니다 \u0026#34;) # app.py from litestar import Litestar, get from controller import TodoController @get(\u0026#34;/\u0026#34;) async def index() -\u0026gt; str: return \u0026#34;Hello, world!\u0026#34; app = Litestar([index, TodoController]) 3. Litestar 의 장점 ASGI 지원 Litestar의 가장 큰 장점 중 하나는 최신(?!) 프레임워크답게 Python ASGI를 프레임워크 레벨에서 잘 지원하고 있다는 점이다. Route 등록할 때 async 로 등록해도 프레임워크가 알아서 잘 처리해준다.\nfrom litestar import Litestar, get @get(\u0026#34;/\u0026#34;) def hello_world() -\u0026gt; dict[str, str]: \u0026#34;\u0026#34;\u0026#34;def 로 생성해도 라우트 등록에 문제가 없다.\u0026#34;\u0026#34;\u0026#34; return {\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;} app = Litestar(route_handlers=[hello_world]) 다양한 기능 그리고 FastAPI 에서 아쉬원던 다양한 기능들을 out of box(크게 설정하지 않고도 기본적으로) 잘 제공하고 있다. 대표적으로 편리하다고 생각하는 기능은 아래와 같다.\nDI 애플리케이션 레벨 / 라우터 레벨 / 컨트롤러 레벨에서 다양하게 의존성을 정의할 수 있다. scope 에 맞게 의존성 override 가 되는 기능도 지원해서 장점이라고 생각한다. 애플리케이션 bootstrapping 과정에서 필요한 dependency 를 register 해놓으면 컨트롤러에서는 편리하게 사용할 수 있다. 살짝(?) 적응이 되지 않았던 건 TypeHint 기반이 아닌 parameter name 기반으로 DI가 동작한다는 점이다. Event 객체지향/DDD 스타일로 코드를 작성할 때 도메인 영역간의 의존성 방향을 정하는 것은 아주 중요하다고 생각한다. 이 때 의존하는 도메인 영역의 이벤트를 전송할 수 있는 수단이 꼭 필요하다고 느끼는데, Litestar 에서는 프레임워크 레벨에서 이벤트 기능을 제공해서 좋았다. 특히나 async 로 정의해놓으면 이벤트를 발생시키는 쪽에서 이벤트를 수신하는 로직을 기다리지 않아도 되게끔 잘 지원하고 있어서 좋았다. OpenAPI FastAPI 에서는 Swagger를 기본 API 문서화 도구로 지원하는데 Litestar 역시 이를 지원한다. Swagger 기본 주소는 /schema/swagger 가 된다. 이 밖에 다양한 Redoc, Scalar, Repidoc 도 지원하고 커스텀 설정도 쉽게 할 수 있다. 이 밖에 Cache / Guard / OpenTelemetry 지원도 유용하다고 느껴졌다. 유용한 라이브러리와의 연결 Litestar는 SQLAlchemy와의 연결을 지원하고 있고, 추가 Plugin 기능도 제공하고 있다. Litestar는 FastAPI처럼 Pydantic을 통해 데이터 검증을 할 수 있다. 거기에 더해 MessageSpec 도 지원하고 있다. 템플린 엔진은 Jinja2, Mako, Minijinja 를 지원한다. 컨셉 Litestar는 FastAPI와 비슷하게 빠르게 시작하고 간단한 설정만으로 웹애플리케이션을 작성이 가능하다. Litestar와 Flask, FastAPI와 같은 프레임워크의 다른점은 마이크로프레임워크를 지향하지 않는 다는 점이다. 웹애플리케이션 개발에 필요한 다양한 기능을 제공하고 필요한 부분들을 손쉽게 커스터마이징 할 수 있는 인터페이스를 제공한다. 다만 \u0026ldquo;차세대 Django\u0026quot;가 되는 것을 목표로 하지도 않기 때문에 자체적인 전체 에코시스템을 형성한다기 보다 여러 기능들을 잘 조합하고 적제적소에 연결할 수 있는 프레임워크라고 보는 것이 좋다. 그래서 전체적으로는 \u0026ldquo;Django\u0026rdquo; 와 \u0026ldquo;FastAPI\u0026quot;의 중간지점이 아닐까 한다. 4. 단점 Litestar가 장점만 있으면 좋겠지만, 모든 요구사항을 만족하는 프레임워크는 없다. 그래서 사용해보면서 체감하는 단점을 생각해보았다.\n레퍼런스 부족 아직은 다른 프레임워크에 비해서 사용자 수가 작기 때문에 레퍼런스를 찾기 어렵다. Github Star 를 비교해보면, Django 가 79.6k, FastAPI 76.5k 인데 비해서 Litestar 는 5.4k 밖게 되지 않는다. 그래서 레퍼런스를 찾는 주요한 수단이 공식 Repo 로 제공하는 Litestar-Fullstack 예제 혹은 Discord 커뮤니티에 직접 참여해서 물어보는 방법이다.\n문서화 문서가 아직 정돈되어 있지 않다고 느끼는데 목차가 조금 더 정리되어 그룹핑되면 좋겠다고 생각한다. 그리고 문서에 예제코드가 간단한 구조로만 되어 있어서 좀 더 상세한 예시와 설명이 있으면 좋겠다고 느끼고 있다.\n5. 정리 및 소감 Litestar는 Python 웹 개발을 위한 새로운 가능성을 제시하는 프레임워크이다. FastAPI처럼 ASGI를 잘 지원하며, 사용하기 쉬운 구조 덕분에 빠르게 적응할 수 있었다. 나의 바램은 FastAPI 보다는 프레임워크에서 지원하는 기능이 풍부하고 그러면서도 Django 만큼 허들이 높지 않은 그럼 프레임워크를 필요로 했는데 여기에 부합한다고 느껴졌다. 굳이 표현하자면 프레임워크 포지션상 FastAPI 와 Django 의 중간정도에 있다고 느껴졌다. FastAPI를 사용하면서 느꼈던 다양한 단점들을 Litestar 쓰면서 쉽게 해소될 수 있다고 느껴져 앞으로 활용처를 늘려볼 생각이다. 하지만, 이 프레임워크는 아직 초기 단계이기 때문에 큰 프로젝트에서 사용하기에는 아직 레퍼런스가 많이 부족하다. 문서도 빈약한 부분이 많아서 개선의 여지가 많다고 느껴진다. 그럼에도 Litestar가 어떻게 발전할지 기대가 되며, 다른 한국의 Python 웹 개발자 분들도 Litestar를 사용해보며 그 매력을 느껴보면 좋겠다. 참고사항 Litestar 공식 사이트 Litestar Dicord Exploring LiteStar: A Python Framework for Lightweight Web Development Python Litestar Introduction Youtube Litestar for Python API Development / Pydantic Model Integration Litestar: Effortlessly Build Performant APIs - Talk Python to Me Ep.433 https://dev.to/v3ss0n/litestar-20-beta-speed-of-light-power-of-stars-1j62 "
},
{
	"permalink": "https://findstar.pe.kr/tags/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/benchmark/",
	"title": "Benchmark",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/java/",
	"title": "Java",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/jdk-21/",
	"title": "Jdk 21",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/throughput/",
	"title": "Throughput",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/virtual-thread/",
	"title": "Virtual Thread",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/07/02/java-virtual-threads-2/",
	"title": "Virtual Thread란 무엇일까? (2)",
	"tags": ["java", "jdk 21", "virtual thread", "throughput", "benchmark"],
	"description": "",
	"type": "post",
	"contents": "Virtual Thread (2) 이전글 에서 가상스레드에 대한 배경과, 목적, 간단한 사용법에 대해서 알아보았다. 이번에는 자주 사용하는 Spring Boot 애플리케이션에서 가상스레드를 사용하는 방법과 기존 스레드 풀 방식에 비해서 실제로 처리량이 늘어나는지 확인해보았다. 마지막으로 가상 스레드를 사용할 때 주의할 점도 정리해보았다. 이 글은 가상 스레드와 관련된 두 번째 글이다.\n시리즈 Virtual Thread란 무엇일까? (1) Virtual Thread란 무엇일까? (2) 스프링 부트에서 사용하기 먼저 Spring Boot에서 가상 스레드를 적용하는 방법을 살펴보기 전에 몇가지 알아두어야 할 것들이 있다.\n주의사항\nJDK 21 은 2023.09.19 정식 릴리즈되었다. Gradle 버전은 8.4 버전 이상에서 JDK 21을 지원한다. Spring Boot 3.2 가 2023.11.23 정식 릴리즈되어 JDK 21을 지원한다. 가상 스레드를 제대로 확인하려면 버전을 꼭 확인하기 바란다.\n적용방법 생각보다 적용방법은 간단하다. Spring Boot 3.2 버전 부터는 spring.threads.virtual.enabled 옵션을 true 설정해주면 된다. 링크 만약 3.2 버전 보다 낮은 버전을 사용중이라면 가상 스레드 Executor Bean 을 등록해주면 된다. 이 Bean 은 Tomcat이 사용자의 요청(Request)을 처리하기 위해 스레드를 사용할 때 플랫폼 스레드(OS 스레드) 대신 가상 스레드 를 사용하게 한다. # application.yaml spring: threads: virtual: enabled: true Sprinb Boot 3.2 보다 낮은 버전 사용중이라면 아래와 같이 직접 Bean 을 등록해주면 된다.\n// Web Request 를 처리하는 Tomcat 이 Virtual Thread를 사용하여 유입된 요청을 처리하도록 한다. @Bean public TomcatProtocolHandlerCustomizer\u0026lt;?\u0026gt; protocolHandlerVirtualThreadExecutorCustomizer() { return protocolHandler -\u0026gt; { protocolHandler.setExecutor(Executors.newVirtualThreadPerTaskExecutor()); }; } // Async Task에 Virtual Thread 사용 @Bean(TaskExecutionAutoConfiguration.APPLICATION_TASK_EXECUTOR_BEAN_NAME) public AsyncTaskExecutor asyncTaskExecutor() { return new TaskExecutorAdapter(Executors.newVirtualThreadPerTaskExecutor()); } 이렇게만 해주면 기존의 플랫폼 스레드를 사용하지 않고 가상 스레드 를 사용하게 된다.\n그럼 이제 실제로 처리량이 좋아지는지 한번 확인해보자.\n성능 테스트 테스트 환경 Ubuntu 20 Java 21 (sdkman) Gradle 8.4 build VM 인스턴스 머신 4 Core / 8 GiB memory 별도의 mariadb instance (max connection size 151) Max heap 2G @GetMapping(\u0026#34;/\u0026#34;) public String getThreadName() { // 단순히 스레드 이름을 반환 아무런 blocking 코드 없음 return Thread.currentThread().toString(); } @GetMapping(\u0026#34;/block\u0026#34;) public String getBlockedResponse() throws InterruptedException { // Thread sleep 1초 // 비지니스 로직 처리에 thread 가 blocking 되는 환경 가정 Thread.sleep(1000); return \u0026#34;OK\u0026#34;; } @GetMapping(\u0026#34;/query\u0026#34;) public String queryAndReturn() { // 쿼리 질의가 1초 걸린다고 가정 return jdbcTemplate.queryForList(\u0026#34;select sleep(1);\u0026#34;).toString(); } 시나리오 테스트는 3개의 API Endpoint 를 호출하였다. (simple response, block response, sleep query) 모든 API 응답이 200 OK 확인될 때까지 VU를 높여보았다. 200 OK 가 유지되는 동안 virtual thread 와 platform thread 의 throughput 을 비교해보았다. 결과 Simple response 호출\n구분 throughput virtual users Virtual Thread(1회차) 24360.88 3000 Virtual Thread(2회차) 24608.85 3000 Virtual Thread(3회차) 24455.14 3000 Platform Thread(1회차) 36085.42 3000 Platform Thread(2회차) 36396.71 3000 Platform Thread(3회차) 36107.85 3000 Thread.sleep(1000) - Blocking 호출\n구분 throughput virtual users Virtual Thread(1회차) 2975.38 3000 Virtual Thread(2회차) 2979.87 3000 Virtual Thread(3회차) 2978.39 3000 Platform Thread(1회차) 199.78 3000 Platform Thread(2회차) 199.58 3000 Platform Thread(3회차) 199.6 3000 Sleep 이 걸려 있는 쿼리 호출 (Hikari connection pool - max 150)\n구분 throughput virtual users Virtual Thread(1회차) SQLTransientConnectionException 3000 Virtual Thread(2회차) SQLTransientConnectionException 3000 Virtual Thread(3회차) SQLTransientConnectionException 3000 Platform Thread(1회차) 149.26 3000 Platform Thread(2회차) 149.53 3000 Platform Thread(3회차) 149.53 3000 결론 Thread Blocking 이 발생하지 않는 경우 Platform Thread 가 더 처리량이 높다. (Virtual Thread Scheduling 을 위한 오버헤드의 영향으로 보인다.) Thread Blocking 이 발생하는 경우 Virtual Thread를 사용할 때가 처리량이 더 높다. DB Query 에 대해서는 Virtual Thread를 사용할 때 SQLTransientConnectionException 이 발생했는데, Tomcat 이후로 로직이 넘어갔는데 DB Connection 을 얻으려다가 timeout (30s)이 발생하는 것으로 추정된다. DB Connection 과 같은 한정된 자원에 접근을 제한하려면 semaphores를 도입하는걸 고려해야할것 같다. 실제 produciton 코드는 테스트 환경과 다르고 구동 환경도 다르기 때문에 참고용임을 감안하더라도 Virtual Thread 가 Platform Thread 에 비해서 처리량이 늘어날 수 있다는 점을 확인했다. Virtual Thread 사용시 기존 Platform Thread 보다 일정영역에서 처리량이 늘어나는 것을 확인할 수 있다.\n주의사항 가상 스레드 를 사용하여 높은 처리량을 얻으려면 이를 잘 사용해야한다. 막연하게 설정을 활성화 하고 처리량이 높아지기를 기대하면 안된다. 몇가지 주의사항을 살펴보자. 기존 스레드 풀을 사용하지 말고, 개별 작업에 가상 스레드 를 할당하는 형태로 변경하자.\nourExecutor.submit(task1); ourExecutor.submit(task2); ===\u0026gt;\u0026gt;\u0026gt;\u0026gt; try (var executor = Executors.newVirtualThreadPerTaskExecutor()) { executor.submit(task1); executor.submit(task2); } ThreadLocals 에 값비싼 객체를 캐싱하지 말자\n가상 스레드 또한 자바 Thread 이므로 ThreadLocal을 지원한다. 기존에 플랫폼 스레드는 비싸기 때문에, 여러 작업 사이에 공유하는 형태로 개발해왔다 (Thread Pool). 그래서 기존에는 스레드 로컬 내부에 값비싼 객체를 캐시하는 것이 일반적인 패턴으로 활용되었다. 모든 작업이 해당 스레드의 객체를 공유하도록 유도하였다. 하지만 가상 스레드는 작업당 하나를 활용하는 것이 권장되며, 내부의 객체를 공유하지 않는다. 따라서 내부에 값비싼 객체를 캐싱하는 것은 도움이 되지 않는다. 오히려 가상 스레드가 예상보다 더 많은 메모리를 사용하게 만드는 주범이 된다. static final ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; cachedFormatter = ThreadLocal.withInitial(SimpleDateFormat::new); ... cachedFormatter.get().format(...); =====\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; static final DateTimeFormatter formatter = DateTimeFormatter....; ... formatter.format(...); synchronized 키워드 사용시 주의가 필요하다. (Pinning 이슈)\nsynchronized 키워드를 사용한 코드 블럭 안에서 blocking IO작업을 수행하는 경우에는 가상 스레드 를 unmount 할 수 없어서 Carrier Thread(Platform Thread)까지 Blocking 되는 현상이 발생한다. (이를 pinning 이라고 지칭함) 이런 경우에는 가상 스레드 의 이점을 누릴 수가 없다. synchronized가 필요한 경우 자바의 동시성 유틸리티에 있는 lock 을 사용하자. 이렇게 되면 pinning의 영향에서 벗어날 수 있다. 이런 제약은 현재 개선작업이 진행중이긴 하지만 JDK21에서는 이를 주의해야한다. (JEP 425 에서 synchronized 키워드를 사용해도 쓰레드가 pinning 되지 않도록 개선하고 있다.) pinning 이 발생하는지 탐지하려면 JFR을 사용하거나 -Djdk.tracePinnedThread 옵션을 사용하면 pinning을 탐지할 수 있다. synchronized(lockObj) { frequentIO(); } =====\u0026gt;\u0026gt;\u0026gt;\u0026gt; lock.lock(); try { frequentIO(); } finally { lock.unlock(); } 정리 요약 지금까지 JDK21 (LTS)에 추가된 가상 스레드에 대해서 알아보았다. 가상 스레드 는 리액티브 프로그래밍과 동일한 결과를 좀 더 쉽게, 덜 장황하게 달성한다. 가상 스레드 가 더 좋은 이유는 기다림에 대한 방식이 개선되기 때문이다. 가상 스레드는 기존의 플랫폼 스레드(전통적인 스레드)를 대체하려는 것이 아니며 둘다 사용이 가능하다. 가상 스레드를 사용시 처리량을 증가시킬 수 있다. Spring Boot 3.2 에서 JDK21과 호환 작업이 적용되었다. Sprinb Boot 버전업을 진행하면 기존 코드 그대로 사용하면서 혜택을 누릴 수 있을 것이다. Profject Loom 의 결과물은 가상 스레드만 있는 것은 아니다. 앞으로 추가 JEP가 더 개발될 예정이다. 소감 가상 스레드가 아무리 좋아보여도 실제 production 에 적용되기 까지는 시간이 필요할 것이다. Spring Boot 의 버전업과 기존에 다양한 라이브러리들이 호환 작업을 진행하고 또 이런 내용들이 안정화 될 때까지 시간이 필요하기 때문이다.\n가상 스레드는 은빛 총알이 아니다. 막연하게 적용만 하면 처리량이 늘어날 것을 기대하면 안된다. 잘 알고 사용하고 또 한계점에 대해서 인지해야한다.\n약간 아쉬운 부분 중 하나는 가상 스레드는 아직 추가 JEP들의 도움을 받아야 기술이 성숙해질 것 같다는 점이고, 다른 하나는 Golang 의 고루틴 과 같은 완전한 경량 스레드는 아니라는 점이다.\n그렇지만 5년내내 묵묵히 개발을 진행해온 Project Loom 개발팀에 박수를 보내며 앞으로 추가로 릴리즈될 JEP를 기대해본다. 👏👏👏👏👏👏\n(+코틀린에서의 지원, 코루틴과 궁합은 또 어떻게 될지..?)\n시리즈 Virtual Thread란 무엇일까? (1) Virtual Thread란 무엇일까? (2) 참고자료 https://www.youtube.com/watch?v=YQ6EpIk7KgY https://www.youtube.com/watch?v=n8uGsc4y6W4 https://medium.com/@zakgof/a-simple-benchmark-for-jdk-project-looms-virtual-threads-4f43ef8aeb1 https://medium.com/naukri-engineering/virtual-thread-performance-gain-for-microservices-760a08f0b8f3 https://perfectacle.github.io/2022/12/29/look-over-java-virtual-threads/ https://blog.devgenius.io/spring-boot-3-with-java-19-virtual-threads-ca6a03bc511d https://howtodoinjava.com/java/multi-threading/virtual-threads/ https://www.linkedin.com/pulse/virtual-threads-java-any-benefit-all-use-cases-arvind-kumar/ https://www.youtube.com/watch?v=zluKcazgkV4\u0026amp;ab_channel=KotlinbyJetBrains "
},
{
	"permalink": "https://findstar.pe.kr/tags/collaboration/",
	"title": "Collaboration",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/communication/",
	"title": "Communication",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/06/18/good-communication-for-collaboration/",
	"title": "협업을 위한 &#39;좋은 커뮤니케이션&#39;",
	"tags": ["communication", "collaboration"],
	"description": "",
	"type": "post",
	"contents": "\u0026lsquo;협업을 위한\u0026rsquo; 좋은 커뮤니케이션 능력 흔히 회사에서 업무를 수행하는데 있어서 갖추어야할 필수 역량의 하나로 \u0026lsquo;커뮤니케이션 능력\u0026lsquo;을 꼽는다. 소프트웨어 엔지니어로 일하면서, 애플리케이션을 작성하는 코딩 스킬, 문제 해결능력과 더불어 협업에 꼭 필요한 역량이라고 생각한다. 그런데, 이렇게 업무를 수행하기 위한 \u0026lsquo;협업\u0026rsquo; 과정에서 필요한 \u0026lsquo;좋은 커뮤니케이션\u0026lsquo;이란 무엇일까? 몇가지 생각들을 정리해보았다.\n왜 좋은 커뮤니케이션 능력이 필요할까? 좋은 커뮤니케이션 능력을 갖추고 있다면 협업이 수월해진다. 동료와 소통하여 공통의 목표를 위해서 일하는 과정에서 자신의 의도를 이야기하고, 문제상황을 전달하는 과정에서 가장 많이 일어나는 일이 바로 커뮤니케이션이기 때문에, 커뮤니케이션의 효율이 좋으면 좋을 수록 시간과 에너지가 절약된다. 요약하자면 \u0026lsquo;좋은 커뮤니케이션 역량\u0026lsquo;을 가지고 있을 수록 커뮤니케이션에 들어가는 시간과 에너지가 줄어들어 협업이 수월해진다.\n좋은 커뮤니케이션이란? 그렇다면 좋은 커뮤니케이션이란 무엇일까? 커뮤니케이션을 대화라고 정의하면 말을 유창하게 하는 것이 좋은 커뮤니케이션일까? 요즘은 말보다는 업무 메신저 (슬랙, 잔디, 카카오톡(?))으로 이야기 하는 경우가 많은데, 메세지를 작성을 잘 해야한다는 것일까? 간단하게 세 가지를 꼽아보았다.\n먼저 첫 번째로 상대방의 시간과 에너지를 아껴줄 수 있는 커뮤니케이션이 협업을 위한 좋은 커뮤니케이션이라고 생각한다.\n1. 상대방의 시간을 아껴주는 커뮤니케이션 상대방의 시간을 아껴주는 커뮤니케이션이란, 커뮤니케이션의 목적을 명확히 하여 상대방이 \u0026lsquo;기다리는\u0026rsquo; 시간을 줄여주는 커뮤니케이션을 말한다. 흔히 업무 메신저와 같은 협업툴을 사용할 때가 많은데 이런 경우 효과를 발휘한다. 다음의 예시를 살펴보자\n얼핏보기에는 별다른 문제가 없어보인다. 하지만, 자세히 보면 상대방에게 용건을 전달하는데 까지 여러번 메세지를 전달했다는 것을 알 수 있다. 좀 더 직관적으로 알수 있도록 시간을 추가해보면 다음과 같다.\n이제 문제점을 명확하게 확인할 수 있다. 위의 경우에는 \u0026lsquo;커뮤니케이션의 목적\u0026rsquo;을 전달하는데 까지 무려 1시간 20여분이 소요된다. 따라서 상대방과 나의 시간이 낭비된다.\n만약 도중에 점심시간이라도 있게되면, 아침에 시작한 \u0026lsquo;커뮤니케이션\u0026rsquo;은 점심시간이 지나서야 목적을 전달할 수 있게 된다.\n이런 커뮤니케이션 방식보다는 \u0026lsquo;두괄식\u0026rsquo;으로 용건을 전달하는 방법을 사용하면 상대방과 나의 시간을 아껴줄 수 있다. 두괄식을 사용하면 \u0026lsquo;커뮤니케이션의 목적\u0026rsquo;을 처음부터 명확하게 전달할 수 있다.\n메신저나, 사내 업무툴은 비동기로 업무를 수행할 수 있게 해주는 장점이 있다. 다만 커뮤니케이션 방식을 의식하지 않으면 일상생활에서 사용하듯이 진행되버리고, 상대와 나의 시간을 소모하는 결과를 가져온다. 아주 사소한 내용이지만, 의식적으로 \u0026lsquo;목적을 명확히 앞부분에 전달\u0026lsquo;하는 두괄식 커뮤니케이션을 진행하는 것이 좋다.\n상대방과 커뮤니케이션할 때 \u0026lsquo;목적\u0026rsquo;을 제일 앞부분에 전달하자.\n2. 상대방의 에너지를 아껴주는 커뮤니케이션 두 번째로는 상대방의 에너지를 아껴주는 커뮤니케이션을 생각해 볼 수 있다. \u0026lsquo;두괄식\u0026rsquo;으로 목적을 명확하게 전달하더라도 \u0026lsquo;커뮤니케이션 효율이\u0026rsquo; 낮은 경우가 발생할 수 있다.\n상대방이 목적을 분명하게 이해했음에도 추가적인 정보가 필요한 경우가 있기 때문이다. 대부분 \u0026lsquo;맥락\u0026lsquo;이 생략된 상태로 커뮤니케이션하는 경우로 볼 수 있다. 다음의 예시를 살펴보자.\n도움을 필요로 하는 입장에 빌드 에러를 해결해달라고 요청하는 커뮤니케이션을 시작하였지만, 상황에 대한 구체적인 \u0026lsquo;맥락\u0026rsquo;이 생략되어 있기 때문에 상대방인 \u0026lsquo;B\u0026rsquo;는 적절한 대응을 하기가 어렵다. 이 때문에 추가적으로 상황에 대한 자세한 확인과정이 필요하고 그만큼 에너지를 소모할 수 밖에 없다. 따라서 이 경우에는 다음과 같이 \u0026lsquo;맥락\u0026rsquo;을 포함하여 커뮤니케이션 하는 것이 좋다.\n이런 방식을 저맥락(low context) 커뮤니케이션이라고 한다. 한국어는 특성상 \u0026lsquo;고맥락(high context)\u0026rsquo; 문화를 표현하기 좋기 때문에 명시적이지 않아도 상황에 따라서 상대방이 대화를 이해하는데 문제없는 경우가 많다. 그러나, 업무를 처리하는데 있어서는 다양한 이슈를 동시에 처리하는 경우가 많기 때문에 특정 경우의 맥락을 계속 유지하기가 어렵다.\n따라서 \u0026lsquo;명시적\u0026rsquo;이고 \u0026lsquo;배경에 대한 설명\u0026rsquo;을 충분히 추가하여 \u0026lsquo;저맥락\u0026rsquo;으로 커뮤니케이션 하는 것이 좋다.\n명시적이고 상황을 충분히 설명한 \u0026lsquo;저맥락\u0026rsquo; 커뮤니케이션을 고민하자.\n고맥락, 저맥락에 대해서 자세히 알아보려면 다음의 글을 읽어보자 링크 3. 친절하게 마지막으로 너무도 당연하지만 잊기 쉬운 \u0026lsquo;친절하게\u0026rsquo; 이다. 누구나 자신의 업무가 바쁘고 스트레쓰를 받기 쉽기 때문에 누군가 말을 걸어오거나, 업무를 요청받을 때 날카로워지기 쉽다.\n이건 나 자신의 경우도 똑같기 때문에, 의식적으로 상대방에게 \u0026lsquo;친절하게\u0026rsquo; 대화하려는 노력이 필요하다. 그렇지 않으면 나도모르게 차갑고 대하기 어려운 동료가 되어 있을 수 있다.\n플라톤은 이런말을 했다고 한다.\n\u0026ldquo;친절하라. 당신이 만나는 사람들은 다 힘겨운 싸움을 하고 있는 중이다.\u0026rdquo;\n\u0026ldquo;Be kind, everyone you meet is fighting a hard battle.\u0026rdquo;\n정리 \u0026lsquo;목적을 앞부분에 두고 분명하게 표현한\u0026rsquo; - \u0026lsquo;두괄식\u0026rsquo; 커뮤니케이션 \u0026lsquo;배경과 맥락을 명시적으로 드러낸\u0026rsquo; - \u0026lsquo;저맥락\u0026rsquo; 커뮤니케이션 그리고 친절하게 이 세 가지를 잘 기억한다면 협업하는데 있어서 \u0026lsquo;좋은 커뮤니케이션 스킬\u0026rsquo;을 발휘할 수 있다.\n물론 커뮤니케이션 상황은 앞서 예시로 든 경우보다 훨씬 더 다양하고 복잡하며, 상대방이 누구냐에 따라서 또 달라질 수 있다. 그렇지만 기본적으로 위의 세 가지는 의식적으로 몸에 익혀야 동료와 협업하는 데 있어서 상대방의 시간과, 에너지를 아껴줄 수 있는 커뮤니케이션을 진행할 수 있다고 생각한다.\n비단 소프트웨어 엔지니어링 분야가 아니더라도, 커뮤니케이션 능력은 필수적이다. \u0026lsquo;좋은 커뮤니케이션 역량을 갖추어\u0026rsquo; 동료와의 협업을 원활하게 진행해보자.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/argocd/",
	"title": "Argocd",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/06/04/argocd-installation/",
	"title": "ArgoCD 설치",
	"tags": ["argocd"],
	"description": "",
	"type": "post",
	"contents": "ArgoCD 최근 K8S 클러스터를 정비할 일이 있어서 새롭게 구성을 마치고 배포를 위해서 ArgoCD를 재설치 해보았다.\nArgoCD는 GitOps를 기반으로 하는 CD 도구로, k8s 애플리케이션의 CD(지속적인 배포)를 지원한다.\nGitOps라는 건 ArgoCD가 Git Repository와 연동하여 애플리케이션 배포를 자동화하고, 롤백과 같은 작업을 수행한다는 걸 의미한다. 따라서 애플리케이션이 빌드되고 컨테이너 이미지가 생성되는 과정에서 Git Manifest의 내용을 수정하면 K8S 클러스터에 자동으로 애플리케이션이 배포된다.\n도식화 하면 다음과 같은 순서가 된다.\n쉽게 말해서 ArgoCD 는 K8S 클러스터를 위한 애플리케이션 배포 도구이고 Gitops 방식으로 배포한다는 특징을 가지고 있다. K8S를 운영하면 애플리케이션을 어떻게 배포할지는 사실 케이스별로 각각 다른 도구를 사용할 수도 있다. 각각의 상황에 따라 사용 가능한 도구가 제한될 수도 있다. (사내 도구를 사용해야한다던가, 이미 운영중인 도구가 있을 수도 있다.) 그만큼 다양한 배포 방식이 존재하고, 다양한 도구들이 있기 때문에 어느 것이 더 좋다고 말하기는 어렵다. 나의 경우에는 ArgoCD를 가장 익숙하게 사용하고 있다.\n설치 방법 준비사항 먼저 ArgoCD가 설치될 K8S 클러스터를 준비한다.\n나의 경우에는 ArgoCD가 설치되어 동작할 별도의 K8S 클러스터를 준비했다. 그 이유는 ArgoCD의 동작이 실제 Production 애플리케이션이 구동되는 클러스터에 영향을 주지 않도록 하고 싶었기 때문이다. 참고로 애플리케이션은 dev, cbt, prod 를 위한 클러스터가 독립적으로 존재했다. ArgoCD 설치 가이드 확인\n설치 시점에 2.6을 사용했다. (그 사이 2.7이 릴리즈 되었다.) https://argo-cd.readthedocs.io/en/release-2.6/getting_started/ ArgoCD CLI 설치\n설치도중 사용할 CLI 명령어를 준비한다. brew install argocd 설치 순서 공식 가이드 문서를 참고하여 클러스터에 ArgoCD를 설치한다.\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 도메인부여\n각각 환경이 달라서 세부 설정은 생략한다. 다만 Ingress 를 설정하여 http 접근으로 ArgoCD에 접근가능하게 하였다. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-server-http-ingress namespace: argocd annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/backend-protocol: \u0026#34;https\u0026#34; nginx.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: name: http host: my-argocd.my-domain.com tls: - hosts: - \u0026#39;my-argocd.my-domain.com\u0026#39; secretName: my-domain-secret 접속확인\nIngress 에서 설정한 \u0026ldquo;my-argocd.my-domain.com\u0026rdquo; 으로 접속해본다. 로그인 창이 표시되면 정상적으로 설치된 것이다. 기본 계정은 admin 이고 패스워드가 필요한데, 패스워드는 ArgoCD가 설치될 때 임의의 값으로 생성된다. ArgoCD가 생성한 임의의 패스워드를 확인하기 위해서 다음과 같이 명령어를 입력해서 확인한다. kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=``\u0026#34;{.data.password}\u0026#34; | base64 -d; echo Github 연결 (optional)\nadmin 사용자로 ArgoCD를 사용할 수 있지만, 팀에서 운영한다면 개별 계정을 사용해서 접근하는 것이 권한관리도 쉽고 관리 이력상 좋다. 계정연동 방법은 여러방법이 있지만, 개발자라면 모두 Github 계정은 있을테니 Github 과 연동해서 로그인해보도록 설정해보았다. 신규 OAuth App 등록 \u0026ldquo;https://github.com/organizations/{my-org}/settings/applications\u0026quot; 에 접속하여 새로운 OAuth 앱을 생성하자 생성한 OAuth App 의 ClientId, ClientSecret 를 복사해놓는다. ArgoCD ConfigMap 등록 다음과 같이 ArgoCD가 설치된 클러스터에 ConfigMap 을 등록한다. apiVersion: v1 kind: ConfigMap metadata: annotations: labels: app.kubernetes.io/name: argocd-cm app.kubernetes.io/part-of: argocd name: argocd-cm namespace: argocd data: url: https://my-argocd.my-domain.com dex.config: | connectors: - type: github id: github name: github config: hostName: github.com clientId: \u0026#34;깃헙에서 복사한 client Id\u0026#34; clientSecret: \u0026#34;깃헙에서 복사한 client secret\u0026#34; orgs: - name: \u0026#34;github organization\u0026#34; Rbac ConfigMap 등록 rbac 를 위한 ConfigMap 도 등록한다. apiVersion: v1 kind: ConfigMap metadata: name: argocd-rbac-cm namespace: argocd data: policy.default: role:admin 이제 다시 ArgoCD 웹 UI에 접속하면 Github 계정을 사용해서 로그인 할 수 있다. Gitops Manifest Repo 연결\nArgoCD에서 변경을 감지하고 K8S 클러스터의 형상을 동기화할 Gitops Manifest Repo를 연결한다. 타겟 K8S 클러스터 등록\nArgoCD 가 애플리케이션을 배포할 대상이 되는 Target K8S Cluster 를 등록해야한다. 이 과정은 Web UI에서 불가능하기 때문에 앞서 준비과정에서 설치한 ArgoCD CLI를 사용한다. argocd login my-argocd.my-domain.com --username admin (패스워드입력필요) argocd cluster add my-prod-target-cluster-context context 를 추가하는 부분에서 알 수 있듯이 cli 를 실행하는 시점에 context 정보가 로컬에 있어야 한다 프로젝트 등록\n이제 프로젝트를 등록해야한다. 프로젝트는 일종의 애플리케이션의 그룹을 지정한다고 이해하면 된다. 애플리케이션 등록\n마지막으로 애플리케이션을 등록해보자. 입력할 항목이 많은데 다음의 부분만 입력하면 된다. GENERAL - Application Name (애플리케이션 이름) GENERAL - Project Name (소속될 프로젝트 이름) GENERAL - SYNC POLICY (Gitops Repo 에 변경이 일어 났을 시 자동으로 클러스터에 동기화 할것인지, 수동으로 할것인지 결정) SOURCE - Repository URL (Gitops Repo 선택) SOURCE - Path (애플리케이션이 참조할 YAML Path 선택) DESTINATION - Cluster URL (클러스터 등록이 잘되었다면 대상 K8S 클러스터의 Master Node 주소가 나온다.) DESTINATION - Namespace (애플리케이션을 배포할 네임스페이스 지정) 나머지 부분은 그대로 두고 등록하면 된다. 동기화가 진행된 애플리케이션 샘플 동기화가 진행되면 다음과 같이 애플리케이션의 상태를 확인할 수 있다. 이제 컨테이너 이미지를 빌드하고 Gitops Repo 에 변경사항을 업데이트 하면 ArgoCD가 자동으로 클러스터에 애플리케이션을 배포해준다.\n기타 ArgoCD는 자체적으로 대상 Gitops Repo를 감시하다가, 변경사항이 있다면 확인하여 클러스터에 적용한다. 하지만 이 과정은 Gitops repo 가 변경되는 즉시 적용되지 않고 약간의 시간텀이 필요하다. 따라서 좀 더 빠른 배포를 위해서 Github 에 Webhook 을 전달하는 방법을 고려할 수 있다. Github Organization 에 Webhook 등록 Github Org 셋팅 페이지 - \u0026ldquo;https://github.com/organizations/{my-org}/settings/hooks\u0026quot; 에서 Webhook 등록 다음과 같이 Webhook 을 등록한다. 등록 주소는 \u0026ldquo;https://my-argocd.my-domain.com/api/webhook\u0026quot; 의 형식이된다. 이렇게 하면 ArgoCD가 좀 더 빠르게 반응한다. 정리 K8S 클러스터를 운영할 때 애플리케이션 배포를 위해서 ArgoCD를 운영할 수 있다. ArgoCD는 Prod 클러스터에 영향을 주지 않기 위해서 별도의 클러스터에서 운영하기로 하였다. 로그인을 쉽게 하기 위해서 Github OAuth 연동을 하였다 클러스터 추가는 CLI에서만 가능하다. 애플리케이션을 추가하여 Sync 를 확인한다. Gitops repo 가 변경되면 자동으로 클러스터에 애플리케이션 형상이 동기화된다. Webhook 을 등록하면 좀 더 빠르게 동기화된다. 참고자료 ArgoCD "
},
{
	"permalink": "https://findstar.pe.kr/tags/spring/",
	"title": "Spring",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/spring-camp/",
	"title": "Spring Camp",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/spring-camp-2023/",
	"title": "Spring Camp 2023",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/04/24/spring-camp-2023/",
	"title": "Spring Camp 2023 참여 후기",
	"tags": ["java", "spring", "spring camp", "spring camp 2023"],
	"description": "",
	"type": "post",
	"contents": "Spring Camp 2023 지난 2023.04.22일 토요일 KSUB에서 진행한 Spring Camp 2023 행사에 참석하였다. 이전까지 종종 영상과 발표자료로만 들었던 행사였는데 직접 오프라인으로 참석하니 현장분위기를 느낄 수 있었다.\n총 7개의 세션을 들었는데, 각 세션에 대한 간략한 내용과 인상깊었던 부분들을 정리해보았다.\n세션 정리 1. \u0026ldquo;어느 월급쟁이 개발자의 스프링 부트 따라잡기\u0026rdquo; 첫 세션은 김지헌 님의 발표였다. 스프링 부트의 변천사를 이야기 해주셨는데, 그 뒤에 숨어있을 발표자분의 삽질내공이 느껴져서 즐겁게 들을 수 있었다.\n특히 최근에 출시된 Spring Boot 3 버전에 대해서 여러가지를 설명해주셨는데 다음 내용들을 더 살펴봐야겠다고 생각했다.\nNative Support Micrometer Observability Hibernate ORM 6.1 R2DBC 1.0 지원 그리고 Java 17 버전 출시와 함께 Spring 에서 굳이 Kotlin 을 사용하지 않아도 충분히 좋다고 느끼고 있다고 하셨는데 어느정도 공감되는 말이었다.\n\u0026ldquo;인생은 Build 와 Deploy 사이의 Code\u0026rdquo; - 이제 스프링캠프에서 그만 발표하고 싶다고 하시던 김지헌님.\n발표자료 2. \u0026ldquo;글로벌 서비스를 위한 Timezone/DST\u0026rdquo; 두 번째로는 김대겸님이 발표해주신 발표로, 글로벌 서비스를 출시할 때 고민해보아야할 Timezone/DST 에 대한 내용이었다. 나 또한 글로벌 서비스를 출시해본 경험이 있어서 시간값을 ZonedDateTime 으로 고정해서 사용했던적이 있었는데, 그 때의 경험이 떠올라서 반가운 마음으로 세션을 들었다.\n또한 LocalDateTime 과 ZonedDateTime 과의 관계, 그 사이에 OffsetDateTime 차이까지 상세히 알려주셔서 좋았다. DST는 잘 모르던 개념이었는데 이 발표를 통해서 Daily Saving Time (일명 써머타임) 이라는 것을 알게되었고 왜 사용하는지도 알게되었다. (여름에는 일조시간이 길어서 하루 시작을 일찍 하게 하려고 사용)\n글로벌 서비스를 한다면 클라이언트와의 통신에서 ZonedDateTime 을 사용하고 표시는 상황에 맞게 LocalTime 기준으로 표시하는 것을 권장\n발표자료 3. 대규모 엔터프라이즈 시스템 개선 경험기 - 1부 - 달리는 기차의 바퀴 갈아 끼우기 세 번째로는 임형태님이 네이버 쇼핑의 레거시 시스템을 새로운 시스템으로 전환하는 경험담을 소개해주셨다. 역시나 남이 삽질한 경험담이 제일 재미있는 법! 스프링 캠프 2023 세션중에서 제일 재미있었던 세션이었다. 달리는 기차의 바퀴를 갈아 끼우기란 얼마나 어려운지 새삼스럽게 느끼게 되었다.\nstrangler pattern 에 대해서 기원과 상세한 비유를 이미지를 통해서 소개해주셔서 쉽게 이해할 수 있었다. 그리고 이전 시스템과, 신규 시스템의 전환을 어떤 타이밍에 진행해야하고, 전환과정에서 겪을 수 있는 문제들을 차근차근 소개해주셔서 많은 도움이 되었던 발표였다. 또한 재미난 발표장표와 더불어 적절한 유머 코드를 겸비한 깔끔한 진행으로 감탄이 절로나왔다. 이어지는 2부 세션을 위한 적절한 안배도 돋보였던 세션이었다.\n\u0026ldquo;우리는 개발팀으로서 서비스 성장에 놓은 품질의 소프트웨어와 개발과정으로 기여하는 것을 중요한 책임으로 여깁니다.\u0026rdquo;\n나중에 제목 때문인지 인터넷밈이 자꾸 생각났다.\n발표자료 4. 대규모 엔터프라이즈 시스템 개선 경험기 - 2부 - 새 술 담을 새 부대 마련하기 앞서 임형태님이 시니어 입장에서의 발표를 진행했다면, 함께 참여한 주니어 입장에서 김선철님이 발표해주신 세션이다. (매트릭스의 빨간약을 드셨다고..) 역시나 팀웤이 돋보였다고 느낄 수 있었는데, 앞선 세션의 위트와 유머가 적절히 유지되어서 보는 내내 웃을 수 있었다.\n본인의 경험에서 느껴지는 디테일한 내용을 잘 설명해주셔서 많은 참고가 되었던 세션이었다. 특히나 패키지 구조의 경우에는 항상 고민스러운 부분인데, 김선철 님과 임형태 님이 생각하는 최선의 방향이 무엇이었는지 알수 있어서 좋았다.\n아래의 \u0026ldquo;포트 앤 어댑터\u0026rdquo; 패턴에 대한 정리 장표도 인상깊었다.\n이벤트와 관련해서는 Kafka payload를 어떻게 구성하셨는지 궁금했었는데 따로 질문할 시간이 없어서 못물어봤다.\n발표자료 무사히 기존 레거시를 걷어내고 새로운 시스템으로의 개선을 마무리 하시길.\n5. 실무에서 적용하는 테스트 코드 작성 방법과 노하우 다섯 번째로 블로그로 익숙한 김남윤님의 발표였다.\n먼저 테스트 코드 작성이 어렵다고 느껴지는 경우를 예시로 들고, 단계적으로 대안을 제시하면서 각각 대안의 장단점을 설명해주셨는데 이해하기 쉬웠다. 테스트 코드에 대한 내용은 들어도 들어도 어려운데 이 세션은 그렇지 않아서 좋았다\n특히 테스트 코드 작성이 어렵다면 그건 테스트 코드가 피드백을 주는 거라고 말씀해주신 부분이 인상깊었는데, 마음속에서 \u0026ldquo;테스트 코드 짜기 어렵다고? 그럼 니가 잘못 짠거야\u0026rdquo; 라고 들려서 많이 찔렸던(?) 세션이었다.\n\u0026ldquo;테스트 코드가 피드백을 준다.\u0026rdquo;\n발표자료 6. 구현부터 테스트까지 - 대용량 트래픽 처리 시스템 여섯 번째로 이경일님의 발표로 본인을 잡부(?)로 소개하셨지만, 전혀 잡부(?)로 보이지 않는 시니어 개발자분이셨다. 무려 3개의 프로젝트를 겸직하시면서(!) 대용량 트래픽을 처리할 수 있는 효율적인 시스템을 고민하신 결과를 공유해주셨다\n로컬캐시와 K8S 환경에서 적용하기 위한 방법들을 경험담과 함께 소개해주셨는데, 몇몇 부분에서는 잘 알지 못하는 기술들이 소개되어서 흐름을 놓쳤었다. 나중에 발표 영상이 올라오면 다시 볼 예정.\n동기화를 설명해주시면서 Volatile은 막연하게 알던 내용을 이해하기 쉽게 소개해주셔서 좋았었다. (Volatile 발음은 어떻게 하는건지.)\n함께할 동료를 찾으신다고 하셨지만, 새벽에 코딩하는 이야기를 해주셔서 당황 ㅎㅎ\n발표자료 7. Journey to Modern Spring (클라우드 시대를 맞이하는 스프링의 자세) 마지막으로 박용권님의 발표로 스프링의 변천사를 한눈에 볼 수 있는 세션이었다. 시작하면서 말씀하실 내용이 많아서 말을 빨리하겠다고 말씀하셔서 정신을 바짝차리고 들었다.\n전체적인 스프링의 변화를 짚어주시면서 왜 그렇게 변화해왔는지 설명해주셔서 좋았다. 중간에 내가 최근에 관심있어하는 Virtual Thread 도 소개되어 반가운 마음이 들었다. 스프링 6와 스프링 부트 3의 주요 변경사항을 잘 정리해주셨고, 특히나 Native 기술에 대해서 강조해주셨는데, 아직은 테스트 중이시라고.. 발표를 듣고 나니, 올 하반기에 나올 JDK21, Spring 6.1 가 더욱 기대되었다.\n여담으로 발표를 많이 하신분답게 내공이 어마어마하구나라는 걸 느낄 수 있었다. (시의적절한 이미지가 너무 잘 좋았어요.)\n발표자료 소감 무려 6시간 넘는 행사인데, 세션 하나하나 즐겁게 들을 수 있었고, 새로운 내용들도 알게되어 좋았었다. 간만에 서울 나들이라 돌아가는 길에 녹초가 되었지만, 뿌듯한 마음이 남아 있었다. 다음의 스프링 캠프도 기대해본다.\n그외에 행사에 대해서 장소\nSKT 타워는 좋았지만, 인원대비 약간 작은 느낌 (특히나 화장실..) 좌석마다 콘센트가 제공이라니! 😍 하지만 와이파이는 없었다. 부스\n인프런, 그리고 연예인 앨리스님 실물로 봄(이벤트 진행하는데 목소리가 너무 또렷하게 들려서 잊을 수 없었다.) DEVOCEAN - 요즘 열심히 프로모션 하는 느낌으로 자주 보인다. 현대자동차 - 채용을 열심히 하고 계셨다. Azure 진행\n입장시 물이없어서 힘들었다. 커피 브레이크는 적절했음. 시작과 마지막에 약간 어수선한 분위기였는데, 개회사-폐회사가 뚜렷하게 없어서 그런 느낌이 들었던 것 같다. 세션의 Q\u0026amp;A가 너무 짧게 진행되어서 아쉬웠다. 참고 나중에 발표영상은 다음 링크에 올라올 것 같다.\n스프링캠프 2023 스프링 캠프 유튜브 "
},
{
	"permalink": "https://findstar.pe.kr/2023/04/17/java-virtual-threads-1/",
	"title": "Virtual Thread란 무엇일까? (1)",
	"tags": ["java", "jdk 21", "virtual thread", "throughput"],
	"description": "",
	"type": "post",
	"contents": "Virtual Thread (1) 2023년 9월 19일에 릴리즈된 Java 21 는 Java 8 이후 세번째 LTS 버전이다(11, 17, 21). 이 버전에서는 많은 사람들이 기다리고 있는 가상 스레드 라는 기능이 추가되었다. 이 Virtual Thread(이하 가상스레드) 가 어떤 의미가 있기 때문에 많은 사람들이 기다리고 있는지 알아보고 그 의미를 정리해보았다. 이 글은 가상 스레드와 관련된 첫 번째 글이고 다음 글로 계속 이어진다.\n시리즈 Virtual Thread란 무엇일까? (1) Virtual Thread란 무엇일까? (2) 가상 스레드 란? 가상 스레드 란 기존의 전통적인 Java 스레드에 더하여 새롭게 추가되는 경량 스레드이다. Project Loom의 결과물로 추가된 기능으로 OS 스레드를 그대로 사용하지 않고 JVM 자체적으로 내부 스케줄링을 통해서 사용할 수 있는 경량의 스레드를 제공한다. 하나의 Java 프로세스가 수십만~ 수백만개의 스레드를 동시에 실행할 수 있게끔 설계되었다.\nProject Loom 이란? 경량의 스레드를 Java에 추가하기 위해서 가상 스레드를 비롯한 여러가지 기능들을 개발하는 프로젝트로 Loom이란 단어는 Thread 의 사전적 정의가 \u0026lsquo;실\u0026rsquo; 이라는데 착안하여 실을 엮어 \u0026lsquo;직물을 만든다는 뜻\u0026lsquo;이다. Loom 프로젝트의 결과로 탄생한 \u0026lsquo;Virtual Thread\u0026rsquo;도 처음에는 Fiber-섬유 라고하는 별도의 기능으로 개발되었으나, 최종적으로는 기존 스레드 문법과 호환될 수 있는 형태로 발전했다.\n먼저 이 가상 스레드 가 왜 필요하게 되었는지 그 배경과 가상 스레드 가 해결하고자 하는 문제에 대해서 알아보자.\n배경 자바의 스레드는 OS의 스레드를 기반으로 한다.\n자바의 전통적인 스레드는 OS 스레드를 랩핑(wrapping)한 것으로 이를 플랫폼 스레드 라고 정의한다. (자바의 전통적인 스레드=플랫폼 스레드) 따라서 Java 애플리케이션에서 스레드를 사용하는 코드는 실제적으로는 OS 스레드를 이용하는 방식으로 동작했다. OS 커널에서 사용할 수 있는 스레드는 갯수가 제한적이고 생성과 유지 비용이 비싸다. 이 때문에 기존에 애플리케이션들은 비싼 자원인 플랫폼 스레드를 효율적으로 사용하기 위해서 스레드 풀(Thread Pool) 만들어서 사용해왔다. 처리량(throughput)의 한계\nSpring Boot와 같은 애플리케이션의 기본적인 사용자 요청 처리 방식은 Thread Per Request 이다. 이는 하나의 request(요청)을 처리하기 위해서 하나의 스레드를 사용한다. 애플리케이션에서 처리량을 늘리려면 스레드를 늘려야 하지만 스레드를 무한정 늘릴 수 없다. (OS 스레드를 무한정 늘릴 수 없기 때문) 따라서 애플리케이션의 처리량(throughput)은 스레드 풀에서 감당할 수 있는 범위를 넘어서 늘어날 수 없다. Blocking으로 인한 리소스 낭비\nThread per Request 모델에서는 요청을 처리하는 스레드에서 IO 작업 처리할 때 Blocking 이 일어난다. 이 때문에 스레드는 IO 작업이 마칠 때까지 다른 요청을 처리하지 못하고 기다려야 한다.(Blocking 동안 대기) 애플리케이션에 유입되는 요청이 많지 않거나 또는 스케일 아웃으로 충분히 커버할 수 있는 정도라면 문제가 없지만, 아주 많은 요청을 처리해야하는 상황이라면 Blocking 방식으로 인해 발생하는 낭비를 줄여야 할 필요가 있다. 이 때문에 Blocking 이 아니라 Non-blocknig 방식의 Reactive Programming이 발전하였다. Reactive Programming의 단점\n처리량을 높이기 위한 방법으로 비동기 방식의 Reactive 프로그래밍이 발전해왔다. 한정된 자원인 플랫폼 스레드가 Blocking 되면서 대기하는 데 소요된 스레드 자원을 Non-blocking 방식으로 변경하면서 다른 요청을 처리하는데 사용할 수 있게 되었다. 대표적으로 Webflux 가 이렇게 Non-blocking으로 동작한다. 다만 이런 Reactive 코드는 작성하고 이해하는 비용을 높게 만들었다. (Mono, Flux) 또한 기존의 자바 프로그래밍의 패러다임은 스레드를 기반으로 하기 때문에 라이브러리들 모두 Reactive 방식에 맞게 새롭게 작성되어야 하는 문제가 있다. 자바 플랫폼의 디자인\n자바 플랫폼은 전통적으로 스레드를 중심으로 구성되어 있었다. 스레드 호출 스택은 thread local을 사용하여 데이터와 컨텍스트를 연결하도록 설계되어 있다. 이 외에도 Exception, Debugger, Profile(JFR)이 모두 스레드를 기반으로 하고 있다. Reactive 스타일로 코드를 작성하면 사용자의 요청이 스레드를 넘나들면서 처리되는데, 이 때문에 컨텍스트 확인이 어려워져 결국 디버깅이 힘들어졌다. 목적 Project Loom 의 결과로 탄생한 가상 스레드는 다음과 같은 목적을 가지고 있는데, 기존의 Reactive Programming 과 비교해서 생각해보자.\n해결하고자 하는 문제 Java 개발자가 하드웨어의 성능을 잘 활용하는 높은 처리량(쓰루풋)의 서버를 작성하는 것\n가상 스레드는 Blocking 이 발생하면 내부적으로 스케줄링을 활용하여 플랫폼 스레드가 그냥 대기하게 두지 않고 다른 가상 스레드가 작업할 수 있도록 한다. 따라서 Reactive programming 의 Non-blcking 과 동일하게 플랫폼 스레드의 리소스를 낭비하지 않는다. 동시에 자바 플랫폼의 디자인과 조화를 이루는 코드를 생성할 수 있도록 하는 것\n기존 Reactive programming 의 장점에도 불구하고 전통적인 자바 언어의 구조는 스레드를 기반으로 하였기 때문에 Webflux등을 사용할 때 디버깅, 성능테스트가 어려웠다. 하지만 가상 스레드는 기존 스레드 구조를 그대로 사용하기 때문에 디버깅, 프로파일링등 기존의 도구도 그대로 사용할 수 있다. Reactive Programming 과의 비교 Reactive programming 이 달성하고자 하는, 리소스를 효율적으로 사용하여 높은 처리량(throughput)을 감당하려는 목적은 동일하다.\n가상 스레드를 사용하면 Non-blocking 에 대한 처리를 JVM 레벨에서 담당해준다.\n따라서 Spring Web MVC 스타일로 코드를 작성하더라도 내부에서 가상 스레드가 기존의 플랫폼 스레드를 직접 사용하는 방식보다 효율적으로 스케줄링하여 처리량을 높일 수 있다.\n결론적으로 가상 스레드 는 기존 스레드 방식의 이점을 누리면서도 Reactive programming의 장점을 취할 수 있다.\n구조 그럼 어떻게 가상 스레드 가 이런 목표를 달성할 수 있는지 그 구조를 살펴보자.\n플랫폼 스레드와 가상 스레드의 구조 차이 앞서 플랫폼 스레드는 OS 스레드를 감싼 것이라고 설명했다. 애플리케이션 코드가 플랫폼 스레드를 사용하면 실제로는 OS 스레드를 사용하는 것이다. 이 때 사용하는 스레드는 비용이 비싸기 때문에 스레드 풀 을 사용하여 접근하는 방식으로 사용해왔다.\n이에 반해 가상 스레드는 OS 스레드를 감싼 구조가 아니기 때문에 애플리케이션 코드는 가상 스레드 풀 없이 사용하고 JVM 자체적으로 가상 스레드를 OS 스레드와 연결하는 스케줄링한다. 이 작업을 mount / unmount 라고 하고 기존에 플랫폼 스레드라고 하던 부분을 Carrier 스레드라고 한다. (가상 스레드를 실제 OS 스레드로 연결해준다는 의미)\n구조적으로 보자면 OS 스레드를 사용하기 전에 하나의 레이어가 더 있는 것 처럼 보인다. (가상 스레드 스케줄링) 하지만 이 자체적인 스케줄링을 통해서 큰 차이가 발생한다. 기존의 스레드는 Blocking 이 발생하면 그냥 기다려야 했는데, 가상 스레드는 Blocking 이 발생하면 내부의 스케줄링을 통해서 실제 작업을 처리하는 Carrier 스레드는 다른 가상 스레드의 작업을 처리하면 된다. 따라서 Non-blocking 이 누리는 장점을 동일하게 누릴 수 있다. 이를 도식화 하면 다음과 같다.\n다만 위와 같은 구조는 가상 스레드가 수십~수백만까지 늘어날 수 있기 때문에 전통적인 플랫폼 스레드와 동일한 메모리 비용, 컨텍스트 비용이 발생하면 감당하기 어렵다. 따라서 플랫폼 스레드와 가상 스레드는 자원 사용량의 차이가 있다.\n사용하는 자원의 차이 플랫폼 스레드 가상 스레드 메타 데이터 사이즈 약 2kb(OS별로 차이있음) 200~300 B 메모리 미리 할당된 Stack 사용 필요시 마다 Heap 사용 컨텍스트 스위칭 비용 1~10us (커널영역에서 발생하는 작업) ns (or 1us 미만) 사용법 가상 스레드의 배경과, 목적, 구조에 대해서 알아보았으니 실제 가상 스레드 를 사용하는 코드를 실행시켜보자.\n준비과정 SDKMan 을 사용하여 JDK 21을 설치한다.\n$ sdk list java ================================================================================ Available Java Versions for macOS ARM 64bit ================================================================================ Vendor | Use | Version | Dist | Status | Identifier -------------------------------------------------------------------------------- Corretto | | 21 | amzn | | 21-amzn | | 21.0.1 | amzn | | 21.0.1-amzn | | 20.0.2 | amzn | | 20.0.2-amzn | | 20.0.1 | amzn | | 20.0.1-amzn | | 8.0.382 | amzn | | 8.0.382-amzn | | 8.0.372 | amzn | | 8.0.372-amzn Gluon | | 22.1.0.1.r17 | gln | | 22.1.0.1.r17-gln | | 22.1.0.1.r11 | gln | | 22.1.0.1.r11-gln GraalVM CE | | 21 | graalce | | 21-graalce | | 21.0.1 | graalce | | 21.0.1-graalce | | 17.0.8 | graalce | | 17.0.8-graalce | | 17.0.7 | graalce | | 17.0.7-graalce GraalVM Oracle| | 21 | graal | | 21-graal | | 21.0.1 | graal | | 21.0.1-graal | | 17.0.8 | graal | | 17.0.8-graal | | 17.0.7 | graal | | 17.0.7-graal Java.net | | 22.ea.25 | open | | 22.ea.25-open | | 22.ea.16 | open | | 22.ea.16-open | | 21 | open | | 21-open | | 21.ea.35 | open | | 21.ea.35-open | | 21.ea.18 | open | | 21.ea.18-open | | 21.0.1 | open | | 21.0.1-open | | 20.0.2 | open | | 20.0.2-open 여러가지 버전을 설치할 수 있지만 OpenJDK 버전의 최신버전인 21.0.1-open 을 설치해보자\n$ sdk install java 21.0.1-open Downloading: java 21.0.1-open In progress... ################################################################################################ 100.0% Repackaging Java 21.0.1-open... Done repackaging... Cleaning up residual files... Installing: java 21.0.1-open Done installing! Do you want java 21.0.1-open to be set as default? (Y/n): Y Setting java 21.0.1-open as default. $ java --version openjdk 21.0.1 2023-10-17 OpenJDK Runtime Environment (build 21.0.1+12-29) OpenJDK 64-Bit Server VM (build 21.0.1+12-29, mixed mode, sharing) 다음으로 가상 스레드 를 사용하는 코드를 작성해보자. IntelliJ 에서는 2023.2.3 이상부터 Java 21 을 지원하기 때문에 버전을 꼭 확인하자. 새로운 프로젝트를 만들고 JDK 21 을 지정한다. 이제 아래와 같이 Main.java 파일을 작성하자.\nimport java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class Main { public static void main(String[] args) throws Exception { run(); } public static void run() throws Exception { // Virtual Thread 방법 1 Thread.startVirtualThread(() -\u0026gt; { System.out.println(\u0026#34;Hello Virtual Thread\u0026#34;); }); // Virtual Thread 방법 2 Runnable runnable = () -\u0026gt; System.out.println(\u0026#34;Hi Virtual Thread\u0026#34;); Thread virtualThread1 = Thread.ofVirtual().start(runnable); // Virtual Thread 이름 지정 Thread.Builder builder = Thread.ofVirtual().name(\u0026#34;JVM-Thread\u0026#34;); Thread virtualThread2 = builder.start(runnable); // 스레드가 Virtual Thread인지 확인하여 출력 System.out.println(\u0026#34;Thread is Virtual? \u0026#34; + virtualThread2.isVirtual()); // ExecutorService 사용 try (ExecutorService executorService = Executors.newVirtualThreadPerTaskExecutor()) { for (int i = 0; i \u0026lt;3; i++) { executorService.submit(runnable); } } } } 이제 Main.java 를 실행해보자. 다음과 같은 결과를 확인할 수 있다.\nHello Virtual Thread Hi Virtual Thread Hi Virtual Thread Thread is Virtual? true Hi Virtual Thread Hi Virtual Thread Hi Virtual Thread 만약에러가 발생하면 Project Structure, Java Compiler 설정을 꼭 확인하자. IntelliJ 를 사용하지 않고 커맨드라인에서 실행하면 다음과 같다.\n$ java --version openjdk 21.0.1 2023-10-17 OpenJDK Runtime Environment (build 21.0.1+12-29) OpenJDK 64-Bit Server VM (build 21.0.1+12-29, mixed mode, sharing) $ javac Main.java $ ls Main.class Main.java $ java Main Hello Virtual Thread Hi Virtual Thread Hi Virtual Thread Thread is Virtual? true Hi Virtual Thread Hi Virtual Thread Hi Virtual Thread 기존의 스레드(플랫폼 스레드)를 생성하던 문법과 큰 차이가 없이 가상 스레드를 만들 수 있다는 걸 알수 있다. 이번에는 Executors 를 사용하여 10만개의 가상 스레드를 만들고 2초간 대기하도록 한뒤에 전체 실행시간을 측정해보자. Blocking 이 발생하면 다른 가상 스레드가 실행되기 때문에 제대로 동작한다면 약 2초를 조금 넘기는 시간 안에 완료되어야 한다.\nimport java.time.Duration; import java.util.concurrent.Executors; public class Main { public static void main(String[] args) throws Exception { run(); } public static void run() throws Exception { while (true) { long start = System.currentTimeMillis(); try (var executor = Executors.newVirtualThreadPerTaskExecutor()) { // 10만개의 Virtual Thread 실행 for (int i = 0; i \u0026lt; 100_000; i++) { executor.submit(() -\u0026gt; { Thread.sleep(Duration.ofSeconds(2)); return null; }); } } long end = System.currentTimeMillis(); System.out.println((end - start) + \u0026#34;ms\u0026#34;); } } } 실행결과는 다음과 같다.\n2542ms 2590ms 2092ms 2129ms 2125ms 2094ms 2101ms 결과를 보면 알 수 있지만 2초에 가까운 시간이 출력된다. Thread.sleep 에 의해서 Blocking 되었지만 내부 가상 스레드 스케줄러에 의해서 다음 가상 스레드가 실행되기 때문에 전체 처리 시간은 2초에 가깝게 나온다. 이 것을 통해서 Blcoking 코드가 있더라도 가상 스레드를 사용하면 처리량이 늘어날 수 있다는 것을 예상할 수 있다.\n정리 요약 높은 처리량을 높이기 위해서 기존에는 Reactive Programming 과 같은 방식을 사용했지만, 가상 스레드 를 사용하면 Reactive Programming 이 추구하는 Non-blocking을 통한 효율적인 자원 사용이 가능해진다. 가상 스레드 가 JVM 내부에서 알아서 스케줄링 해주기 때문에 가상 스레드 풀 을 사용하지 않는다. Reactive Programming 보다 가독성 좋은 코드를 유지할 수 있고, 기존 스레드와 동일하게 동작하므로 디버깅이 용이하다. 생각해볼 부분 가상 스레드 기능이 추가되었다고 해서 기존의 스레드(플랫폼 스레드)를 사용하지 못하는 것은 아니다. 기존의 스레드도 사용가능하고, 추가된 가상 스레드 도 사용가능하다. 서로 대치되는 것이 아니라 공존하는 것이다. 가상 스레드 를 사용하더라도 응답속도가 빨라지지는 않는다. (오히려 약간 느려질 수도). 다만 처리량이 늘어날 수 있다. 일반적으로 애플리케이션을 개발할 때 스레드를 직접 다루거나 Executors를 사용하는 코드를 많이 작성하지는 않는다. 오히려 기존의 라이브러리들이 가상 스레드를 사용할 수 있도록 개선될 것같다. Reactive Programming 과 같이 높은 처리량을 필요로하는 부분들은 가상 스레드를 사용하는 방식으로 전환될 수도 있을것 같다. 소감 가상 스레드 를 알아보면서 Java 21 버전을 사용한 애플리케이션 코드작성이 기대되기 시작했다. 더욱이 Java 21 버전이 LTS (Long Term Support)로 출시되기 때문에 새로운 프로젝트들은 Java 21 을 기반으로 시작한다면 가상 스레드의 이점을 적용할 수 있을 것이다. 그리고 기존 프로젝트들도 높은 처리량(throughput)을 필요로 하는 경우에 사용하는 Java 버전업을 고려해볼만 하다고 생각된다.\n자료를 정리하면서 Project Loom 에서 가상 스레드 기능을 구현하기까지 5년 넘게 개발했다는 것을 알게되었다. 기존의 스레드 사용성을 해치지 않으면서도 단점만을 개선한 가상 스레드 를 내놓기 까지 얼마나 많은 고민을 했을까 생각해았다. 쉽지 않은 일이었을텐데 결국 이렇게 Java 21에 포함되는 기능을 내놓은 것이 대단하다고 느껴졌다.\n내용이 길어져 다음 글에서 실제 Spring Boot 에 가상 스레드 적용한뒤 실제로 기존 스레드 풀 방식과 비교하여 처리량이 늘어나는지, 그리고 가상 스레드를 사용할 때 주의해야할 점에 대해서 추가 내용을 정리해보겠다.\n시리즈 Virtual Thread란 무엇일까? (1) Virtual Thread란 무엇일까? (2) 참고 자료 https://openjdk.org/jeps/444 https://spring.io/blog/2022/10/11/embracing-virtual-threads https://www.infoq.com/articles/java-virtual-threads/ https://dev.to/jorgetovar/virtual-threads-in-java-23mf https://howtodoinjava.com/java/multi-threading/virtual-threads/ https://www.infoq.com/news/2023/04/virtual-threads-arrives-jdk21/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/k8s-api/",
	"title": "K8s Api",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/service-account--role/",
	"title": "Service Account &amp; Role",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/04/09/access-k8s-api-from-pod/",
	"title": "쿠버네티스 클러스터의 POD에서 클러스터 API 사용하기",
	"tags": ["k8s", "kubernetes", "k8s api", "service account &amp; role"],
	"description": "",
	"type": "post",
	"contents": "K8S 클러스터 POD에서 K8S API 사용하기 배경 쿠버네티스(줄여서 k8s라고 부르는) 클러스터에서 애플리케이션을 운영하다보면, 경우에 따라 K8S 클러스터 자체의 API를 사용하고 싶을 때가 있다. 예를 들어 \u0026lsquo;클러스터의 동작 일부를 직접 모니터링 한다던가\u0026rsquo;, \u0026lsquo;동적으로 POD 또는 서비스의 라이프사이클을 관리한다던가\u0026rsquo; 하는 경우가 그렇다. 나의 경우에는 단순히 POD의 목록을 확인하려는 필요가 있어서 API 사용방법을 찾아보았는데, 생각보다 고려할 부분이 많아서 글로 정리해보았다.\n1. K8S 클러스터 살펴보기 K8S 클러스터의 노드 구성 기본적으로 K8S 클러스터는 다음의 2가지 노드로 구분될 수 있다.\nMaster Node Worker Node 필요에 따라서는 더 세분화할 수도 있지만(Ingress Node 라던가, Node 그룹핑으로 나눌 수도 있다), 최소한 위와 같이 2가지로 구분되며, Master Node 가 클러스터 전체를 관리하는 역할을 수행하며 클러스터 API를 제공하고, Worker Node 는 실제 우리가 원하는 Pod 가 실행되는 역할을 수행한다.\nK8S API란? 쿠버네티스 클러스터를 학습하게 되면 배우게 되는 CLI 명령어가 바로 kubectl 이다. 알고보면 이 kubectl 명령어는 K8S 클러스터의 마스터 노드의 REST API를 사용하여 각종 작업을 처리하는데 이 때 사용하는 API가 바로 K8S API 이다.\n2. K8S API를 사용하기 위한 인증방법 이해하기 (로컬에서 호출해보기) kubectl 도 결국 동작원리를 살펴보면 K8S Master Node API를 호출하는 것을 알게되었지만, 아무런 준비없이 K8S API를 호출한다고 해서 클러스터에게 다양한 작업을 처리하도록 지시할 수는 없다. 먼저 기본적인 개념이해를 위해서 로컬에서 다음과 같이 하나씩 확인해보자. (미리 준비되어 있는 k8s 클러스터가 있고, 로컬에서 kubectl 을 사용할 수 있는 환경이라고 가정한다.)\n클러스터의 정보 획득 kubectl 을 사용중이라면 다음과 같이 명령어를 입력하여 클러스터의 정보를 확인할 수 있다.\n$ kubectl cluster-info Kubernetes control plane is running at https://192.168.55.300:8443 # 클러스터의 Control plane 이 Master Node 로 연결되기 위한 주소이다. 클러스터의 API 조회해보기 클러스터의 주소를 확인하였으면, HTTPS 를 사용한다는 점을 알 수 있다. HTTPS 통신을 위해서는 인증서가 필요한데 지금 당장은 이를 확인할 수 없으니 무시하고 CURL로 API를 요청해보자 .\n$ curl https://192.168.55.300:8443 # 그냥 호출해본다. curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. $ curl https://192.168.55.300:8443 -k # insecure 옵션을 추가해서 https 인증서 오류 무시하여 요청 { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;forbidden: User \\\u0026#34;system:anonymous\\\u0026#34; cannot get path \\\u0026#34;/\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { }, \u0026#34;code\u0026#34;: 403 } # 또는 단순하게 \u0026#39;Unauthorized\u0026#39; 라고 응답될 수도 있다. 토큰을 포함하여 요청해보기 ~/.kube/config 파일을 확인해보면 클러스터에 연결하기 위한 token 을 확인할 수 있다. 이 토큰을 복사하여 다음과 같이 직접 curl 을 요청해보면 사용가능한 클러스터 API 목록을 확인할 수 있다.\ncurl -H \u0026#34;Authorization: Bearer JWT_형태의_TOKEN_내용을_이곳에_넣는다\u0026#34; https://192.168.55.300:8443 -k { \u0026#34;paths\u0026#34;: [ \u0026#34;/api\u0026#34;, \u0026#34;/api/v1\u0026#34;, \u0026#34;/apis\u0026#34;, \u0026#34;/apis/\u0026#34;, ..... ] } kubectl proxy 사용해보기 매번 이렇게 힘들게 API를 요청하려면 불편하다. 로컬에서 HTTP 연결을 쉽게 하기 위해서 kubectl 의 proxy 기능을 활용해보자.\n$ kubectl proxy Starting to serve on 127.0.0.1:8001 # 새로운 터미널에서 확인해보자. $ curl http://localhost:8001 { \u0026#34;paths\u0026#34;: [ \u0026#34;/api\u0026#34;, \u0026#34;/api/v1\u0026#34;, \u0026#34;/apis\u0026#34;, \u0026#34;/apis/\u0026#34;, ..... ] } 복잡하게 토큰을 전달하지 않아도 클러스터 API 호출이 가능하다. 반환된 API 목록을 살펴보면서 내가 필요한 API를 선택할 수 있다.\n로컬 호출 정리 K8S API 호출은 기본적으로 HTTP API 이고 이 호출을 위해서는 HTTPS 인증서 확인, JWT 토큰을 활용한 인증처리가 필요하다. 로컬에서는 kubectl proxy 를 통해서 쉽게 확인할 수 있다. 3. POD 에서 K8S API 호출하기 (첫번째) CURL로 API 호출해보기 로컬에서 K8S API 호출을 위한 기본적인 개념을 이해했으니, 이제 POD 안에서 호출하는 방법을 살펴보자.\npod 는 기본적으로 kubectl 과 같은 명령어를 가지고 있지 않다. 따라서 pod 에서 다음의 3가지를 알아내야 한다.\nAPI 서버의 위치 API 서버의 HTTPS 인증서 대응 API 서버에 요청하기 위한 인증토큰 테스트용 POD 구동하기 동작 확인을 위해서 간단한 curl POD를 띄워보자.\nkubectl run curl -it --rm --image curlimages/curl -- sh POD 에서 API 주소 찾기 Pod 에서는 k8s 관련 정보가 환경변수로 제공된다. 다음과 같이 확인해보자.\n$ env | grep KUBERNETES KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_HOST=10.0.0.1 KUBERNETES_SERVICE_PORT_HTTPS=443 ...그 밖의 다른 환경변수들... 정보를 확인하였으면 다음과 같이 호출해보자\n$ curl https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT -k { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;forbidden: User \\\u0026#34;system:anonymous\\\u0026#34; cannot get path \\\u0026#34;/\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { }, \u0026#34;code\u0026#34;: 403 } 클러스터 내부의 DNS 활용하여 API 질의하기 Pod 내부에서 매번 환변경수를 사용하기는 불편하다 이 때는 클러스터에서 제공하는 내부 DNS 를 사용할 수 있다.\n$ curl https://kubernetes -k ... 응답 확인... https 인증서 처리(서버 인원 확인) 매번 -k 옵션을 붙이자니 찜찜하다. 이를 해결해보자. 먼저 k8s 클러스터에서 제공하는 secret 를 확인하자.\n$ ls /var/run/secrets/kubernetes.io/serviceaccount/ ca.crt namespace token 바로 여기에 있는 ca.crt 가 서버 인증서이다. 인증서를 사용해서 접근해보자.\n$ curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes ... 응답 확인... 매번 --cacert 옵션을 주려니 귀찮다. CURL 환경변수로 등록해보자.\n$ export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt 여기까지 실행하면 curl 클라이언트가 API 서버를 신뢰할 수 있게 되었다. 하지만 아직 token 을 전달하지 않았기 때문에 API 응답이 권한이 없다고 나온다. 토큰도 전달해보자.\n$ TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) $ curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://kubernetes/ # 응답 확인... { \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { }, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;forbidden: User \\\u0026#34;system:serviceaccount:default:default\\\u0026#34; cannot get path \\\u0026#34;/\\\u0026#34;\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;pods\u0026#34; }, \u0026#34;code\u0026#34;: 403 } 서비스 어카운트 권한이 필요함 위와 같이 ca.crt, token 을 모두 전달하였을 때 로컬에서 호출했을 때와 동일한 결과를 기대하였지만, 실제로는 그렇지 않다. 그 이유는 POD에 제공된 서비스 어카운트 system:serviceaccount:default:default 에 권한이 부여되어 있지 않기 때문이다. 이를 위해서는 서비스 어카운트와 role 을 부여해야한다.\npod 에서 curl로 API 호출하기 정리 pod 에서 curl 로 API를 호출하려면 내부 dns 를 사용하여 https://kubernetes 로 호출하면된다. 서버 신원 확인을 위한 ca.crt, API 인증을 위한 token 정보는 /var/run/secrets/kubernetes.io/serviceaccount/ 디렉토리에 있다. 하지만 pod 에 연결된 서비스 어카운트에 제대로된 role 이 부여되어 있지 않다면 (권한이 없다면) API를 정상적으로 호출할 수 없고 403 응답을 받는다. 4. 서비스 어카운트와 Role 이해하기 RBAC - Role Based Access Control 초기 버전의 k8s 클러스터는 RBAC가 비활성화 되어 있어서 모든 POD에서 K8S API를 호출할 수 있었다. 그러나 최근의 K8s 클러스터에서는 RBAC가 기본적으로 활성화 되어 있고, 이 때문에 POD 안에서 K8S API를 호출하려면 해당 POD에게 부여된 서비스 어카운트가 API를 호출할 수 있는 Role 을 부여받고 있어야 한다. Role 이라는 것은 말 그대로 어떤 역할을 부여 해준다는 것이며 RBAC는 부여한 역할에 따라서 접근을 제어하는 기능을 말한다.\n서비스 어카운트와 기본 서비스 어카운트 서비스 어카운트는 말 그대로 Account 즉, K8S API 서버에 접근하기 위한 계정을 말한다. Pod 가 구동될 때에는 자동으로 Pod 에게 서비스 어카운트가 부여된다. 부여되는 서비스 어카운트는 다음과 같이 확인할 수 있다.\n$ kubectl get sa # serviceaccoutn 의 약어 NAME SECRETS AGE default 1 7d 서비스 어카운트는 네임스페이스 단위로 관리되며 새로운 k8s 리소스가 생성될 때 서비스 어카운트가 없다면 자동으로 default 서비스 어카운트가 생성된다. 앞서 Pod 안에서 curl 로 API 호출했을 때 403 이 반환되는데 이 때 서비스 어카운트를 다음과 같이 표현한다는 것을 알 수 있다.\nsystem:serviceaccount:\u0026lt;namespace\u0026gt;:\u0026lt;service account name\u0026gt; system:serviceaccount:default:default 새로운 서비스 어카운트 생성하기 system:serviceaccount:default:default 서비스 어카운트는 default 네임스페이스의 모든 pod 가 공통으로 사용하는 서비스 어카운트다. API 호출이 필요한 POD를 위한 새로운 서비스 어카운트를 생성해보자.\n$ kubectl create serviceaccount my-serviceaccount serviceaccount/my-serviceaccount created $ kubectl describe serviceaccount my-serviceaccount Name: my-serviceaccount Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: my-serviceaccount-token-5wch8 Tokens: my-serviceaccount-token-5wch8 Events: \u0026lt;none\u0026gt; Role 을 생성하여 권한 부여하기 서비스 어카운트만으로는 앞서 기본으로 제공된 default:default 서비스 어카운트와 다를게 없다. 권한을 부여하기 위한 Role 을 생성하고 이 Role 을 서비스 어카운트에 연결해보자. 이 과정을 RoleBinding 이라고 한다.\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: role-for-k8s-api-call namespace: default rules: - apiGroups: - \u0026#34;\u0026#34; # 리소스에 그룹을 지정할 수 있으나, 여기에서는 사용하지 않으므로 \u0026#34;\u0026#34; 으로 입력한다. resources: - pods # 어떤 리소스를 확인할 것인지 지정할 수 있다 복수형으로 입력해야한다. ex) services, configmaps, secrets ..., 여기에서는 pod 를 대상으로 한다. verbs: # 개별 Pod를 이름으로 가져오고, 목록을 확인할 수 있는 - get - list ( 내용이 길어서 yaml 로 작성했지만 다음과 같이 명령어로도 가능하다.)\nkubectl create role role-for-k8s-api-call --verb=get --verb=list --resource=pods -n default\n작성한 yaml 을 적용하고, 생성한 role 을 서비스 어카운트에 연결한다.\n$ kubectl apply -f role-for-k8s-api-call.yaml role.rbac.authorization.k8s.io/role-for-k8s-api-call created $ kubectl create rolebinding api-call --role=role-for-k8s-api-call --serviceaccount=default:my-serviceaccount -n default rolebinding \u0026#34;api-call\u0026#34; created 서비스 어카운트와 Role 정리 역할에 따라 API 에 접근할 수 있는 체계를 RBAC 라고 한다. K8S API에 접근하기 위한 계정을 서비스 어카운트라고 한다. 서비스 어카운트는 지정되지 않으면 default 가 부여된다. 별도의 서비스 어카운트를 만들고 Role 을 생성하여 연결하는 것을 롤바인딩이라고 한다. 5. 서비스 어카운트 지정하여 Pod 실행하기 자 이제 필요한 서비스 어카운트도 생성하였고, Role 도 생성하여 롤바인딩도 마쳤다. 생성한 서비스 어카운트를 Pod 에 적용해보자.\n서비스 어카운트는 Pod 가 생성될 때 지정되며 도중에 변경할 수 없다. 그리고 Pod 는 하나의 서비스 어카운트만 가질 수 있다. Pod 에 서비스 어카운트를 지정하는건 명령어로는 불가능하다. yaml 파일을 작성하자. 서비스 어카운트가 적용된 Pod 생성 apiVersion: v1 kind: Pod metadata: name: curl-with-serviceaccount spec: serviceAccountName: my-serviceaccount containers: - image: curlimages/curl command: # pod 가 바로 종료되지 않도록 하는 명령어 - sleep - \u0026#34;999999\u0026#34; name: curl-with-serviceaccount $ kubectl apply -f curl-with-serviceaccount.yaml pod/curl-with-serviceaccount created 다시 Pod 내부에서 API 호출하기 다시 pod 안에서 K8S API를 호출해보자.\n$ kubectl exec -it curl-with-serviceaccount -- sh / $ # pod 안으로 들어옴 / $ export TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) # TOKEN 을 변수에 등록 / $ export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt # ca.crt 를 curl 이 신뢰할 수 있도록 curl 변수에 등록 / $ curl -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; https://kubernetes/api/v1/namespaces/default/pods # pod 목록을 조회하는 k8s 클러스터 API 호출 { \u0026#34;kind\u0026#34;: \u0026#34;PodList\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;selfLink\u0026#34;: \u0026#34;/api/v1/namespaces/default/pods\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;184545912\u0026#34; }, \u0026#34;items\u0026#34;: [ { \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bb-5c747f77c8-mw5nw\u0026#34;, \u0026#34;generateName\u0026#34;: \u0026#34;bb-5c747f77c8-\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, .... .... # 전체 pod 목록이 출력된다. } } ] } 이제 원하는대로 Pod 안에서 K8S API를 호출하고 그 결과를 확인할 수 있다.\n6. API Client 사용하기 위와 같이 curl 을 사용하여 Pod 안에서 K8S API를 호출할 수 있지만, 매번 수동으로 호출할 수는 없다. K8S 공식 사이트에서 안내하고 있는 언어별 Client 라이브러리를 사용해보자.\nGo 라이브러리 Python 라이브러리 Java 라이브러리 나의 경우에는 Spring Boot 애플리케이션에서 사용하기 위해서 Java 라이브러리를 선택했고, 코드는 Kotlin 으로 작성하였다. \u0026ldquo;default\u0026rdquo; 네임스페이스의 전체 pod 목록을 확인하는 샘플코드는 다음과 같다.\nval client: ApiClient = Config.defaultClient() Configuration.setDefaultApiClient(client) val api = CoreV1Api() val list = api.listNamespacedPod(\u0026#34;default\u0026#34;, null, null, null, null, null, null, null, null, null, false) for (item in list.items) { println(item.metadata!!.name) } 정리요약 K8S 클러스터 안에서 Pod 가 K8S API를 호출하기 위해서는 서비스 어카운트, Role, RoleBinding 이 필요하다. pod 내부에서 curl 과 같이 직접 raw 하게 호출하려면 token, ca.crt 값을 신경써야 한다. 하지만 K8S 공식 사이트에서 안내하고 있는 API Client 라이브러리를 사용하면 손쉽게 API를 호출하고 그 결과를 확인할 수 있다.\nk8s API를 호출하기 위한 작업들이 복잡하게 나열된것 같지만, 알고나면 크게 어렵지는 않다. 다만 k8s 에 대해서 잘 이해하고 있지 않다면, 개념을 잡는데 시간이 걸린다. 이렇게 또 서비스 어카운트와 Role, RoleBnding 에 대해서 좀 더 이해하게 되었다.\n참고자료 쿠버네티스 API 설명 : https://kubernetes.io/ko/docs/concepts/overview/kubernetes-api/ access cluster api : https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/ kubernetes api reference : https://kubernetes.io/docs/reference/kubernetes-api/ API Client 설명 : https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/#using-official-client-libraries 클라이언트 라이브러리 소개 페이지 : https://kubernetes.io/docs/reference/using-api/client-libraries/ https://kubernetes.io/ko/docs/tasks/run-application/access-api-from-pod/\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/blockhound/",
	"title": "Blockhound",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/03/25/block-hound-for-webflux/",
	"title": "BlockHound - Webflux를 사용할 때 Blocking 코드가 사용되고 있는지 검출하는 방법",
	"tags": ["webflux", "blocking code", "blockhound"],
	"description": "",
	"type": "post",
	"contents": "배경 Spring Webflux 기반의 애플리케이션을 작성할 때에는 모든 코드가 reactive 해야 최상의 성능(처리량)이 나온다. 이 말은 코드 사이에 blocking 코드가 존재한다면 원하는 대로 충분한 성능이 발휘되지 않는다는 뜻이다. 그러나 작성한 코드에 blocking 코드가 존재하는지 확인하기 위해서 일일이 코드를 살펴볼 수는 없다. 그래서 별도의 도구가 필요하다.\nBlockHound BlockHound는 webflux 에서 사용하는 reactor 팀에서 개발한 도구로 애플리케이션에서 blocking 코드가 작성되었는지 여부를 검출해주는 도구이다. 직접 작성한 코드 뿐만 아니라, 서드 파티 라이브러리에서 사용한 블로킹 코드도 전부 검출한다.\n사용법 의존성 추가 먼저 의존성을 추가해주어야 한다. // BlockHound 의존성을 추가한다. // 기본적으로 이 도구는 test 수행시 사용되나, 나의 경우에는 blcking 코드 검출 작업을 위해서 implementation 의존성으로 추가했다. // 버전은 github 에서 알려주는 최신버전을 사용했다. dependencies { ... implementation(\u0026#34;io.projectreactor.tools:blockhound:1.0.7.RELEASE\u0026#34;) ... } 애플리케이션에 적용 // Application Main 함수에서 BlockHound 를 활성화 한다. fun main(args: Array\u0026lt;String\u0026gt;) { BlockHound.install() runApplication\u0026lt;MyApplication\u0026gt;(*args) } blocking 코드 검출 애플리케이션을 구동하고, blocking 코드가 실행되도록 하자. 나의 경우에는 http 요청을 처리하는 spring security filter 로직에서 blocking 코드가 존재했다.\nreactor.blockhound.BlockingOperationError: Blocking call! java.io.RandomAccessFile#readBytes at java.base/java.io.RandomAccessFile.readBytes(RandomAccessFile.java) Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: Error has been observed at the following site(s): *__checkpoint ⇢ Handler my.example.app.controller.SampleApiController#blockingMethod(Continuation) [DispatcherHandler] *__checkpoint ⇢ org.springframework.security.web.server.authentication.AuthenticationWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.authorization.AuthorizationWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.authorization.ExceptionTranslationWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.savedrequest.ServerRequestCacheWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.context.SecurityContextServerWebExchangeWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.authentication.AuthenticationWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.authentication.AuthenticationWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.context.ReactorContextWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.header.HttpHeaderWriterWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.config.web.server.ServerHttpSecurity$ServerWebExchangeReactorContextWebFilter [DefaultWebFilterChain] *__checkpoint ⇢ org.springframework.security.web.server.WebFilterChainProxy [DefaultWebFilterChain] *__checkpoint ⇢ HTTP GET \u0026#34;/api/v1/blocking-code-test\u0026#34; [ExceptionHandlingWebHandler] 작성한 코드에서 blocking 코드가 검출되면 이를 nonblock 으로 대체하는 작업을 진행한다. 만약 라이브러리에서 blocking 코드가 검출된다면 reactive 스타일을 지원하는지 확인하고 대체할 수 있는지 확인하였다.\n동작방식 BlockHound 가 활성화되면 JVM 활성화 되면 바이트 코드레벨을 조작하여 메서드 호출에 다음의 코드를 추가한다.\n// java.net.Socket public void connect(SocketAddress endpoint, int timeout) { reactor.blockhound.BlockHoundRuntime.checkBlocking( \u0026#34;java.net.Socket\u0026#34;, \u0026#34;connect\u0026#34;, /*method modifiers*/ ); 커스터마이징 도메인 로직 또는 의도적으로 추가한 인프라 코드가 아니라, 라이브러리들이 blocking 코드로 검출되어 에러를 유발시키는 경우가 있다. 이런 경우에는 직접적으로 관심있는 코드가 아니기 때문에 에러가 너무 많이 발생하므로 이를 의도적으로 허용하도록 설정할 수 있다.\n대표적으로 Jackson 의 ObjectMapper 가 (jackson-module-kotlin 의존성) blocking 코드를 사용한다고 BlockHound 에러가 발생하는데, 이는 외부 IO를 사용하는 것이 아니기 때문에 허용가능한 수준으로 본다는 github 이슈도 있었다.\nBlockHound 에서 allow 코드 등록 // BlockHound 를 그냥 install 하지 않고 다음과 같이 허용할 대상을 지정해줄 수 있다. BlockHound.install( BlockHoundIntegration { builder: BlockHound.Builder -\u0026gt; builder .allowBlockingCallsInside(ObjectMapper::class.qualifiedName, \u0026#34;readValue\u0026#34;) .allowBlockingCallsInside(ObjectMapper::class.qualifiedName, \u0026#34;canSerialize\u0026#34;) } ) 소감 Reactive 스타일의 코드가 아직 익숙하지 않은 상황에서 blocking 코드를 사용했는지 파악하기 어려운데, BlockHound 를 사용해서 내가 작성한 코드가 blocking 을 유발하는지 확인할 수 있어서 도움이 되었다. 하지만 여전히 reactive 스타일은 적응이 잘 되지 않는다. 🥲\n참고자료 [NHN FORWARD 2020] 내가 만든 WebFlux가 느렸던 이유 - https://forward.nhn.com/2020/session/26 BlockHound의 동작 방식 - https://blog.frankel.ch/blockhound-how-it-works/ WebFlux의 개념 / Spring MVC와 간단비교 - https://devuna.tistory.com/108 "
},
{
	"permalink": "https://findstar.pe.kr/tags/blocking-code/",
	"title": "Blocking Code",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/webflux/",
	"title": "Webflux",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/02/25/environment-to-do-well/",
	"title": "&#39;일을 잘하기 위한 환경&#39;과 리더쉽",
	"tags": ["environment to do well", "leadership"],
	"description": "",
	"type": "post",
	"contents": "일을 잘 하기 위한 환경 며칠 전 이전 조직의 팀장님과 커피를 마시며 근황을 이야기하다가 일을 잘하기 위한 환경과 리더쉽에 대해서 이야기하게 되었다. 올 초부터 새로운 조직으로 이동하여 업무에 적응하고 있었고, 최근 리더쉽에 대한 고민을 하고 있던 터라 귀가 솔깃한 내용이었다. 저절로 고개가 끄덕여지는 공감 가는 부분들이 많아서 대화를 토대로 나의 생각을 정리해보았다.\n일을 잘 한다는 것은 무엇을 의미할까? 제일 먼저 생각해 볼 것은 협업이다. 조직 안에서 개개인의 작업 결과는 조직의 목적에 맞게 연결되어야 제대로된 의미를 가진다고 생각한다. 따라서 협업이 효율적으로 이루어져 \u0026lsquo;1+1 \u0026gt; 2\u0026rsquo; 가 되어야 한다. 따라서 일을 잘 한다는 것 이라는 것은 다시말해 협업이 잘 이루어지는 일 이라고 해석할 수 있다.\n협업을 하기 전에 준비되어야 할 것들 일을 하기 전에 무작정 할 수는 없다. 미리 준비되어 있어야 할 것들이 있다.\n1. 신뢰 - 안정감 기본적으로 구성원들 사이에서의 신뢰가 형성되어 있어야 한다. 협업이라는 것은 결국 사람과 사람과의 관계를 기반으로 하기 때문에 신뢰할 수 있는 동료가 있어야만 원활하게 협업할 수 있다. 몇가지 케이스를 생각해보면,\n조직에 새로운 멤버가 합류하는 경우에는, 다양한 방식으로 신뢰를 형성하는 활동들을 진행한다. 다양한 조직 활동들을 통해서 스킨쉽을 쌓고, 업무를 진행하는 과정에서 상호교류를 통해서 신뢰를 형성할 수 있다. 나는 이런 활동들에서 멤버들 사이의 긴장감을 느끼지 않도록 하는게 필요하다고 생각한다. 꼭 구성원들이 모두 친해지지 않더라도 긴장하지 않고 편하게 대면할 수 있을 정도의 친숙함은 가지고 있어야 한다고 생각한다. 무엇인가 의견을 제시하는 경우에는 두려움이 발언할 수 있으려면 신뢰를 바탕으로 \u0026lsquo;심리적 안정감\u0026rsquo;이 형성되어 있어야 한다. 신뢰가 형성된 조직에서는 내가 실수를 하더라도 동료와 함께 해결해 나갈 수 있다는 믿음이 있기 때문에 구성원 입장에서는 보다 적극적이고, 주도적으로 일을 수행할 수 있다. 따라서 비난받지 않는다는, 지지하고 응원받는다는 심리적 안정감이 있어야 한다고 생각한다. 만약 갈등이 발생하는 경우라면, 조직의 신뢰가 깨어지지 않도록 해소하는 것이 중요하다. 깨어진 신뢰는 회복되기 어렵기 때문이다. 2. 방향성 - 미션 방향성이라는 단어는 조직 내에서 무수히 많이 듣게되는 말이면서도, 항상 추상적이고 뜬구름 잡는 이야기라고 느껴지는 경우가 많다. 사실 그렇게 느껴지는 이유는 바로 조직 구성이 그만큼 복잡하고 계층이 많기 때문이라고 생각한다.\n많은 인원들의 생각을 포용하고, 큰 의미를 가지려고 하다보니, 어느새 마지막 실무레벨에서 바로 인지하기 어려운 수준으로 모호해지는게 아닌가라는 생각이 든다. 따라서 그만큼 개별 조직으로 내려오는 그 방향성에 부합하는 미션이 뚜렷해져야 한다고 생각한다.\n방향성 큰 조직레벨에서의 \u0026lsquo;방향성\u0026rsquo;은 전략적으로 모호함을 띄거나, 다양한 수용성을 위해서 추상적인 단어로 나열되기 쉽다 그 만큼 실무레벨에서의 뚜렷한 목적을 이해할 수 있는 수준으로 미션이 선명해져야 한다. 3. 업무 프로세스 협업을 하기 위해서 또 하나 준비해야할 것은 바로 업무 프로세스에 대한 합의라고 생각한다. 기존 조직에 합류한다면, 기존에 정리된 프로세스를 확인해야하고 새로운 프로젝트를 시작한다면 구성원들이 일을 어떻게 할 것인지에 대한 합의가 필요하다.\n출퇴근 방식도 정해야하는(!?) 하나의 주제일 수 있다. 예를 들어 전면 재택이라던가, 2출근 3재택이라던가 조직에 따라 구성원들이 자율적으로 정할 수도 있고, 이미 정해져 있는 경우도 있다. 주요 커뮤니케이션 도구를 무엇으로 사용하는지. 슬랙, 잔디, 팀즈와 같은 툴부터 내부용 자체 메신저와 같은 도구중 어느것을 메인으로 하고, 서브 커뮤니케이션 툴이 있다면 해당 툴도 가이드가 되어야 한다. 협업 과정에서 다양하게 생산되는 문서와 자료들을 어떻게 관리할 것인가에 대한 정리가 필요하다. 위키나 노션이 주로 많이 사용되는 것 같다. 스케줄 관리 방식 구글 캘린더를 사용하거나, 별도의 스케줄 관리툴을 사용할 수도 있다. 이슈 관리 방식 JIRA와 같은 툴부터, 스프린트/칸반과 같은 관리 방법을 정할 수도 있다. 정기 일정 데일리 미팅의 진행 유무 및 방식, 위클리 스케줄과 같은 공식 일정을 말한다. 개발 환경 개발하기 위한 환경, 언어, 인프라 도구, 릴리즈 프로세스, 테스트 방법등을 말한다. 일이 잘 진행되게 하려면 조직 구성원들이 서로 신뢰가 형성되어 있고, 명확한 미션과 방향성을 세웠다고 가정해보자. 그 뒤에 일을 하기 위한 프로세스도 합의해서 준비되었다고 생각해보자. 그 뒤에 실제 업무를 수행하는 과정에서는 일이 잘 진행되게 하려면 무엇이 필요할까?\n1. 분명한 역할과 책임 해야할일이 무엇이고, 언제까지 어떤 결과물을 만들어 내야할지 각자의 역할과 책임이 명확하게 나눠져 있는 것이 좋다고 생각한다.(흔히 R\u0026amp;R이라고 한다) 물론 업무 과정에서 그 중간 언저리의 모호한 구간이 발생하기 마련이지만, 이 또한 사전에 협의에 대한 룰을 정해놓고 진행하는 것이 좋다고 생각한다.\n2. 빠른 정보공유 조직내에서 의사결정의 배경, 이슈의 맥락과 같은 정보는 빠르게 공유되어야 한다고 생각한다. 물론 과다한 정보가 에너지를 소모시키는 측면이 있지만, 정보가 흐르지 않기 보다는 활발하게 공유되는 것이 더 낫다고 생각한다.\n3. 활발한 커뮤니케이션 정보가 공유되는 과정에서 당연하게도 커뮤니케이션도 활성화 된다. 활발한 커뮤니케이션이 조직 구성원들이 서로서로 협력할 수 있는 환경을 만들어 준다고 생각한다. 때로는 직군을 넘어서 다른 동료에게 아이디어를 주는 행위도 좋다고 생각한다. 주의할 점은 커뮤니케이션에서 상대방을 배려하는 자세가 필요하다는 점이다. 공격적인 말투는 서로의 신뢰를 해치고, 분위기를 경직되게 만든다. 커뮤니케이션에 부담을 느끼지 않게 하기 위해서 유머와 위트가 있어야 한다.\n일을 잘 하기 위한 리더쉽 일이 잘 진행되게 하기 위해서 리더쉽이 중요하다고 생각한다. 리더쉽은 조직의 형태와 리더의 성향에 따라서 다양한 형태가 있다. 나도 여러번의 조직이동을 겪으면서 다양한 리더들을 만났고, 그들 모두 각자의 개성이 반영된 리더쉽을 가지고 있었다. 때문에 항상 어떤 리더쉽 형태가 더 좋다라고 말할수는 없다고 생각한다. 다만 이랬으면 하는 내용들이 있어서 정리해보았다.\n1. 명확한 업무지시 조직의 방향과 미션에 맞추어 개개인에게 역할을 분배하는 행위, 특정 작업 결과물에 대한 기대치에 대한 가능한 상세한 설명이 있을 수록 구성원들이 업무를 빠르고 명확하게 처리할 수 있다고 생각한다.\n2. 위임 잘하기 조직원들이 각자가 맡은 일에 대한 전문가라는 자각을 가지고, 업무를 위임하는 것이 필요하다. 경우에 따라서 리더가 경험치가 더 높은 부분도 있겠지만, 그럼에도 구성원에게 위임하여 결과를 내도록 유도해야한다고 생각한다.\n3. 명확한 상위 Align 조직이 달성하고자 하는 미션은 시간이 지나면서 세부적인 디테일이 변경될 수 있다. 특히나 빠르게 변화하는 시장상황에 따라 업데이트가 잦은 경우도 있다. 이럴 때마다 상위 조직의 목표에 맞게 조직의 방향성을 점검하고 적절하게 공표하는 행위가 필요하다.\n4. 빠른 정보전달 어떤 의사결정이 있다면 그 배경, 이슈가 발생했을 때의 맥락등 리더가 제일 처음 접하게 되는 정보가 많기 때문에 이런 정보를 자주 공유해주는게 좋다고 생각한다. 물론 때로는 모르는게 약일수 있지만, 그래도 공유가 없는것 보다는 많은게 낫다고 본다.\n5. 적절한 피드백 구성원들이 어려움을 겪고 있으면 적절하게 지원해주고, 잘 하는 점은 인정을, 못하는 점은 지적을 해주어야 한다. 너무 잦은 피드백은 마이크로 매니징이 되지만, 피드백이 없으면 방치되는 느낌을 주어 소속감을 해치게 된다.\n6. 조율 업무 도중에 충돌과 갈등, 그리고 예상하지 못한 이슈가 발생할 수밖에 없다. 따라서 이런 상황에 조직의 리더가 갈등을 해소하고, 이슈를 위한 적절한 자원을 분배해야 한다. 필요한 경우 스케줄을 조정하여 조직의 미션을 달성할 수 있도록 해야 한다.\n정리 생각들을 정리해놓고 보니, 리더가 할 일이 많다 라는 생각이 들었다. 물론 리더 한 사람의 몫이라기보다는 조직이 함께 만들어가는 문화의 일부분이라고 생각되지만, 그만큼 리더가 누구냐에 따라서 많이 달라지는 영역들이라 새삼스레 더 리더의 역할이 중요하다고 생각한다.\n최근에 새로운 조직에서 새로운 역할을 부여받아 일을 하는 나의 입장에서는 리더로써의 역할이 점점 더 가까워져 오는 느낌이라 더 크게 와닿았던 대화였다.\n\u0026lsquo;일을 잘 하기 위한 환경\u0026rsquo;을 만들고 그런 환경 위에 조직을 이끌고 성과를 낼 수 있는 리더쉽에 대한 생각을 정리하다 보니 좀 더 명확하게 \u0026lsquo;내가 리더가 된다면\u0026rsquo; 이렇게 해야겠구나 라는 생각들이 이어졌다. 그렇다고는 해도 만약 나에게 리더의 역할이 주어진다면 \u0026lsquo;내가 잘 해낼 수 있을까?\u0026rsquo; 걱정이 앞선다.\n맛있는 커피를 사주신 팀장님의 마지막 말을 떠올리며 두려움을 이겨내 봐야겠다.\n\u0026ldquo;너무 걱정하지 마세요. 부딪치면서 하나하나 해결해 나가면 됩니다. 저도 처음부터 능숙하진 않았어요. 문제가 있으면 문제를 담당하는 사람을 찾아가서 물어보세요. 모르면 모른다고 이야기해도 괜찮아요. 충분히 잘 해내실 거예요.\u0026rdquo;\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/environment-to-do-well/",
	"title": "Environment to Do Well",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/leadership/",
	"title": "Leadership",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/aws/",
	"title": "Aws",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/experience/",
	"title": "Experience",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2023/02/04/aws-develop-experience/",
	"title": "신규 서비스 개발을 위한 AWS 사용 경험기",
	"tags": ["aws", "architecture", "experience"],
	"description": "",
	"type": "post",
	"contents": "AWS를 활용하여 신규 서비스 개발하기 작년 한 해 동안 신규 조직에서 새로운 서비스를 개발하는 미션을 부여받았다. 이전까지는 on prem 를 기반으로한 private cloud를 활용한 인프라스트럭처 구조를 사용했었는데, 이번에는 AWS를 사용하여 서비스를 개발해야 하는 상황이었다. 그동안의 커리어 중에서 AWS를 접해본건 2011년 처음 접해보았고, 2016년도에도 AWS를 기반으로 서비스를 개발해보았었지만, 밑바닥부터 AWS를 사용하여 주도적으로 인프라를 설계한 것은 이번이 처음이었다. 아쉽게도 프로젝트가 도중에 중단되는 바람에 더 이상 서비스 개발을 진행하지는 못했지만, AWS를 사용해서 인프라스트럭처를 설계하고 개발해본 경험을 정리해보았다.\n서비스 요구사항 해외 사용자를 대상으로하는 신규 앱을 위한 백엔드 인프라. REST API와 모바일 앱을 제공하는 것. API 서버 애플리케이션은 K8S 환경위에 돌아가는 구조를 마련하기. 적용 컨셉 빠른 시간내에 인프라 스트럭처를 구성하는 것을 목표로 하였다. 기존 on prem 환경에서 k8s 기반 개발 경험이 있으므로, 이와 유사하게 AWS EKS 기반으로 인프라 구조를 잡기로 했다. 사용한 AWS 서비스 S3 (스토리지) CloudFront (CDN) EKS : K8S 클러스터 사용 EC2 : EKS Worker 로 EC2 인스턴스를 사용하여 직접적으로 사용하지는 않았다. ElasticCache (Redis) : Redis 캐시를 위한 ElasticCache Kinesis, SQS, SNS : Mobile Notification 용도로 사용 Lambda : 이미지 썸네일 생성 및 모니터링 알림 용으로 사용 Route 53 \u0026amp; Certificate Manager : 도메인 및 인증서 관리 인프라 스트럭처 설계한 인프라 스트럭처 요약 보안 계정 관리 정책 root 계정은 사용하지 않고, administrator 계정을 별도로 생성하여 사용하였다. 개발용 계정과, Production 에서 사용하는 AWS Account 분리하여 관리하였다. 개발자 개인 계정을 생성하되, 필요한 권한만 부여하여 권한 부여를 최소화 하였다. 모든 개인 계정은 MFA 활성화 하여 접근하도록 관리했다. 권한 부여 기본적으로 Role Base 권한 체계를 지향하였다. Web Application (Spring boot)이 구동될 때는 EC2 인스턴스에 Role을 부여 하여 AccessKey \u0026amp; Secret 을 직접 사용하지 않도록 하였다. S3 는 퍼블릭 엑세스를 제한하고 Role 을 통한 접근만 허용하였다. 네트워크 AWS 네트워크는 사내 내부망과 DirectConnect 로 연결하였다. 모든 Web Application 은 private subnet 에서 구동되도록 하여 외부에서 접근할 수 없도록 하였다 EKS 는 private api 만 접속을 허용하였다. RDS 는 사내 내부망 or VPC 대역에서만 접근을 허용하였다. VPC 내부에서 AWS managed service 에 접근하기 위한 VPC EndPoint 를 생성하고 이를 위해서 endpoint 전용 subnet을 생성하였다. 고민들 1. VPC 대역정하기 AWS의 시작은 뭐니뭐니 해도 VPC이기 때문에 VPC 설계를 어떻게 해야할지 고민이 되었다. VPC 대역을 얼마나 크게 잡아야 할까? 흔히 VPC 설계 튜토리얼 문서에 나오는 10.0.0.0/16 대역은 너무 크다. 다른 리전에서도 VPC를 활용중이기 때문에 대역이 겹치면 나중에 통신에 문제가 발생한다! (처음에 고민을 잘해야한다) 초기 활용하는 주요 서비스는 EKS 클러스터 이므로 이 클러스터가 운용할 Pods 수량에 맞춰서 잡아야 한다. EKS 클러스터는 관리차원에서 클러스터 버전업을 해야할 때가 있는데, 이 경우 라이브로 production EKS 클러스터를 업그레이드 하는 것은 너무 리스크가 크다. 따라서 신규 EKS 클러스터를 셋업하고 트래픽을 넘기는 방향으로 진행하기로 한다. -\u0026gt; 이를 위한 예비 대역이 필요하다. 2. 인프라스트럭처 (Subnet 구성) VPC 대역을 결정한 다음 subnet 을 어떻게 할것인지 고민을 했다. 프로젝트의 서비스는 흔한 모바일 API 를 제공하는 기능을 필요로 했다. 외부 트래픽을 받는 ALB를 위한 public subnet 이 필요했다. (+Nat gateway) EKS Worker 를 위한 private subnet 이 필요했다. 데이터베이스를 위한 private subnet 을 구성하기로 했다. AWS Managed Service (ElasticCache, S3 등)과 연결하기 위한 VPC Endpoint 용 private subnet 을 구성하기로 했다. 최종적으로 public 1 개, private 3 개로 구성하였다. AZ는 몇개나 사용해야할지 고민했다. 기본적으로 2 AZ는 고려했는데 3개로 할지 2개로 할지 고민되었다. 당연히 3 AZ로 가면 좋겠지만 그만큼 비용이 추가되기 때문이다. AWS aurora가 아닌 RDS mysql 을 사용하기로 해는데 데이터베이스의 write / read replica 설정에 따라 3AZ를 필요로 하기도 했다. 최종적으로는 2AZ로 구성하기로 했다. (3AZ는 과해보여서) 3. EKS 클러스터 사이즈 EKS Node Group 인스턴스 타입 정하기 EKS 클러스터를 구성할 때에는 얼마나 큰 node size를 정해야 하는지 고민이 되었다. 최종적으로는 이전에 작성한 블로그 포스트에서 기록해두었다. 4. 배포파이프라인 배포를 어떻게 할 것인가? 다음으로 배포를 어떻게 할 것인가에 대해서 고민이 했었다. 이건 뭐 정답이 없기 때문에 오히려 더 고민이 되었다. 최종적으로는 on prem 에서 container 이미지를 만들고 만들어진 container 이미지를 ECR 에 push 한 다음 Codebuild 를 트리거 하여 EKS에 rolling update 했다. Code Commit + ECR + Code Build 구성을 할 수도 있었지만, on prem 에서 운영하던 Image Security Scanning 기능을 더 선호하였기 때문에 이렇게 구성하였다. 5. 기타 이미지 썸네일 생성을 위한 방법 서비스를 만들다 보니 이미지 썸네일 생성 기능이 필요했는데 다음의 2가지를 리서치 했다. lambda를 사용하여 이미지 업로드시 미리 생성해두기 on demand 로 생성하기 그 중에서 일단 lambda를 사용하여 이미지 업로드시 미리 생성해 두는 방식으로 했는데 이게 더 간편하고 시간이 덜걸려서 이 방법으로 개발하였다. Mobile App Push Notification 구조 SQS+SNS를 통해서 APNS, Firebase FCM notification을 사용하도록 설계하였다. SNS의 mobile push 를 iOs, Android 용으로 각각 생성하였다. 어려웠던 점 AWS에는 수많은 기능들이 존재한다. 따라서 필요한 기능을 적재적소에 활용할 수 있으면 쉽고 빠르게 인프라를 준비하고 서비스를 개발 할 수 있다. 그렇지만, 그 만큼 방대한 내용을 이해하고 있어야 하는데, 짧은 기간안에 서비스를 만들어 냈어야 하는 나의 입장에서는 그 많은 내용들을 빠르게 이해하고 적용하기가 쉽지 않았다. SA분들과 AWS 파트너의 지원이 없었다면 빠르게 대응하기 어려웠을 것이다. 인프라 스트럭처를 구성하는데 정답이 없다. 따라서 프로젝트의 성격, 처한 상황에 따라서 절절한 스트럭처를 구성해야했는데 이게 계속해서 결정을 해야하는 상황이다 보니 그런 점에 있어서 피로감이 많이 쌓여갔다. 이런 AWS의 문제라기 보다는 우리 팀의 사정이었는데, 서버 개발 인원이 나를 포함하여 6명이었는데 그 중에 AWS 서비스 운영 경험자가 나뿐이었기 때문에 초반에 설계하고 정보 공유하는 역할을 도맡아 하느라 힘든 부분이 있었다. 소감 이번에 AWS를 활용한 인프라 구성을 경험하면서 느낀점이 필요한 기능들이 참 적재적소에 잘 준비되어 있다는 점이었다. 과거 2011년 처음 AWS를 접했을 때는 VPC도 없고, EC2 만 덩그러니 있었던 시절이라, 참 많이 좋아졌구나를 알 수 있었다. AWS에는 무수히 많은 세부 서비스들이 존재한다. 하루가 멀다하고 신규 서비스와, 기능 업데이트 공지가 올라오기 때문에 세부 서비스들에 대해서 모두 알기는 어렵다. 다행히 커뮤니티의 도움과 SA분들의 도움을 받을 수 있었지만, 계속 공부해야 한다는 점이 쉽지는 않다. 그만큼 함께 고민을 나누고 경험을 나눌 수 있는 커뮤니티 활동이 더 중요하다고 생각들었다. 마치며 길다면 길고 짧다면 짧은 기간동안 AWS를 기반으로 밑바닥 부터 인프라를 설계하여 서비스를 런칭해보았다. 앞으로 또 다른 프로젝트에서 다시 사용해보는 경험을 기대해본다.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/database/",
	"title": "Database",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/ulid/",
	"title": "Ulid",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/uuid/",
	"title": "Uuid",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/10/14/resource-id-generation/",
	"title": "리소스의 고유한 식별자는 어떤 형식을 사용해야할까?",
	"tags": ["database", "ulid", "uuid"],
	"description": "",
	"type": "post",
	"contents": "고유한 식별자를 가지는 리소스 일반적인 웹 애플리케이션을 설계하다 보면 많은 Entity, 도메인 모델, 데이터베이스 테이블을 정의하게 된다. 이 때 특정 리소스가 고유한 값을 가져야 되는 요구사항은 너무나도 자주 마주친다. 예를 들어 사용자 정보를 저장하기 위한 ID 라던지 (사용자가 입력하는 UserID와 구분한다.), 작성한 글의 번호 라던지, URL을 통해서 접근하는 대부분의 리소스의 구분자가 모두 고유한 값을 가지고 있어야만 하는 경우라고 할 수 있다.\n이런 리소스를 구분하기 위해서는 고유한 식별자 (Identifier)를 가져야 하는에 이 식별자를 생성하는 방식은 여러가지가 있다.\nAuto Incremental ID - Sequential Auto Incremental ID란?\n데이터베이스가 자동으로 생성해주는 number ID 이다. mysql 기준으로하면 int : -2147483648 ~ 2147483647 ( 4 바이트 ) bigint -9223372036854775808 ~ 9223372036854775807 (8바이트) 의 값을 가진다. 장점\n순차적인 값을 가지고 데이터베이스가 알아서 고유한 id 를 생성해주기 때문에 크게 고민하지 않고 사용할 수 있다. 생성 순서대로 정렬하기 용이하다. 단점\n외부 노출의 문제 : URL을 통해서 외부에 노출 되었을 때 서비스의 주요 지표가 드러나게 되는 상황이 발생할 수 있다. 예를 들어 사용자의 프로필을 조회하는 URL이 https://myservice.app/users/12345와 같이 표현된다면 경쟁사 입장에서는 전체 회원수가 몇명인지 금방 알아낼 수 있다. 새로운 user가 등록될 때마다 id값도 1씩 오르기 때문인데 이를 회피하기 위해서는 auto incremental id 를 노출하지 않도록 신경써야하거나 다른 id 형식을 사용해야한다. 불필요한 내부 정보 추정이 가능해지는 문제 이외에도 보안 취약점을 위한 스캔이나, 웹 공격 대상으로 지정되는 경우 타겟 리소스의 주소가 추정이 가능하기 쉬운 문제도 있다. JPA 쓰기 지연 활용 불가 : 만약 Spring JPA같은 기술을 사용한다면 실제 Identification Key 가 Insert 쿼리가 실행된 이후에 결정되버린다는 특징에 영향을 받는다. 영속성관리 타이밍상 mysql 의 auto increment id 를 사용하면 IDENTITY 식별자 생성 전략(mysql의 auto-increment 등)은 어쩔 수 없이, 쓰기 지연이 동작하지 않고 영속화 할 때 insert 쿼리가 먼저 데이터베이스로 나간다 (즉 JPA의 쓰기 지연 효과를 누릴 수 없다) UUID - Universally Unique Identifier UUID란?\n문자형 ID RFC4122에 정의되어 있는 ID 생성 방식이다.(https://tools.ietf.org/html/rfc4122) 여러가지 버전이 있지만 그 중에서 UUID4를 많이 사용함 ex) 83fda883-86d9-4913-9729-91f20973fa52 128bit(16바이트)를 사용한다. 장점\n위의 Auto incremental ID가 순차적인 특성 때문에 가지는 단점을 극복할 수 있다. 노출되더라도 서비스의 주요 지표가 노출되는 일이 없다. JPA 기술을 사용할 때 소스코드레벨에서 ID를 결정할 수 있기 때문에 쓰기 지연을 활용할 수 있다. 단점\n순차 정렬을 사용할 수 없다. 128비트이기 때문에 int id 값보다 보다 4배나 크다. ID라는 특성상 참조하는 테이블이 많아질 수록 스토리지의 용량이 많이 필요하다. 이에 따라서 메모리 사용량이 커지고, 성능에 영향을 끼칠 수도 있다. percona 블로그 포스트에 따르면 평균 2.5배, 최악의 경우, UUID가 Auto Increment에 비해 28배 느린 경우도 발생할 수 있다고 한다. 인덱스 테이블에서는 기본키를 포인터로 해서 보조 인덱스가 저장되므로 5개의 보조 인덱스의 UUID기반 스키마의 경우 총 6번 저장. 총 1억 개의 테이블에서 216G, 즉 스토리지의 70%를 차지할 수도 있음 (https://percona.com/blog/2019/11/2) 어떤 방식을 선택할 것인가에 대한 고민 각각의 장단점이 있기 때문에 어떤 방식을 선택할지는 상황에 맞게 결정해야한다. 다만 생각해볼 것은, 고민하지 않고 그냥 가장 익숙한 방식을 선택하지는 말자는 것이다. ID가 외부에 노출되기 부담스러운 경우에는 auto incremental id 보다는 UUID 방식이 나을 수 있고, 그 반대라면 auto incremental id 가 더 나을 수도 있다.\n추가적으로 생각해볼 것들 만약 아주 큰 사이즈의 데이터를 담게되는 경우 아주 많은 데이터를 저장해야하는 경우, 그러니까 경험적으로는 데이터의 갯수가 수천만건~1억건 사이에서 부터 데이터를 어떻게 분산해서 저장할지에 대한 고민이 필요해진다. 이럴 때에도 키를 어떤 방식을 사용했느냐에 따라고 고민이 달라질 수 있다.\n분산 데이터베이스에서의 키 관리 spanner 와 같은 분산 데이터베이스를 고려한다면 구글의 아티클에서 권장하는 UUID 방식이 적합해보인다. https://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots Redis 에서 key 를 적극적으로 사용하는 경우 redis와 같은 캐싱 시스템을 활용하는경우 엔티티의 id 값을 저장하게 되는데, 경우에 따라서는 bitmap 구조를 활용할 수도 있다. 리멤버 블로그 - 유저 목록을 Redis Bitmap 구조로 저장하여 메모리 절약하기 그런데 이렇게 bitmap 을 활용하기 위해서는 id 구조가 number id 구조, 즉 auto incremental 인 경우를 가정해야한다. 따라서 UUID 방식이라면 bitmap을 활용한 목록 캐싱은 불가능하다.\n다른 대안은 없을까? ID를 어떤 방식으로 선택할 것인가에 대한 문제는 오랫동안 논의되어온 주제인 만큼 여러가지 대안들도 존재한다.\nHashIDs (https://hashids.org/) 이 선택지의 주요 컨셉은 auto incremental ID를 선택했을 때 외부에 노출되는 것이 문제라면, 외부에 노출될 때 hashing 을 통해서 노출하자라는 것이다. 예를 들면 ID가 12345 일 때 hashing 을 통해서 \u0026lsquo;3sy561e\u0026rsquo; 와 같이 변환해서 노출할 수 있다. 해싱에서는 salt 로 사용되는 값을 정할 수 있는데 이 값을 관리하여 \u0026lsquo;3sy561e\u0026rsquo; 라는 값을 다시 12345 와 같이 변환할 수 있다. 따라서 이 값은 외부에 노출되면 안된다.\nSnowflake 이 선택지의 주요 컨셉은 ID를 생성하는 것 자체가 독립적인 별도의 서비스를 사용해서 처리한다는 것이다. 트위터에서 선택하는 방식으로 알려져있다. UUID에 비해서 절반인 64비트이며, 앞부분에 시간값을 입력하여 정렬도 가능하도록 설계되어 있다. discord, instagram에서 사용했다고 알려졌다. 현재 github 주소는 아카이빙 처리되었다. (https://github.com/twitter/snowflake)\nULID (https://github.com/ulid/spec) Universally Unique Lexicographically Sortable Identifier 이 선택지의 주요 컨셉은 UUID의 단점을 개선하는데 초점이 맞춰져 있다. UUID 128비트 구조와 호환하면서, 정렬이 가능하며, 특수문자를 포함하지 않아 URL에서 사용해도 안전하다. 다양한 언어별 구현체가 있다. nano id (https://github.com/ai/nanoid) UUID 보다 가볍고, URL 친화적인 ID를 생성하는 라이브러리이다. JavaScript 가 메인 타겟이지만, 다양한 언어 구현체도 존재한다. 이 선택지는 사용하는 문자의 범위를 지정하고, 사이즈를 가변적으로 선택할 수 있다는 장점이 있다.\n결론 고유한 식별을 위한 리소스 ID를 선택할 때, 다양한 방식을 선택할 수 있다는 것을 알고 있으면 되겠다. 꼭 어떤 방식이 무조건적으로 선택될 수도 없고, 각각의 상황에 맞게 선택하면 된다고 생각한다. 그리고 그 상황에는 분산처리, 외부 노출의 허용정도, 캐싱에서의 활용, JPA등에서 사용하는 쓰기 지연의 활용등을 고려하여 선택할 수 있다. 물론 지금 만들고 있는 아주 작은 볼륨을 가진 서비스에는 이렇게 생각하는 것이 오버엔지니어링일 수도 있다.\n참고자료 : PK-를-auto-increment자동-증가-할-경우-생기는-문제점 (https://ssunw.tistory.com/entry/14-PK-%EB%A5%BC-auto-increment%EC%9E%90%EB%8F%99-%EC%A6%9D%EA%B0%80-%ED%95%A0-%EA%B2%BD%EC%9A%B0-%EC%83%9D%EA%B8%B0%EB%8A%94-%EB%AC%B8%EC%A0%9C%EC%A0%90) UUID vs Auto Increment (https://velog.io/@qnfmtm666/DevTip-UUID-vs-Auto-Increment) UUID 퍼포먼스 이슈 - percona (mysql 8.0 미만) - (링크) ycombinator.com ULID 토론 (2018.12 시작) - https://news.ycombinator.com/item?id=18768909 sequential-uuid-generators (https://www.2ndquadrant.com/en/blog/sequential-uuid-generators/) ulid-creator (https://github.com/f4b6a3/ulid-creator) google colud spanner 스키마 설계 권장사항 (https://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots) 가상의 상황 트위터 (https://twitter.com/dylayed/status/1496020581610778625 ) JPA Auto incremet 가 포함된 Insert 쿼리는 언제 나갈까? (https://velog.io/@sangwoo0727/JPA-Auto-Increment%EA%B0%80-%ED%8F%AC%ED%95%A8%EB%90%9C-Insert%EC%BF%BC%EB%A6%AC%EB%8A%94-%EC%96%B8%EC%A0%9C-%EB%82%98%EA%B0%88%EA%B9%8C) sharing \u0026amp; IDs at instagram (https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c) hashids (https://hashids.org/) "
},
{
	"permalink": "https://findstar.pe.kr/tags/event-driven/",
	"title": "Event Driven",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/spring-event/",
	"title": "Spring Event",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/09/17/points-to-consider-when-using-the-Spring-Events-feature/",
	"title": "스프링 이벤트 기능을 사용할 때의 고려할 점",
	"tags": ["spring", "spring event", "event driven"],
	"description": "",
	"type": "post",
	"contents": "배경 애플리케이션의 코드를 작성하다보면, 처음에는 간단하게 시작한 도메인 로직이 시간이 지나면서 여러가지 추가 로직이 늘어나 복잡해지는 경험을 하게된다.\n예를 들어서 velog, medium 과 같은 블로그 애플리케이션을 만든다고 가정하고 다음의 코드를 살펴보자.\n// 초기 코드 @Service open class PostService { // ... @Transactional open fun savePost(post: Post) { // post 저장 postRepository.save(post) } } // 시간이 지나고 나서 // 복잡해진 코드 @Service open class PostService { // ... @Transactional open fun savePost(post: Post) { // 추천 시스템을 위한 콘텐츠 풀에 전송 recommendPoolSender.send(post) // 통계 시스템의 카운팅 statisticsCounter.count(post) // 보다 빠른 검색을 위한 ElasticSearch 검색엔진 연동 elasticSearch.indexing(post) // post 저장 postRepository.save(post) } } 위의 코드는 상당한 비약을 섞어 놓은 코드이지만 예시로는 충분하다. 이런 로직들은 핵심 로직과 부가적인 코드가 묶여 있어서 쉽게 분리하기 어려울 때도 있다. 위의 예제를 정리해보면 다음과 같은 비지니스 로직을 표현하고 있다.\n블로그의 포스트가 저장될 때 추천 시스템을 위한 콘텐츠 풀 전송 서비스 전체의 통계를 위한 카운팅 작업 수행 서비스 전체의 글 검색을 위한 ElasticSearch 인덱싱 여기에서 핵심은 블로그의 포스트를 저장하는 로직이고 나머지는 이와 연관된 코드라고 정의할 수 있다. 이를 리팩토링하는데 스프링에서 제공하는 Event 기능을 사용할 수 있다.\nSpring Event 개요 Spring Event란 스프링 프레임워크를 사용할 때 내부에서 데이터를 전달하는 방법 중 하나이다. 이를 사용하면 각각의 코드의 관심사를 분리할 수 있다. 스프링 이벤트 기능은 이벤트를 발생시키고(publish) 이벤트를 수신하는(subscribe)하는 로직을 분리해서 작성할 수 있다. 다음의 로직을 살펴보자.\n@Service open class PostService { // ... @Autowired private lateinit var applicationEventPublisher: ApplicationEventPublisher @Transactional open fun savePost(post: Post) { // post 저장 postRepository.save(post) applicationEventPublisher.publishEvent(PostCreatedEvent(post)) } } //이벤트 리스너. 일반 이벤트 리스너를 사용하여 핵심 로직과 부가로직을 분리시켰다. @Component class PostCreateEventListener { @Autowired private lateinit var recommendPoolSender: RecommendPoolSender @EventListener fun sendContentPool(event: PostCreatedEvent) { recommendPoolSender.send(event.post) } // ... } 핵심은 스프링이 Event를 발생시키고 이를 처리하는 로직(Listener)들 사이에 데이터(Event)를 전달해주는 역할을 해줌으로써 개발자가 각각 분리된 코드를 작성할 수 있다는 것이다. 이를 통해서 초기의 핵심 로직인 블로그 포스트를 저장하는 로직은 간결하게 유지하고 이 이벤트가 발생했을 때 추가적으로 처리해야하는 부가적인 코드들은 Listener 를 통해서 처리할 수 있기 때문에 하나의 Service 클래스(위 예시에서는 PostService) 안에 코드가 계속적으로 증가하는 문제를 해결할 수 있다.\n요약 PostService 는 Post 처리에 집중 부가적인 코드들은 Event Listener 를 통해서 호출 이벤트의 발생과 전달은 Spring 이 해결해줌 이런 구조를 흔히들 pub / sub 구조라고 이야기한다.\n트랜잭션과 이벤트 처리 그런데 위의 코드는 한 가지 고민해보아야 할 문제가 있다. 바로 트랜잭션 안에서 Post 가 저장된다는 사실이다. 따라서 부가적인 코드(추천 연동, 검색 연동, 통계 연동..)는 트랜잭션이 성공적으로 수행된 이후에 실행되어야만 하는데, 위의 코드로는 트랜잭션의 성공적인 수행을 보장할 수 없다는 문제가 발생한다.\n이를 위해서 스프링에서는 EventListener 대신 TransactionalEventListener 제공한다. 말 그대로 트랜잭션 안에서 이벤트를 발생시킬 때 트랜잭션 처리와 결합하여 이벤트를 수신하는 로직을 처리할 수 있다. 따라서 위의 로직은 다음과 같이 변경할 수 있다.\n// 이벤트 리스너. 트렌젝션 이벤트 리스너를 사용했다. // 스프링이 이를 식별할 수 있도록 @Component 어노테이션을 붙여주어야 한다. @Component class PostCreateEventListener { @Autowired private lateinit var recommendPoolSender: RecommendPoolSender @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) fun sendContentPool(event: PostCreatedEvent) { recommendPoolSender.send(event.post) } // ... } @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) 어노테이션을 자세히 보면 phase 옵션이 있는데 이를 해석하자면 AFTER_COMMIT 즉 트랜잭션이 성공적으로 commit 된 이후에 리스너 로직을 처리하라는 의미가 된다.\nphase 값은 다음의 4가지 옵션을 지정할 수 있고, 일반적으로 AFTER_COMMIT 이 적용하기 적합한 경우가 많다.\nAFTER_COMMIT (트랜잭션이 성공했을 때 실행) AFTER_ROLLBACK (트랜잭션 롤백시 실행) AFTER_COMPLETE 트랜잭션 완료시 (AFTER_COMMIT+AFTER_ROLLBACK) BEFORE_COMMIT (트랜잭션 commit 되기전에) 결과적으로 TransactionalEventListener 어노테이션을 사용하면 원하는 대로 트랜잭션 처리를 보장하면서 로직을 분리해서 관리할 수 있게 된다.\n하!지!만! 이 어노테이션의 phase를 AFTER_COMMIT 로 사용할 때 주의하지 않으면 문제가 발생할 수 있다.\n이벤트 리스너의 트랜잭션 처리 주의사항 TransactionalEventListener을 사용하여 이벤트 구조를 도입하여 간결한 코드 구조를 유지할 수 있어서 장점이 있지만, 트랜잭션과 함께 이벤트를 처리할 때 주의사항이 있다. phase 값이 AFTER_COMMIT 으로 정의해놓은 경우 리스너 코드 안에서 다시 트랜잭션을 처리하면 해당 트랜잭션은 커밋되지 않는 현상이 발생한다.\n// ... @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) fun updateCounterForStatistics(event: PostCreatedEvent) { // 통계 시스템의 카운팅 갱신 // 이 코드는 @Transactional 코드 // 정상적으로 트랜잭션이 commit 되지 않는다. statisticsCounter.count(post) } // ... 이 현상에 대한 원인은 TransactionSynchronization 의 afterCommit 주석부분에 설명되어 있다.\n/** * Invoked after transaction commit. Can perform further operations right * \u0026lt;i\u0026gt;after\u0026lt;/i\u0026gt; the main transaction has \u0026lt;i\u0026gt;successfully\u0026lt;/i\u0026gt; committed. * \u0026lt;p\u0026gt;Can e.g. commit further operations that are supposed to follow on a successful * commit of the main transaction, like confirmation messages or emails. * \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;NOTE:\u0026lt;/b\u0026gt; The transaction will have been committed already, but the * transactional resources might still be active and accessible. As a consequence, * any data access code triggered at this point will still \u0026#34;participate\u0026#34; in the * original transaction, allowing to perform some cleanup (with no commit following * anymore!), unless it explicitly declares that it needs to run in a separate * transaction. Hence: \u0026lt;b\u0026gt;Use {@code PROPAGATION_REQUIRES_NEW} for any * transactional operation that is called from here.\u0026lt;/b\u0026gt; * @throws RuntimeException in case of errors; will be \u0026lt;b\u0026gt;propagated to the caller\u0026lt;/b\u0026gt; * (note: do not throw TransactionException subclasses here!) **/ 요약하자면, 이전의 이벤트를 publish 하는 코드에서 트랜잭션이 이미 커밋 되었기 때문에 AFTER_COMMIT 이후에 새로운 트랜잭션을 수행하면 해당 데이터소스 상에서는 트랜잭션을 커밋하지 않는다는 것이다. 따라서 @Transactional 어노테이션을 적용한 코드에서 PROPAGATION_REQUIRES_NEW 옵션을 지정하지 않는다면 (매번 새로운 트랜잭션을 열어서 로직을 처리하라는 의미) 이벤트 리스너에서 트랜잭션에 의존한 로직을 실행했을 경우 이 트랜잭션은 커밋되지 않는다.\n매뉴얼을 안읽으면 이렇게 되기 쉽다. 이를 해결할 수 있는 추가적인 방법이 필요하다.\n문제 해결 방법 위 문제를 해결하는 첫 번째 방법으로는 AFTER_COMMIT 이후에 동일한 데이터소스를 사용하지 않는 방법이 있다. 이를 위해서는 이벤트 리스너를 별도의 스레드에서 실행하는 방법이 있다. 바로 @Async 어노테이션을 추가하는 방법이다.\n// ... @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) @Async fun updateCounterForStatistics(event: PostCreatedEvent) { // 통계 시스템의 카운팅 갱신 // 이 코드는 @Transactional 코드 // AFTER_COMMIT 이라서 이후의 트랜잭션은 커밋되지 않지만, // @Async 어노테이션으로 인해서 별도의 스레드에서 처리하므로 커밋이 정상적으로 실행됨 statisticsCounter.count(post) } // ... 이렇게 하면 이벤트 리스너 로직이 별도의 스레드에서 실행되어 트랜잭션이 커밋되기 때문에 의도한 결과를 얻을 수 있다. 단 이렇게 하면 테스트코드를 작성하기는 좀 까다로울 수 있다. 그리고 이 방법을 사용하면 EventListener 는 별도의 쓰레드에서 실행되기 때문에 Listener 로직이 실행되는 시간이 사용자의 응답을 느리게 만들지 않는다.\n두 번재 방법으로는 AFTER_COMMIT 대신 BEFORE_COMMIT 을 사용하는 방법이다. 이렇게 되면 커밋이 되기 전에 리스너 로직이 실행되기 때문에 정상적으로 리스너 로직의 트랜잭션이 커밋될 수 있다. 하지만 이 경우 리스너 로직에서 예외가 발생하면 이벤트를 발생시키는 핵심 로직의 트랜잭션에 영향을 줄 수 있기 때문에 주의해서 사용해야한다.\n// ... @TransactionalEventListener(phase = TransactionPhase.BEFORE_COMMIT) fun updateCounterForStatistics(event: PostCreatedEvent) { // 통계 시스템의 카운팅 갱신 // 이 코드는 @Transactional 코드 // BEFORE_COMMIT 이기 떄문에 이벤트를 발생시킨 트랜잭션 안에서 이 로직이 실행된다. statisticsCounter.count(post) } // ... 세 번째 방법으로는 @TransactionalEventListener 어노테이션에 추가로 @Transactional(propagation = Propagation.REQUIRES_NEW) 을 붙여주는 방법이다. 이렇게 하면 이벤트 리스너의 로직 안에서 실행되는 @Transactional 로직을 위한 새로운 트랜잭션이 이전의 트랜잭션과 구분되어 새롭게 시작한다. (Propagation.REQUIRES_NEW가 이를 알려줌) 따라서 이벤트를 발생시킨 트랜잭션과는 별도의 분리된 트랜잭션 안에서 이벤트 리스너 로직이 실행된다.\n// ... @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) @Transactional(propagation = Propagation.REQUIRES_NEW) fun updateCounterForStatistics(event: PostCreatedEvent) { // 통계 시스템의 카운팅 갱신 // 이 코드는 @Transactional 코드 // Propagation.REQUIRES_NEW 옵션에 의해서 아래의 로직을 실행하기 위한 새로운 트랜잭션이 시작된다. // 이전의 트랜잭션은 이벤트 리스너의 시작시점만 알려줄뿐 트랜잭션을 공유하지 않는다. statisticsCounter.count(post) } // ... 결론 스프링 이벤트기능을 사용하여 복잡한 로직을 별도의 리스너 로직으로 분리해서 핵심 로직은 간결하게 유지할 수 있다. 스프링 이벤트의 핵심은 이벤트의 발생(publish)와 이벤트의 처리(listener)를 연결해주는 역할이며 이런 구조를 pub/sub 구조로 이해할 수 있다. 개발자는 이벤트 listener 로직을 필요할 때마다 추가할 수 있으므로 핵심로직은 간결하게, 리스너로 분리된 메서드로 유지할 수 있어서 구조를 이해하기 용이해진다. 트랜잭션과 함께 사용할 때는 AFTER_COMMIT 이 기본값으로 사용되지만, 리스너 로직에서 트랜잭션이 필요한 경우 BEFORE_COMMIT, Propagation.REQUIRES_NEW, @Async 등을 사용하여 처리할 수 있다. 비동기 처리를 사용하여 사용자 응답에 영향을 주고 싶지 않다면 @Async를 사용할 수 있으나, 테스트가 까다로울 수 있다. 더 고민해볼 내용 스프링 이벤트를 사용하여 핵심 로직과 리스너 로직을 분리하고, 이벤트의 전달은 스프링 이벤트가 책임지게 하여 간결한 코드를 유지할 수 있게 되었다. 하지만 추가적으로 더 고민해보아야할 이슈들이 있다.\n이벤트 리스너 로직의 예외처리 및 재처리 이벤트 리스너 로직을 수행하는데 예외가 발생하는 경우 분리된 구조로 인해서 핵심 로직에는 영향을 주지 않을 수 있다. 그래서 리스너 안에서 예외처리에 주의를 기울여야 한다. 그리고 리스너로 처리하는 로직이 핵심로직과 함께 중요도가 높은 로직이라면 별도의 재시도 처리등을 수행해야 하는 경우가 발생한다. 이런 경우는 이벤트 리스너안에 복잡한 처리를 추가하기 어렵다. 따라서 이런 경우라면 메세지 큐를 도입하여 이벤트 발생시 kafka 등으로 메세지를 보내고, 이에 대한 처리는 별도의 분리된 마이크로 서비스가 처리하게 하는 것이 나을 수도 있다.\n메세지 큐 도입시 메세지 발행 실패에 대한 처리 만약 스프링 이벤트를 메세지 큐로의 메세지 발행을 처리하는 용도로 사용한다면, 이후에 연결되는 로직들은 메세지 큐에 의존적이게 되고, 이 메세지 큐는 트랜잭션과 함께 아주 중요한 인프라 자원이된다. 그리고 트랜잭션과 함께 메세지 큐의 발행이 실패하면 안되게 되는데, 이 때 딜레마가 발생한다. 바로 DB 트랜잭션과 메세지 큐 발행이 동시에 성공해야하는데 둘 중 하나만 성공하는 경우에 대한 처리가 어렵다는 점이다. 이런 경우라면 Transactional outbox pattern 같은 기법을 도입하여 이벤트의 발행 신뢰도를 높일 수도 있다. 다만 이렇게 까지 해야하는가에 대한 고민은 비지니스 요구사항과, 애플리케이션의 복잡성, 중요도등을 고려해서 결정해야한다.\n참고자료 Baeldung Srping event : https://www.baeldung.com/spring-events https://kwonnam.pe.kr/wiki/springframework/transaction/transactional_event_listener Transactional outbox pattern : https://microservices.io/patterns/data/transactional-outbox.html https://velog.io/@znftm97/%EC%9D%B4%EB%B2%A4%ED%8A%B8-%EA%B8%B0%EB%B0%98-%EC%84%9C%EB%B9%84%EC%8A%A4%EA%B0%84-%EA%B0%95%EA%B2%B0%ED%95%A9-%EB%AC%B8%EC%A0%9C-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0-ApplicationEventPublisher https://www.youtube.com/watch?v=b65zIH7sDug\u0026amp;t=1s\u0026amp;ab_channel=%EC%9A%B0%EC%95%84%ED%95%9CTech "
},
{
	"permalink": "https://findstar.pe.kr/2022/09/03/accessing-private-rds-instance-using-ssm/",
	"title": "gossm 을 사용하여 Private RDS 인스턴스에 접근하기",
	"tags": ["aws", "rds", "system manager"],
	"description": "",
	"type": "post",
	"contents": "SSM을 사용하여 Private RDS 인스턴스에 접근하기 Private RDS AWS에서 RDS(데이터베이스) 인스턴스를 생성하면 외부에서 접근이 되지 않는 VPC 내부의 Private subnet 에 위치시키는 것이 일반적이다. RDS 인스턴스가 public subnet에 존재하여 public IP를 통해서 외부에서 접근할 수 있게 되면 보안입장에서 좋지 않기 때문이다. 따라서 간단한 웹 애플리케이션을 구동한다고 가정하면 RDS 인스턴스는 다음의 그림처럼 private subnet에 들어간다.\nrds instance in private subnet 로컬에서의 접근 불가 문제 보안을 위해서 private subnet 안에 RDS 인스턴스를 생성한 것 까지는 좋은데, 한 가지 불편한 점이 있다. 바로 나의 로컬 환경에서 RDS 데이터베이스에 접근이 안된다는 점이다. public subnet에 있는 인스턴스에서만 RDS에 접근이 가능하기 때문에 나의 로컬 환경(노트북)에서는 private subnet 안에 있는 RDS 데이터베이스에 한번에 접근이 안되는 것이다. 생각해보면 당연한 것이지만, 개발환경에서 또는 일부 작업을 위해서 종종 RDS 데이터베이스에 직접 접속해야할 필요가 있는데 이 경우 매번 EC2 인스턴스를 거쳐서 접근해야하기 때문에 불편하다.\n로컬 머신에서는 접근이 불가능하다. AWS System Manager - AWS SSM 이런 문제를 좀 더 쉽게 해결하기 위해서 AWS에서는 AWS System Manager(SSM)(https://aws.amazon.com/ko/systems-manager/) 이라는 기능을 제공한다. (AWS System Manager 의 줄임말 인데 ASM 이 아니라 SSM인지는 모르겠다.) SSM 에는 기능이 여러가지가 있지만 그 중에서 Session Manager(https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html) 기능을 활용할 수 있다. 쉽게 요약하자면, public subnet에 있는 EC2 인스턴스에 SSH를 통해서 접근한 다음에 RDS에 접근하는 방법을 간략하게 실행할 수 있게 만들어 놓은 편의 기능정도로 이해할 수 있다. (다른 기능들도 많지만, 이게 제일 많이 사용되는 것 같다.)\nSSM을 사용하기 위한 사전 조건 SSM을 사용하기 위해서는 먼저 다음의 준비사항들이 필요하다.\nSSM-Agent 가 설치된 인스턴스가 있어야 한다. amazone linux image를 기반으로 인스턴스를 생성했다면 이미 설치되어 있다.(링크) 따라서 대부분의 경우에는 특별한 작업이 필요없다. 필요하다면 SSM 을 위한 전용 인스턴스를 생성하기도 한다. (t3.micro 도 충분하다) EC2 인스턴스에 부여된 Role(역할)에 SSM 사용을 위한 AmazonSSMManagedInstanceCore Policy 가 존재해야한다. EC2 인스턴스에 부여된 Role 에 SSM Policy를 추가한다. policy를 추가한 다음에는 ssm agent 재시작이 필요하다. amazone linux 라면 sudo systemctl restart amazon-ssm-agent 다른 인스턴스 이미지라면 매뉴얼 참고 AWS CLI 가 필요하다. https://aws.amazon.com/ko/cli/ aws cli 사용을 위한 aws configure 는 미리 완료해놓아야 한다. SSM 사용하기 SSM을 사용하려면 다음과 같이 입력하면 된다. 뒤에 붙는 target 은 SSM Agent 가 설치된 인스턴스 ID이다.\n\u0026gt; aws ssm start-session --target i-aabbccddeeffgg Starting session with SessionId: aws-cli-user-hhiijjkkllmmnn $ whoami ssm-user $ sudo su -l ubuntu ubuntu@ip-10-232-193-202:~$ SSM을 사용하면 이렇게 ssh를 위한 key 가 없이도 EC2 인스턴스에 접근할 수 있다. 이후에 private subnet에 있는 인스턴스에 접근할 수 있다.\nSSM 에서 리모트 포트포워딩 기능 사용하기 SSM 을 사용해서 에이전트가 설치된 EC2 인스턴스에 접근하는 것만으로는 충분하지 않기 때문에 포트포워딩 기능을 사용할 수 있다. 이 기능을 사용하면 다음과 같이 SSM Agent 가 설치된 인스턴스가 트래픽을 리모트 호스트로 포트포워딩해준다.\naws ssm start-session --target $INSTANCE_ID \\ --document-name AWS-StartPortForwardingSessionToRemoteHost \\ --parameters \u0026#39;{\u0026#34;portNumber\u0026#34;:[\u0026#34;3306\u0026#34;],\u0026#34;localPortNumber\u0026#34;:[\u0026#34;3306\u0026#34;],\u0026#34;host\u0026#34;:[\u0026#34;aws-remote-rds-instance.aabbccddee.ap-northeast-2.rds.amazonaws.com\u0026#34;]}\u0026#39; Starting session with SessionId: aws-cli-user-0c2856a4240a4e548 Port 3306 opened for sessionId aws-cli-user-0c2856a4240a4e548. Waiting for connections... 데이터베이스 커넥션 테스트 이제 로컬 머신의 3306 포트에 연결하면 private subnet 에 있는 RDS 인스턴스(aws-remote-rds-instance.aabbccddee.ap-northeast-2.rds.amazonaws.com)의 3306 포트로 연결된다.\ndatagrip 에서 datasource 추가 localhost 3306으로 연결하지만 실제로는 remote rds에 연결된다. 로컬 커넥션 테스트 OK (실제로 remote로 연결된다.) 트러블슈팅 cli 명령어를 사용해 리모트 호스트로 포트포워딩하려는데 다음과 같이 doesn't support port forwarding to remote hosts 에러가 나오는 경우가 있다.\naws ssm start-session --target $INSTANCE_ID \\ --document-name AWS-StartPortForwardingSessionToRemoteHost \\ --parameters \u0026#39;{\u0026#34;portNumber\u0026#34;:[\u0026#34;3306\u0026#34;],\u0026#34;localPortNumber\u0026#34;:[\u0026#34;3306\u0026#34;],\u0026#34;host\u0026#34;:[\u0026#34;aws-remote-rds-instance.aabbccddee.ap-northeast-2.rds.amazonaws.com\u0026#34;]}\u0026#39; An error occurred (BadRequest) when calling the StartSession operation: The version of the SSM agent installed on this instance doesn\u0026#39;t support port forwarding to remote hosts. Install the latest version of the SSM agent to use port forwarding sessions to remote hosts. 이 에러의 원인은 SSM Agent 의 버전이 너무 낮아서 발생하는 문제이다. \u0026ldquo;AWS Systems Manager \u0026gt; 노드 관리 \u0026gt; 플릿 관리자\u0026rdquo; 에서 버전을 확인할 수 있다.\n설치된 SSM Agent 들의 버전 목록을 확인할 수 있다. 에이전트 버전이 3.1.1374.0 이상이어야만 리모트 호스트로의 포트포워딩이 가능하다. 에이전트의 버전을 올려보자. 제일 쉬운 방법은 수동으로 업데이트를 사용하는 방법이다.\nssm agent 가 설치된 instance 에 명령을 실행한다. ssm agent 가 설치된 instance 에 명령을 실행한다. ssm agent 버전이 올라갔다. gossm 활용하기 aws cli 의 ssm 명령어를 사용하면 리모트 호스트로 접속이 가능하지만, 매번 인스턴스의 ID를 확인하고 명령어도 복잡하고 길어서 불편한점이 있다. 이를 간편하게 해결해주는 gossm 을 사용해보자.\ngossm 설치방법 $ brew tap gjbae1212/gossm $ brew install gossm gossm 사용방법 다음과 같이 gossm fwdrem 명령어를 입력하면 ssm agent 가 설치된 인스턴스 목록이 표시된다.\n$ gossm fwdrem region (ap-northeast-2) ? Choose a target in AWS: [Use arrows to move, type to filter] \u0026gt; my-ssm-agent-micro-instance\t(i-aabbccddeeffgg) some-ec2-instance-a\t(i-aabbccddeeffgg) some-ec2-instance-b\t(i-abcedfghijklmn) some-ec2-instance-c\t(i-abcedfghijklmo) 이 다음에 ssm agent 로 사용할 인스턴스를 선택하면 포트포워딩을 연결할 포트번호(리모트, 로컬), 그리고 연결할 리모트 호스트를 입력받는다.\n$ gossm fwdrem region (ap-northeast-2) ? Choose a target in AWS: my-ssm-agent-micro-instance\t(i-aabbccddeeffgg) ? Remote port to access: 3306 ? Local port number to forward: 3306 ? Type your host address you want to forward to: aws-remote-rds-instance.aabbccddee.ap-northeast-2.rds.amazonaws.com [start-port-forwarding 3306 -\u0026gt; 3306] region: ap-northeast-2, target: i-aabbccddeeffgg Starting session with SessionId: aws-cli-user-0c2856a4240a4e548 Port 3306 opened for sessionId aws-cli-user-0c2856a4240a4e548. Waiting for connections... 그 다음은 aws ssm 명령어와 동일하게 로컬에서 데이터베이스 연결을 수행할 수 있다.\naws 명령과 gossm 모두 aws configure 가 설정된 상태에서 사용이 가능하다.\n결론 결론적으로 ssm (gossm)을 사용하면 private subnet 에 있는 RDS 인스턴스에 접근하여 데이터베이스 작업을 수행할 수 있다.\nssm을 사용하면 기존의 복잡한 ec2 인스턴스를 통한 ssh 연결을 대체하여 손쉽게 인스턴스를 관리할 수 있다 ssh 키를 관리하지 않아도 된다. 리모트 포트포워딩 기능을 사용하여 Private RDS 인스턴스에 접근할 수도 있다. ssm 명령어가 복잡하다면 gossm 을 활용할 수 있다. 참고자료 https://musma.github.io/2019/11/29/about-aws-ssm.html#%EC%8B%A4%EC%8A%B5-aws-system-manager-%ED%99%98%EA%B2%BD-%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0 ssm 포트포워딩 지원 발표(https://aws.amazon.com/ko/about-aws/whats-new/2022/05/aws-systems-manager-support-port-forwarding-remote-hosts-using-session-manager/) gossm AWS SSM Agent 버전 확인 방법 "
},
{
	"permalink": "https://findstar.pe.kr/tags/rds/",
	"title": "Rds",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/system-manager/",
	"title": "System Manager",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/eks/",
	"title": "Eks",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/08/21/which-instance-type-is-right-for-EKS/",
	"title": "EKS를 구축할 때 어떤 인스턴스 타입이 적합할까?",
	"tags": ["aws", "eks", "instance type"],
	"description": "",
	"type": "post",
	"contents": "AWS EKS 에서 인스턴스 타입 선택하기 새로운 프로젝트에 AWS를 사용하기로 하면서 어떻게 인프라를 구성할까 고민했다. 우선 개발팀 멤버 모두가 K8S 환경에 익숙한 상태였기 때문에 제일먼저 EKS를 사용하기로 결정했다. 다른 AWS 서비스들에 대한 구상을 마치고, EKS 클러스터를 설정하기 위한 방법을 알아보았다. 그 중에서 노드를 어떻게 관리할지에 대한 내용을 정리해보았다.\nEKS에서 노드를 관리하는 3가지 방법 EKS에서 노드를 관리하는 방법은 3가지가 있다.\n노드그룹을 통해서 사용하는 방법은 노드(VM-EC2 Instance)를 그룹핑하여 EKS가 관리하게 하는 방법으로 인스턴스의 타입, 인스턴스의 갯수등을 지정하기만 하면 된다. Fargate를 사용하는 방법은 Pod에 정의된 리소스 설정에 맞게 AWS에서 알아서 인스턴스를 배정해준다(1 pod / 1 instance) 두 가지 방법을 섞어서 사용할 수도 있다. 노드를 직접 관리하는 방법은 잘 권장되지 않는데, 워커노드의 K8S 연결을 위한 kubelet, kube-proxy 를 고려해야하기 때문이다. 노드의 직접 관리를 제외하고 두 가지 방법중에 어떤 방법이 더 적합할지 고민하다가 결과적으로 노드 그룹으로만 관리하고 Fargate는 사용하지 않기로 하였는데 그 이유는 다음과 같다.\nFargate는 Pod의 vCPU와 메모리 조합이 정해져 있다. 그래서 일반적인 애플리케이션 구동에는 문제가 없지만, CPU를 많이 사용하거나 메모리가 많이 필요한 애플리케이션을 구동하기에는 적절하지 않다. Node를 직접관리하지 않기 때문에 편하다는 장점이 있지만, Node 에 DaemonSet을 설치하여 노드 자체의 모니터링을 수행하는 등의 작업을 수행할 수 없다. (보안관점에서 별도로 노드를 모니터링 하는 경우가 있다.) DaemonSet 대신 사이드카를 사용할 수 있지만 사이드카 컨테이너가 Pod의 리소스(vCPU, Memory)를 일부 사용하게 된다. 그리고 Fargate는 public subnet 을 사용할 수 없고, NLB을 사용할 수 없다는 점도 고려하였다. 따라서 노드 관리는 \u0026ldquo;노드 그룹\u0026ldquo;을 직접 생성하여 관리하기로 하였다.\n노드 그룹을 통해서 노드를 관리할 때 인스턴스 타입 결정 노드 그룹을 통해서 노드를 관리하고자 결정한 다음에는 어떤 인스턴스 타입을 적용해야할지 고민되었다. 물론 서비스의 특성에 따라, 애플리케이션이 필요로 하는 성능에 따라, 예상 트래픽등 다양한 변수에 따라서 달라지겠지만, 기준점을 정해놓고 싶었다. 먼저 인스턴스의 유형을 다시한번 정리해보았다.\nAWS EC2 인스턴스 유형 EC2 인스턴스는 기능에 따라서, 그리고 가격에 따라 구분된다.\n기능별 구분 : 인스턴스를 표현할 때 흔히 t3.micro, m4.large, c2.medium 와 같이 표현하는데 이런 표현은 기능별 유형을 나타낸다. 예를들어 c2.medium 이라면 c는 인스턴스 family 라고 해서 어떤 특성을 가진 compute 자원인지 나타낸다. 2 는 몇세대 인스턴스인지 표시한다. 여기서는 2세대 인스턴스라는 의미가 된다. 시간이 지남에 따라 AWS에서는 새로운 세대의 인스턴스가 계속 출시된다. (새로운 세대가 나오면 성능이 향상되기 마련이라, 가성비가 좋아진다.) medium은 인스턴스의 사이즈를 나타낸다. 따라서 c2.medium 이라는 표현은 컴퓨팅에 최적화된 2세대 Medium 사이즈의 인스턴스 라는 의미가 된다.\nfamily 구분 M : 범용 C : 컴퓨팅 최적화 G : GPU 최적화 R : 메모리 최적화 .. 기타 여러가지 구분이 더 있다. AWS 인스턴스 타입 비교 페이지(https://aws.amazon.com/ko/ec2/instance-types/) 가격별 구분 : 기능별 구분이외도 가격을 기반으로 구분할 수도 있다. 인스턴스를 기본 생성하면 온디멘드 유형의 인스턴스를 사용하게 된다.\n온디멘드 인스턴스 : 필요할 때 사용하는 기본적인 인스턴스 유형 리저브 인스턴스 : 핸드폰에 비유하면 약정을 걸고 사용하는 유형이다. 2년 혹은 3년 사용을 보장하고 좀 더 저렴한 비용으로 인스턴스를 사용한다. 오래 사용한다면 당연히 좋아보이지만, 새로운 세대의 인스턴스가 나오면 가성비가 떨어질 수 있다는 점을 고려해야한다. 스팟 인스턴스 : 온디멘드와 비교하여 최대 90% 저렴하게 사용할 수 있는 인스턴스. AWS 클라우드 전체로보면 리전 내부에 사용하지 않는 인스턴스 자원이 있을 것이고 이 남는 인스턴스를 저렴하게 사용하는 형태가 바로 스팟 인스턴스 이다. 나의 인프라스트럭처를 모두 스팟 인스턴스로 구성하면 당연히 저렴한 비용으로 인프라를 구축할 수 있지만, 리전내부에 스팟인스턴스 자원이 남지 않는 경우도 있으니 주의가 필요하다. EKS에서 사용할 때의 인스턴스 고려사항 기능과 가격구분에 대해서 이해하였으니 이제 EKS에서 사용할 때의 고려사항을 정리해보았다.\nEKS 쿠버네티스의 Pod는 한 개 이상의 컨테이너를 구성하고 같은 Node의 Network 스택을 공유한다. 그리고 클러스터 내부의 여러 Node에 걸쳐 생성된 Pod은 Overlay Network를 통해 서로 통신한다. 이 말은 Pod는 AWS VPC의 네트워크를 차지한다는 뜻이 된다. 즉 하나의 EC2 인스턴스(노드) 가 차지하는 네트워크 IP가 여러개가 될 수 있다. 그런데 EC2 인스턴스는 타입 사이즈에 따라서 연결할 수 있는 네트워크 인터페이스 (ENI)의 갯수가 정해져있고, 매뉴얼 링크 사이즈에 따라 사용할 수 있는 네트워크 대역폭도 차이가 있다. 따라서 xlarge 이상의 사이즈를 선택해야 POD를 배포했을 때 무리 없이 활용할 수 있다. (너무 작은 사이즈는 EKS 노드로 활용하기 힘들다는 말이 된다. 실제 노드 그룹을 생성할 때 t3.nano 같은 타입은 선택 옵션에 나오지도 않는다.) eks 노드 그룹의 instance 선택 CLI 에서 다음 명령어를 입력하면 테이블 형태로 CPU, Memory, ENI, IP 수를 확인할 수 있다.\naws ec2 describe-instance-types --filters \u0026#34;Name=instance-type,Values=m6*\u0026#34; --query \u0026#34;InstanceTypes[].{Type: InstanceType, MaxENI: NetworkInfo.MaximumNetworkInterfaces, IPv4addr: NetworkInfo.Ipv4AddressesPerInterface, VCpuInfo: VCpuInfo.DefaultVCpus, MemoryInfo: MemoryInfo.SizeInMiB}\u0026#34; --output table ------------------------------------------------------------------ | DescribeInstanceTypes | +----------+---------+-------------+----------------+------------+ | IPv4addr | MaxENI | MemoryInfo | Type | VCpuInfo | +----------+---------+-------------+----------------+------------+ | 30 | 8 | 131072 | m6i.8xlarge | 32 | | 15 | 4 | 16384 | m6g.xlarge | 4 | | 50 | 15 | 262144 | m6g.metal | 64 | | 50 | 15 | 393216 | m6i.24xlarge | 96 | | 10 | 3 | 8192 | m6i.large | 2 | | 30 | 8 | 65536 | m6g.4xlarge | 16 | | 30 | 8 | 65536 | m6i.4xlarge | 16 | | 50 | 15 | 524288 | m6i.metal | 128 | | 30 | 8 | 131072 | m6g.8xlarge | 32 | | 10 | 3 | 8192 | m6g.large | 2 | | 50 | 15 | 262144 | m6i.16xlarge | 64 | | 50 | 15 | 524288 | m6i.32xlarge | 128 | | 50 | 15 | 262144 | m6g.16xlarge | 64 | | 30 | 8 | 196608 | m6g.12xlarge | 48 | | 15 | 4 | 16384 | m6i.xlarge | 4 | | 15 | 4 | 32768 | m6g.2xlarge | 8 | | 30 | 8 | 196608 | m6i.12xlarge | 48 | | 15 | 4 | 32768 | m6i.2xlarge | 8 | | 4 | 2 | 4096 | m6g.medium | 1 | +----------+---------+-------------+----------------+------------+ EKS 에서 사용하기로한 기준 인스턴스 결과적으로 우리 팀에서 사용하기로한 기준 인스턴스 타입은 m6i.xlarge - 온디멘드 유형으로 결정하였다.\n인스턴스 Family 중에서 범용타입인 m 타입으로 결정한다. (현재 애플리케이션 개발 단계라 c, r 타입보다는 범용성을 고려하기로 했다.) m 타입중에서 최신 세대인 6세대를 선택하기로 했다. (가성비가 제일 뛰어나다.) 6세대 m 타입은 다시 m6i, m6a, m6g 로 나뉘는데 각각 intel, amd, arm(Graviton2) cpu를 의미한다. cpu 중에서는 가장 일반적인 intel 을 선택했다. (arm을 써볼까 했지만.. 선택안했다. ) 사이즈는 xlarge 를 기준점으로 삼기로 했다. 덧) arm 인스턴스(graviton)를 사용하지 않는 이유 AWS에서 자체적으로 만든 ARM 기반의 CPU 인스턴스 Graviton은 다른 intel 대비 가성비가 좋다고 설명하고 있다. 하지만 실제 produciton 쓰기에는 다음과 같은 우려점 때문에 선택하지 않았다.\nARM 인스턴스는 별도의 빌드 과정이 필요했다. (우리는 Docker를 기반으로 배포전략을 구성했는데 자체 구축된 Docker build 시스템에서 ARM을 지원하지 않았다.) CPU, Memory 는 인텔 대비 동일했지만, 네트워크 대역폭 지원등이 상대적으로 부족했다. 결론 EKS에서 POD도 VPC 내의 IP를 차지하고 NODE (EC2 Instance) 가 가질 수 있는 네트워크 ENI는 사이즈에 따라 갯수 제한이 있어서 네트워크 관점에서 최소 xlarge 를 선택\n인스턴스 Family는 범용타입인 M 선택\nCPU 유형은 제일 무난한 인텔\n최종 선택은 m6i.xlarge\n기준이 되는 인스턴스를 정했지만, 이 기준은 만들고자 하는 애플리케이션의 특성, 예상 트래픽 정도, 인프라의 구성에 따라 달라질 수 있다. 만약 자신만의 인프라스트럭처를 고민하고 있다면 참고가 되길 바란다.\n참고링크 AWS 인스턴스 타입 비교 페이지(https://aws.amazon.com/ko/ec2/instance-types/) Graviton 소개 페이지 (https://aws.amazon.com/ko/ec2/graviton/) 인스턴스 타입의 네트워크 퍼포먼스 소개 페이지 (https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/general-purpose-instances.html#general-purpose-network-performance) "
},
{
	"permalink": "https://findstar.pe.kr/tags/instance-type/",
	"title": "Instance Type",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/m1-macbook-pro/",
	"title": "M1 Macbook Pro",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/07/31/new-macbook-pro-max-configuration/",
	"title": "새로운 M1 macbook 장비 개인 셋팅",
	"tags": ["m1 macbook pro"],
	"description": "",
	"type": "post",
	"contents": "새로운 맥북 \u0026ldquo;M1 Max\u0026rdquo; 6개월을 기다린 끝에 새로운 맥북을 받았다. 작년 출시 이후에 12월에 MAX로 장비를 신청했는데, 반도체 대란의 영향인지(?!) 대기가 너무 길어져서 결국 7월이 되어서야 새로운 맥북을 받을 수 있었다. M1 프로세서의 좋은점은 귀가 따갑게 들어왔던지라 \u0026ldquo;역시 조용하고 쾌적하군\u0026rdquo; 이라며 행복해 하고 있는데, 그런 감상을 떠나서 새로운 장비가 왔으면 새로운 셋팅을 해야되지 않겠냐며 팔을 걷어부치고 새장비 셋팅에 나섰다. 이전에 사용하던 맥북이 2017년형이라 맥북 환경 설정을 한지는 너무 오래되어서, 새로운 마음으로 설정하면서 내용들을 정리해보았다.\n개인취향의 셋팅 참고로 아래의 애플리케이션과 환경 셋팅은 지극히 개인적인 내용이라, 다른 분들이 적용하기에는 호불호가 있을 수 있음을 먼저 알린다.\n키보드를 설정해보자. 보조키 설정 변경 : 새로운 맥북에서 기본셋팅된 키 설정이 내가 사용하던 설정과 달라서 보조키 설정을 바꿔주었다. (나는 \u0026ldquo;한/A(이전의 CapsLock)\u0026rdquo; 키를 \u0026ldquo;Command\u0026rdquo; 키로 사용한다.) 펑션키 기능 옵션 변경 : F1~F12 까지의 키를 바로 사용할 수 있도록 체크박스를 활성화 하였다. 시스템 환경설정 / 키보드 키보드 / 보조키 보조키 키보드 탐색 컨트롤 포커스 활성화 : 이 옵션을 켜두면 Confirm 또는 Alert 화면에서 탭으로 포커스를 이동할 수 있다. 키보드 탐색 초첨 이동 활성화 애플리케이션을 설치해보자. 이제 자주 사용하는 앱들을 설치해본다.\nFantastical : 유료 캘린더 앱이다. 월 구독료 6500원을 내고 있지만, 개인과 회사 일정을 관리하는데 이만한게 없다. fantastical : 구독료 월 6500원 커뮤니케이션 툴들 : 커뮤니케이션용으로 사용하는 툴들은 제법 많은데 리스트만 나열해보았다. 슬랙 : 대체 몇개의 슬랙 채널을 보고 있는 걸까, 새로 설치한 김에 몇개는 로그아웃 해버렸다. 카카오톡 : 가족과의 대화에 빠질 수 없는 앱 카카오 워크 : 협업하는 몇몇 파트너사들에서 사용하고 있어서 설치했다. 디스코드 : 이건 활동하는 몇몇 커뮤니티에서 사용하고 있어서 설치했다. IDE \u0026amp; Editing : 개발하는데 필요한 IDEA와 코드 에디팅 프로그램이다. Jetbrains ToolBox : 이전 맥북에서는 Jetbrains의 제품을 All Product 라이선스를 구독하고 있는 데다가, 하다보니 \u0026ldquo;IntelliJ\u0026rdquo;, \u0026ldquo;DataGrip\u0026rdquo;, \u0026ldquo;PyCharm\u0026rdquo;, \u0026ldquo;GoLand\u0026rdquo;, \u0026ldquo;PhpStorm\u0026rdquo;, \u0026ldquo;Clion\u0026quot;까지 사용했었다. 새로 설치하면서 \u0026ldquo;IntelliJ\u0026quot;와 \u0026ldquo;DataGrip\u0026rdquo;, \u0026ldquo;Goland\u0026quot;만 남겨놓았다. vscode : 나의 경우에는 간단한 markdown 메모용으로 많이 쓰지만(?!) 가벼워서 애용하고 있다. extension 은 Jetbrains Key Map만 설치해서 사용한다. (난 Jetbrains이 좋더라, Jetbrains에서 가벼운 텍스트 에디터 내놓으면 이것도 사고 싶다.) Note 앱 : 이전에는 Notion, Obsidan, Craft 를 사용했지만, 지금은 Craft 만 남겨놓았다. 생각해보니 이것도 유료 구독제다. craft : 구독료 월 6900원 브라우저 : 브라우저를 깜빡했다. Google Chrome 과 Google Chrome Beta 를 설치하였고 다음은 몇가지 추가한 extension이다. Google Chrome 은 개인용 , Google Chrome Beta 는 회사용이다. 계정을 나눠서 관리하고 있다. Adblock 확장 : 미안하지만 광고가 너무 많다. 유튜브용 Adblock : 여전히 광고는 좀 덜 봤으면. Classic Cache Killer : 웹 페이지 테스트 할 때 캐시를 없애주는 확장기능 Json Formatter : Json 응답을 예쁘게 보여주는 확장 기능 json formatter chrome extension 암호 관리 프로그램 Enpass : 암호 관리용 프로그램으로 Enpass를 사용하고 있다. 구독제로 변경되기전 막차를 타서 Pro 를 한번 구매한 뒤에 평생 사용할 수 있는 권한을 얻었다. chrome extension 도 지원해서 연결해서 편하게 사용하고 있다. 게다가 암호 저장 Vault 를 개인용 아이클라우드에 저장할 수 있어서 더 맘에 든다. enpass Rectangle : 애플리케이션의 윈도우 사이즈를 키보드 단축키로 편하게 이동시킬 수 있는 앱이다. 이전에는 Spectacle을 사용하고 있었는데 더이상 업데이트가 되지 않고 있어서 이번에 변경했다. Rectangle 기본 설정 방식이 Spectacle 과 유사해서 큰 부담없이 적응할 수 있었다. rectangle 터미널 Iterm2 : 하루에도 수십번씩 열어보는 터미널앱 iterm2 Alfred 4 : 기본 내장된 스팟라이트 대신 사용하는 Alfred 4 이다. 새로 설치하려고 보니 Alfred 5 가 나왔는데 그냥 이전 버전 파워팩 라이선스를 가지고 있어서 4 버전으로 설치했다. alfred4 Paw : API 테스팅을 도와주는 도구로 PAW를 사용하고 있다. 다른 분들은 Postman을 많이 쓰던데 나는 그냥 개인 취향으로 이걸 쓰고 있다. paw eul : 맥북의 시스템 자원 모니터링 툴이다. 에전에는 stats를 쓰다가 지금은 eul을 쓰고 있다. 이런 앱은 워낙 종류가 많지만 한글이 잘 나와서 애용하고 있다. 홈페이지 스크린샷이라 한글이 없지만 한글 메뉴 잘 나온다. Steam : 이건 맥이라도 거부할 수가 없다. 맥북이라도 스팀은 피할 수 없다. 터미널 환경 셋팅 필요한 애플리케이션들을 설치했으니 이제 터미널 환경을 설치할 순서다.\nhomebrew 설치 : 처음에는 역시 Homebrew 다. /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; oh-my-zsh 설치 : 테마를 바꾸고 싶어서 oh-my-zsh를 설치했다. sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; Cobalt2 Theme 설정 : 내가 좋아하는 Cobalt2 테마를 설정했다. github 에서 cobalt2.zsh-theme 파일들을 다운받아 ~/.oh-my-zsh/themes/ 디렉토리에 넣어주고 ~/.zshrc 파일의 테마 부분을 ZSH_THEME=cobalt2 와 같이 변경해준다. 일부 디렉토리에서 Git branch 를 예쁘게 보여주기 위한 폰트가 필요하다. powerline 폰트를 설치한다. git clone https://github.com/powerline/fonts cd fonts ./install.sh iterm 설정을 변경해준다. profiles \u0026gt; text profiles \u0026gt; color 에서 cobalt2.itermcolors 파일을 import 한다. IDE 환경 설정 이제 가볍게 IntelliJ 환경 설정을 진행해보았다.\n폰트 변경 : 이제는 좀 큰 글씨가 좋다. 어쩔 수 없는 노화의 영향인가. 사이즈 15라니... ㅠㅠ 힙 메모리 사용량 증가 : IntelliJ 가 사용하는 힙 메모리 량을 늘려주자. vm options 다른 옵션을 더 추가할 수 있지만 일단 Max Heap Memory Size 를 4G로 늘려줬다. (이 기능은 Jetbrains ToolBox 에서도 설정 가능하다) -Xms2048m -Xmx4096m 개인적으로 사용하는 단축키 등록 : 프로젝트 뷰의 포커스를 현재 보고 있는 파일의 위치로 이동시키는 단축키가 없어서 커스컴하게 매크로로 만들어서 사용하고 있다. 참고 : 매크로 생성방법 직접 만들어서 쓰는 단축키 기타 트러블 슈팅 처음에 Jetbrains Toolbox 를 사용해서 IntelliJ 를 설치했는데 왠지 모르게 기대와 다른 느림의 미학을 보여주고 있었다. 순간 향로님의 예전글 M1 맥북 개발환경 세팅 을 본 기억이 있어서 IntelliJ 와 Goland 는 수동 설치했다. ㅠㅠ ToolBox 는 라이선스 관리용도로만 활용되는 현실. 키보드 한영 변경을 \u0026ldquo;Shift + Space\u0026quot;를 사용하고 있었는데 불가능해졌다. Andrew Note - macOS Monterey 업그레이드 후 한영 변환키 설정 [ shift + space key ]를 참고해서 plist 를 변경해주었다. 라이선스와 계정 연결 새로운 애플리케이션을 설치하였더니 새로 실행할 때마다 계정연동 및 라이선스 확인을 새로 해주어야 했다. 일부는 자동으로 소셜로그인을 통해서 해결이 되었지만 일부는 이메일을 뒤져서 라이선스 키를 찾아서 등록해주었다.\n남은 이야기 사실 새로운 맥북을 구동할 때 이전 맥북에서 마이그레이션 옵션을 사용하면 이런 수고로움을 덜 수도 있었겠지만, 왠지 몇년 만에 바꾼 장비다 보니 새롭게 Fresh 한 느낌을 가져보고 싶어서 하나하나 설치해보았다. 게다가 M1 으로 바뀐 뒤에 얼마나 속도가 빨라졌을지 기대하면서 설정 작업들을 진행했는데, 확실히 이전 머신보다 빠르고 조용하고 쾌적했다. (I Love Apple Silicon) 당분간은 새로운 머신에 적응하면서 또 자잘한 설정들을 계속 해나가겠지만, 큰 작업들은 마무리 되었고, 이 참에 새롭게 개발환경을 다듬어 보고 싶어서 위의 작업들만 셋팅하고 나머지는 차차 진행해보기로 마음 먹었다. 여기까지 진행하고 나니, 회사 동료들은 어떤 앱들과 환경을 셋팅해서 사용하실지 궁금해져서 동료들에게 물어보고 싶어졌다. 다음 티타임에 질문을 해보아야겠다. 그럼 이제 스팀을 실행하고 게임을 해봐야겠다. ^^;\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/heap-memory/",
	"title": "Heap Memory",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/07/10/java-application-memory-size-on-container/",
	"title": "컨테이너 환경에서의 java 애플리케이션의 리소스와 메모리 설정",
	"tags": ["java", "heap memory"],
	"description": "",
	"type": "post",
	"contents": "개요 k8s 와 같은 컨테이너 환경에서 jvm 기반 애플리케이션을 배포할 때 메모리 설정에 주의하지 않으면 애플리케이션의 성능에 이슈가 발생할 수 있다. 그래서 적절한 메모리 설정 방법에 대해서 정리해보았다.\n배경 k8s 환경에서 pod를 배포할 때 종종 5XX 에러가 발생하는 경우가 있는데, 원인이 여러가지가 될 수 있지만 (graceful deploy 적용이 되어 있지 않다면 이전 글을 참고해주기를 바란다.) 메모리 설정을 잘못하면 이런 에러가 발생할 수 있어서 관련 설정을 주의해야 한다. 특히 pod 가 간헐적으로 restart 되는 경우가 지속된다면 메모리 설정을 제대로 되어 있는지 확인해볼 필요가 있다.\n기본적인 pod 의 메모리 설정 k8s deployment yml 파일 설정이 다음과 같다면 최대 가용 메모리는 4G 로 설정된다.\ncontainers: - name: my-app image: our-compay-private-repo/our-org/my-app:latest imagePullPolicy: Always resources: limits: cpu: 4000m memory: 4Gi requests: cpu: 4000m memory: 4Gi 주의사항 1. limit 과 request를 동일하게 설정하자. 먼저 확인할 것은 resource 설정에서 limit 과 request 를 동일하게 설정하는 것이다. 처음 resource 설정할 때 실수하기 쉬운 부분 중 하나인데 이 두 값이 같지 않다면 (ex, request 2G, limit 4G 와 같이 설정한다면) 의도하지 않게 pod가 restart 되는 경우가 발생할 수 있다.(에러 메세지는 OOM eviction) 따라서 두 값을 동일하게 설정하기를 권장한다.\nk8s 의 자원할당 QOS 구성은 총 3가지 클래스가 있다. Guaranteed : request = limit Burstable : request \u0026lt; limit BestEffort : request, limit 설정 없음 위에서 언급한 request 와 limit을 같게 설정하라는 의미는 QOS 클래스 중에서 guaranteed를 사용하라는 의미인데, 그 이유는 k8s의 메모리 보장방식 때문이다.\n클러스터에서 Pod를 생성할 때 충분한 메모리가 없으면 우선순위가 낮은 pod를 죽여서 메모리를 보장해주는데, 여기서는 OOM Killer에 대해서 알아야 한다. OOM Killer는 실행 중인 모든 프로세스를 살펴보며 각 프로세스의 메모리 사용량에 따라 OOM 점수를 산출한다. OS에서 메모리가 더 필요하면 점수가 가장 높은 프로세스를 종료시킨다. Pod도 프로세스이기 때문에 여기에 해당하는데 QOS 클래스에 따라서 OOM Killer가 참고하는 oom_score_adj 값이 다르다.\nPod QOS oom score adj Guaranteed -998 Burstable min(max(2, 1000 BestEffort 1000 따라서 Pod QOS 클래스를 Guaranteed 로 설정해놓으면 OOM Killer로 인해서 갑자기 Pod가 죽는 상황은 발생하지 않는다. (갑자기 죽어버린 Pod가 있다면 의심해보자.) 개런티드 QOS 에 대해서 공식 매뉴얼을 참고하길 바란다. 링크\n2. Heap Memory 설정 위와 같이 설정해놓았더라도 pod 가 메모리를 효율적으로 사용하기 어렵다. 높은 확률로 메모리의 낭비가 발생하게 되기 때문에 다음으로 Heap 설정을 살펴보아야 한다. JVM 애플리케이션에서 OOM Exception이 발생한다면 다음과 같은 부분이 잘 설정되었는지 고민해보아야 한다.\nJVM의 Default MaxHeap 사이즈 JVM은 기본적으로 Max Heap 메모리의 사이즈를 물리적인 메모리의 25%를 할당한다. (JVM 10이상부터 컨테이너의 메모리의 25%를 할당한다. 이전 버전에는 버그로 인해서 컨테이너 메모리를 제대로 인식하지 못하는 오류가 있었다.) 따라서 Memory Limit 을 4G로 설정해놓은 상태에서 별도의 설정을 하지 않았다면 Max Heap 사이즈는 1G가 된다. Heap 영역 이외에도 기본적으로 애플리케이션이 구동되기 위한 메모리가 필요하긴 하지만 이를 감안하더라도 4G - Heap memory 1G - (non-heap + system) = 남는 메모리 가 되는 것이다. non-heap + system이 1.5G라고 가정한다면 Pod에서 1.5G는 Java 애플리케이션에서 활용하지 못하고 낭비된다. 이렇게 되면 트래픽이 늘어나는 상황에서 메모리가 부족해지면 애플리케이션 메모리는 Pod 메모리 Limit에 도달하지 못했음에도 Heap 메모리가 부족하여 성능이 안 좋게 되는 현상이 발생할 수 있다. 따라서 MaxHeap 에 대한 설정을 추가해주어야 한다.\nPod 메모리 구성 JVM (Heap) JVM (Non-heap) System IDLE 애플리케이션마다 메모리 사용량이 다를 수 있기 때문에 기본적인 System 메모리의 사용량을 확인하고 Idel을 여유 있게 설정한 다음 Max Heap Size를 늘려서 설정하는 것이 좋다. (최적의 사이즈는 모니터링을 통해서 조절하는 것이 좋다.) 앞선 경우와 같이 Pod Limit이 4G 라면 System + Non Heap + Idel을 포함하여 2G로 설정하고 Max Heap을 2G로 설정한다면 기본 설정보다 Heap Size가 두 배가 되는 것이다.\njava -Xms2048m -Xmx2048m -jar myapp.jar InitialRAMPercentage, MaxRAMPercentage 옵션 사용 그런데 이렇게 매번 Pod의 Limit 에 따라서 JVM 애플리케이션 구동 옵션을 변경시키는 것은 번거롭다. 그래서 Xms, Xmx 옵션대신 InitialRAMPercentage, MaxRAMPercentage 옵션을 사용하는 것이 더 좋다. MaxRAMPercentage 옵션은 말 그대로 지정된 메모리 사이즈 대신 퍼센트를 입력할 수 있다. 따라서 결과적으로는 다음과 같게 설정할 수 있다.\njava -XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=50.0 -jar myapp.jar 단, 낮은 jdk 버전에서는 MaxRAMPercentage 옵션이 정상동작하지 않는 버그가 있다. 다음은 openjdk 11 에서의 MaxRAMPercentage 옵션이 제대로 할당되지 않는 모습을 확인할 수 있다. openjdk 13 에서 수정되었다고 하는데 나의 경우에는 openjdk 17 에서 정상동작 하는 것을 확인할 수 있었다.\n$ docker run -m 4GB openjdk:11 java -Xms2G -Xmx2G -XshowSettings:vm -version VM settings: Max. Heap Size: 2.00G Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;11.0.15\u0026#34; 2022-04-19 OpenJDK Runtime Environment 18.9 (build 11.0.15+10) OpenJDK 64-Bit Server VM 18.9 (build 11.0.15+10, mixed mode, sharing) $ docker run -m 4GB openjdk:11 java -XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=50.0 -XshowSettings:vm -version VM settings: Max. Heap Size (Estimated): 994.00M Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;11.0.15\u0026#34; 2022-04-19 OpenJDK Runtime Environment 18.9 (build 11.0.15+10) OpenJDK 64-Bit Server VM 18.9 (build 11.0.15+10, mixed mode, sharing) $ docker run -m 4GB openjdk:13 java -XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=50.0 -XshowSettings:vm -version VM settings: Max. Heap Size (Estimated): 994.00M Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;13.0.2\u0026#34; 2020-01-14 OpenJDK Runtime Environment (build 13.0.2+8) OpenJDK 64-Bit Server VM (build 13.0.2+8, mixed mode, sharing) $ docker run -m 4GB openjdk:17 java -XX:InitialRAMPercentage=50.0 -XX:MaxRAMPercentage=50.0 -XshowSettings:vm -version VM settings: Max. Heap Size (Estimated): 2.00G Using VM: OpenJDK 64-Bit Server VM openjdk version \u0026#34;17.0.2\u0026#34; 2022-01-18 OpenJDK Runtime Environment (build 17.0.2+8-86) OpenJDK 64-Bit Server VM (build 17.0.2+8-86, mixed mode, sharing) 정리 k8s 와 같은 컨테이너 환경에서 jvm 기반 애플리케이션을 배포할 때 request / limit 리소스 설정을 동일하게 하여 Guaranteed QOS 클래스를 설정하자. 그렇지 않으면 OOM Killer 에 의해서 의도하지 않게 Pod가 restart 되는 경우가 있다. Heap Size에 대한 설정을 고려하자. 그렇지 않으면 리소스가 할당되어 있음에도 충분히 활용하지 못해서 성능저하가 발생할 수 있다. 컨테이너 환경에서는 메모리 할당이 가변적이기 때문에 MaxRAMPercentage 옵션을 사용하는 것이 좋다. 단 jdk 버전이 낮으면 버그로 동작하지 않는다. 리소스와 메모리 설정을 잘 지정하여 의도하지 않게 Pod 가 restart 되거나, 메모리가 남아 있음에도 활용하지 못하고 성능이 저하되는 경우를 겪지 않도록 주의하자.\n참고자료 누가 Kubernetes 클러스터에 있는 나의 사랑스러운 Prometheus 컨테이너를 죽였나! quality of service and oom in kubernetes 파드에 대한 서비스 품질(QoS) 구성 OpenJDK MaxRAMPercentage on a machine with very large amount of memory "
},
{
	"permalink": "https://findstar.pe.kr/tags/backoffice/",
	"title": "Backoffice",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/ux-consideration/",
	"title": "Ux Consideration",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/06/12/backoffice-ux-considerations/",
	"title": "백엔드 개발자의 운영툴 UX 고민들",
	"tags": ["backoffice", "ux consideration"],
	"description": "",
	"type": "post",
	"contents": "백엔드 개발자의 운영툴 고민 벡엔드 개발자로 여러 서비스 개발을 진행하면서 드러나지 않지만 필수적으로 처리해야하는 작업 중에 하나로 운영툴 작업을 생각할 수 있다. 보통 서비스 이용자들과 접점이 있는 화면들이나 API에 대해서는 잘 관리되지만, 그 뒷단에 있는 운영툴-흔히 backoffice / admin / 관리자툴등으로 불리는 이 시스템은 잘 관리되지 않는 경우가 많았다. 이 운영툴 시스템은 서비스가 안정적으로 운영될 수 있도록 각종 정보 조회부터 CS(Custom Support)를 처리하거나, 서비스 통계 / 분석 기능을 제공하기 때문에 잘 관리되어 있다면 운영효율성을 증가시키고, 불필요한 커뮤니케이션을 줄여줄 수가 있다. 그런데 매번 서비스 개발에서 우선순위에서 밀리거나 촉박한 시간 탓에 잘 관리되기가 어려운 경우가 많았다. 그러다 보니 시간이 지나면서 동작하지 않거나 오류가 발생하게 되었지만 방치되고 있는 메뉴, 급하게 붙여넣은 기능으로 인해서 사용하기 어려운 UX, 무엇을 의미하는지 알 수 없는 기능들을 갖게 되는 모습을 볼 수 있었다.\n다음에는 잘 해봐야지 지금은 바쁘니까, 다음에는 잘 해봐야지 하던 생각들을 하곤 했지만, 결국에 다음번에 새로운 운영툴 시스템을 만들 때에도 동일한 패턴이 반복되었다. 그래서 \u0026ldquo;이래서는 안되겠다\u0026quot;라는 마음을 먹고 새로운 프로젝트에 투입되었어 운영툴을 만드는 작업이 주어졌을 때 기존에 단편적으로 생각했던 내용들을 정리해야 겠다고 마음먹었다. 그 동안은 적절한 가이드와 고민들이 없었기 때문에 매번 기능을 쳐내는데 급급했다고 생각해서 고민들을 정리해보았다.\n난 프론트 개발자가 아닌데 물론 나는 백엔드 개발영역이 메인 작업이었기 때문에 프론트엔드 지식은 얕은 수준으로 알고 있었고, 프레임워크도 React 하나만 알고 있었기 때문에 이 가이드가 높은 퀄리티를 갖추고 있다고 생각하지는 않는다. 다만 주어진 상황 속에서 최소한 이정도는 해야지라고 생각했던 고민들의 결과로 정리해보았다.\n프론트엔드 코드를 마주할 때의 나의 심정 운영툴을 만들 때 고민들 먼저 기존의 운영툴 작업을 수행하며 문제가 되었던 부분들을 정리해보았다.\n문제점 원하는 정보를 확인하기 위해서 어떤 메뉴에 접근해야햐는지 알기 어렵다. 이 버튼/메뉴가 어떤 기능을 수행하지는지 알기 어렵다. 방치된 메뉴들이 있거나, 비슷한 기능이 있음에도 새로운 메뉴를 만들어서 기능을 추가한다. 대응 방법 통합적인 검색 기능을 추가하여 원하는 정보의 접근을 쉽게한다. 이해하기 쉬운 UX 가이드를 만든다. 기능들에 대한 설명을 추가하도록 가이드를 만든다. 모든 메뉴를 표시하고 권한에 따라 접근할 수 없도록 한다. : 막상 문제점과 고민들을 정리해보니 어떻게 해야 잘 만드는 것인지 명확하게 생각이 정리되지 않은 상태에서 무작정 작업을 진행하다보니 매번 효과적인 사용성을 가진 운영툴이 만들어지지 않았고 또 시간이 지나면서 유지보수가 어려워지는 상황이 발생했다고 느껴졌다. 그래서 우선적으로 생각들을 정리한 가이드를 만들어서 개발에 참고하도록 하였다.\nUX 가이드 레이아웃 Full Size 활용 무료로 제공되는 오픈소스 Admin Template 들을 살펴보면 대부분이 Full size layout을 사용하는 것을 알 수 있다. 운영툴의 특성상 개별 도메인 정보 하나만을 취급하기 보다는 연관된 데이터를 함께 표시하거나 정보를 확인하고 추가적인 기능을 수행하는 경우가 많기 때문에 UI가 복잡해지기 마련이다. 따라서 단순해보인다는 이유로 width를 제한해버리거나 UI 컴포넌트의 너비를 너무 넓게 잡아버리면 주요 콘텐츠를 표시할 영역자체가 좁아진다. 따라서 가장 먼저 Full size 로 배치하는 것이 좋았다. 넓게 넓게 쓰자 LNB / GNB / RNB 네비게이션 영역을 각 용도에 맞게 활용하는 것이 필요했다. 오픈소스 템플릿에서는 RNB까지 활용되는 예시를 보여주고는 했지만 실제로는 LNB / GNB만 있어도 충분했다. LNB는 메뉴영역으로, GNB는 검색+현재 시스템에 로그인한 사용자 정보를 노출하는 정도로 활용했다. GNB 영역의 컬러를 Development Phase 마다 바꿔서 표시한다. 운영툴도 dev / stage / produciton 단계가 나뉘어져서 개발을 진행하게 된다. 이 때 dev 운영툴에서 수행해야 하는 작업을 실수로 proudction 에서 수행하는 경우가 종종 있었다. 그래서 아예 개발 phase 별로 시각적으로 다르게 인식할 수 있도록 GNB 영역의 컬러를 다르게 지정하도록 했다. local 환경의 GNB production 환경의 GNB 메뉴에 대하여 메뉴 위치 (LNB or GNB) : LNB 운영툴을 만들 때마다 메뉴를 상단(GNB-Global Navigation Bar)에 배치할 것인지 좌측(LNB-Left Navigation Bar)에 배치할지 고민했던 적이 있었다. 두개의 방법 모두 시도해보았었고 나의 결론은 LNB가 적합하다라고 결론을 지었다. 그 이유는 서비스가 오래되었을 때 운영툴에서 필요로 하는 기능이 늘어나면서 메뉴가 처음 생각보다 많아지기 때문이다. 운영툴의 메뉴가 늘어나면서 한 화면에 노출되기 어려운 상황이 발생하는데 이 때 GNB로 배치한 메뉴는 좌우 스크롤이 생길수 있어서 운영툴의 사용자가 좌우 스크롤을 인지하기 어려워 진다. 따라서 늘어나는 메뉴를 생각해서 LNB 메뉴를 구성하는 것이 적합하다고 정리했다. 물론 GNB도 메뉴를 펼쳐서 그룹화하면 좌우 스크롤이 생기지 않게 할 수 있지만, 운영툴을 사용하는 사람들은 대부분 메뉴가 펼쳐져 있는 상태를 좋아했다. (클릭 한번이라도 덜 하는 것을 선호했다) 메뉴의 그룹화와 Depth 메뉴는 그룹화하여 표시하는 것이 좋다. 그리고 depth 를 표시할 때 folding 되는 UI 보다는 펼쳐져 있는 UI를 선호한다. 폴딩되는 UI는 예뻐보이고 깔끔해보일 수 있지만, 운영툴에서 필요한건 한번 이라도 덜 클릭하는 것이 더 중요하다고 생각해서 폴딩 UI는 배제하였다. 그리고 depth 도 메뉴가 많아질것을 고려하여 최소한 2depth, 많으면 3depth 까지 표현할 수 있도록 가이드를 정했다. 메뉴 그룹 현재 확인하고 있는 메뉴가 어느 메뉴인지 Activation 을 표시해주어야 한다. 빠뜨리기 쉬울수 있지만 현재 확인하고 있는 메뉴가 어떤 메뉴인지 활성/비활성 표시가 가능해야한다. 접근권한이 없더라도 모든 메뉴는 존재여부를 알 수 있어야한다. 운영툴을 개발하다보면, 권한에 따라(개발팀/마케팅/CS/다른 협업부서등.) 접근가능한 메뉴가 다르고, 또 일부 기능이 제약이 있을 수 있다. 이런 경우 메뉴 자체를 보여주지 않는 경우도 있는데 이렇게 하면 나중에 기능이 있는지 여부를 알기 어려워 새로운 기능-메뉴를 추가하는 일이 발생하고는 한다. (아..레거시.) 그래서 가급적이면 모든 메뉴는 항상 표시하고 권한이 없는 경우에는 별도로 권한 신청이 필요함을 알리도록 가이드를 정했다. (옵션) 가능하다면 즐겨찾기 배치 가능하다면 Local storage 기능을 활용해서 즐겨찾기 기능을 추가해두면 좋다. 운영툴 시스템의 사용자별로 자주 사용하는 기능이 다르기 때문에 이를 LNB 상단에 배치해주면 이용이 한결 수월해진다. 물론 복잡하게 즐겨찾기 기능을 DB에 저장하거나 하면 API 개발도 필요하고 기능이 복잡해지기 때문에 local storage로 처리하는것이 편했다. 검색기능 GNB 영역에 검색기능을 추가하자. 운영툴에서 꼭 필요한 기능이 검색기능이라고 생각한다. 다만, 여기에서의 검색은 모든 운영툴 콘텐츠를 인덱싱해서 보여주는 통합검색이 아니라 일종의 단축기능-숏컷(shortcut) 이라고 이해하면 좋다. 실제 서비스 이용자의 정보를 조회하거나, 특정 주문 내역을 조회하는 것이 아니라, 12345 가 입력되면 해당 주문내역 혹은 해당 아이디를 가진 사용자의 정보를 조회하는 상세 페이지로 이동시켜주는 일종의 편의기능으로 동작한다. 따라서 몇가지 자주 사용되는 패턴(이메일, 숫자형, 문자형, UUID등)에 대응되는 코드를 작성해두고 운영툴의 모든 페이지에서 이 검색영역이 노출되도록 하였다. 검색바를 사용한 편의기능 콘텐츠의 표시 콘텐츠의 표시는 Title / Description / Body 로 구분한다. 콘텐츠를 표시할 때는 제목과 설명 그리고 주요 정보가 표시되는 영역을 구분하였다. 필요한 경우 Wiki 로 연결되는 기능설명에 대한 별도 문서를 두어 사용자가 도메인 지식을 익힐 수 있도록 정보를 제공하도록 하였다. content layout 페이징 목록을 표시할 때 페이징을 고려해야하는데 기본적으로는 Next / Prev 구조로 가고 꼭 페이지가 필요하지 않도록 가이드를 정했다. 이유는 나중에 표시하는 콘텐츠의 수가 많아졌을 때 (ex 사용자) 페이징이 느려지는 문제가 발생했기 때문이다. 전체 숫자등의 정보는 상시 노출되도록 별도로 가이드를 정했다. 상세 페이지 콘텐츠를 노출할 때 간단한 정보를 보여주는 경우와 / 복잡한 액션 필요한 경우를 구분하여 가이드를 정했다. 예를 들어 간단한 정보라면 body 영역을 Full 로 사용하고, 복잡한 액션이 필요한 경우라면 Body 를 다시 Left / Right 로 구분하여 왼쪽에는 주요 콘텐츠 정보 / 오른쪽에는 이 정보를 기반으로 수행하는 액션 버튼들이 표시되도록 가이드를 정했다. 첫 페이지에서는 무엇을 보여주어야 할까? 운영툴에 로그인을 하고나서 처음에 보여주어야 할 정보는 무엇이어야 할까? 흔히 대시보드 챠트를 생각하기 쉬운데, 경험적으로 대시보드는 그다지 유용하지 않았었다. 통계는 서비스가 개발되면서 보고자 하는 분석 데이터가 달라지기 때문에 URL 설계 가이드 퍼머링크 운영툴의 URL도 퍼머링크를 지원해야한다. 링크가 크게 중요하지 않을 것 같지만, 동료들과의 커뮤니케이션을 할 때에는 URL 자체를 복사해서 전달하기 때문에 동일한 콘텐츠를 동일하게 볼 수 있어야 한다. 따라서 퍼머링크를 지원하여 같은 화면과 무엇인가 선택이 되어 있다면 그런 부분도 동일하게 보여져야 한다고 가이드를 정했다. 세그먼트를 메뉴 그룹을 기반으로 URL 세그먼트를 정할 때 메뉴 그룹을 기반으로 depth 에 따라서 정하도록 하였다. ex) /marketing/banner 이렇게 하면 나중에 URL만 확인해도 어떤 기능인지 짐작할 수 있도록 가이드를 정했다. 정리 벡엔드 API를 만드는게 주 업무라 UX에 대해서 이렇게 고민해본 일이 없었는데 이렇게 정리한 내용을 발전시켜서 다음에는 더 사용하기 편리하고 효율적인 운영이 가능한 운영툴을 만드는데 활용했으면 한다. 물론 여기에 정리한 가이드는 매우 주관적인 경험을 바탕으로 하고 있기 때문에 모든 경우에 적용하기는 어려울 수 있다. 그렇지만 정리한 UX 가이드를 통해서 사용하기 편리한 운영툴을 만들고 관리할 수 있게된다면, 운영업무의 효율성 나아가 커뮤니케이션 비용이 줄어들 수 있다고 생각한다. 그러니 운영툴 작업이 우선순위가 낮아서 매번 나중에 해야지 하고 밀리는 작업이 아닌 나와 동료들의 시간을 아껴줄 수 있는 주요 시스템이라고 인식하고 잘 만들도록 노력하면 좋겠다. 그렇게되면 개발자가 서비스의 핵심 기능을 개발하는데 더 집중할 수 있지 않을까?\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/graceful-deploy/",
	"title": "Graceful Deploy",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/05/27/k8s-graceful-deploy-pod/",
	"title": "K8S 환경에서 pod을 안전하게 배포하는 방법",
	"tags": ["k8s", "graceful deploy"],
	"description": "",
	"type": "post",
	"contents": "개요 k8s 환경에서 pod를 배포할 때 순간적으로 5XX 에러가 발생하는 경우가 있다. 별다른 문제가 아니라고 생각할 수도 있지만, 배포가 진행될 때마다 사용자 중 일부가 오류를 경험할 수 있기 때문에, k8s 옵션을 적절하게 설정하여 5XX에러를 줄이는 것이 좋다. deployment yml 설정에서 이런 옵션들을 살펴보고 안전하게 pod를 종료/시작하는 방법을 살펴보았다.\nStep1. 기본적인 Deployment k8s 환경에서 배포를 위한 방법은 여러가지가 있지만 기본적인 Deployment Yml 은 다음과 같은 형태라고 가정해보자.\napiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: replicas: 4 selector: matchLabels: stage: production app: my-app strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 50% template: metadata: labels: stage: production app: my-app spec: containers: - name: my-app image: my-org/my-app:latest imagePullPolicy: Always ports: - containerPort: 8080 기본적인 Deployment 설정을 하고 나면 배포할 때 마다 순간적으로 5XX 오류를 확인할 수 있다. 이유는 POD는 시작하자마자 트래픽을 처리할 수 없기 때문이다. POD가 정상적으로 트래픽을 받을 수 있는지 없는지 확인하기 위해서 Probe 설정을 추가해보자.\nK8S deployment Step2. Probe 설정 기본적인 Deployment 설정을 했다면 Probe 설정을 추가해야한다.\nlivenessProbe : 컨테이너가 정상적으로 동작중인지 확인하는데 사용한다. 주로 HTTP 요청을 보내서 응답 결과가 정상적으로 200OK 가 반환되는지 확인해서 동작 유무를 판단한다. 응답이 200OK가 아니라면 k8s 클러스터는 pod를 죽이고 재시작하게 된다. 설정하지 않으면 기본값이 Success 로 간주된다. readinessProbe : 컨테이너가 시작할 때 요청을 받을 준비가 되었는지 확인하는데 사용한다. 응답이 200 OK 가 아니라면 k8s 클러스터는 이 pod에 트래픽을 전달하지 않는다. 설정하지 않으면 기본값이 Success 로 간주한다. readinessProbe: httpGet: path: /actuator/health/liveness #굳이 actuator 경로를 쓸 필요는 없다. port: 8080 timeoutSeconds: 3 # 타임아웃 시간 설정 periodSeconds: 30 # 확인 주기 successThreshold: 1 #성공 기준 횟수 failureThreshold: 5 #실패 기준 횟수 livenessProbe: httpGet: path: /actuator/health/readiness #굳이 actuator 경로를 쓸 필요는 없다. port: 8080 timeoutSeconds: 3 periodSeconds: 30 successThreshold: 1 failureThreshold: 5 Probe 라는 건 중학교 건전지 회로 실험에서 사용해보았던, 전류가 흐르는지 확인하는 도구라고 생각하면 이해가 쉽다. 탐침, 조사하다는 의미로 해석할 수 있다. 오늘쪽 바늘 모양 도구가 probe pod 가 실행되어 rediness 가 success 가 되더라도 바로 트래픽을 전달받으면 안되는 상황이 있을 수 있다. Spring 과 같은 경우에는 애플리케이션 내부에서 일종의 Warm up (애플리케이션이 구동하기 위한 준비 작업들)이 더 필요할 수도 있기 때문이다. 특히 단순히 redinessProbe를 path: /_probe/readiness 와 같이 설정한뒤에 200 OK를 반환하는 형태로 설정해둔 경우라면 트래픽의 일부가 여전히 오류를 반환할 수 있다. 이런 경우에는 initialDelaySeconds 를 설정할 수 있다.\n@GetMapping(value = \u0026#34;/_probe/readiness\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; healthCheck() { return ResponseEntity.ok().body(\u0026#34;ok\u0026#34;); } Step 3 initialDelaySeconds 설정 readinessProbe: httpGet: path: /actuator/health/readiness #굳이 actuator 경로를 쓸 필요는 없다. port: 8080 initialDelaySeconds: 10 timeoutSeconds: 3 periodSeconds: 30 successThreshold: 1 failureThreshold: 5 livenessProbe: httpGet: path: /actuator/health/liveness #굳이 actuator 경로를 쓸 필요는 없다. port: 8080 initialDelaySeconds: 10 timeoutSeconds: 3 # 타임아웃 시간 설정 periodSeconds: 30 # 확인 주기 successThreshold: 1 #성공 기준 횟수 failureThreshold: 5 #실패 기준 횟수 initialDelaySeconds 를 설정하면 pod 가 생성되고 난 뒤에 딜레이를 줘서 pod 가 트래픽을 처리할 준비가 된 후에 응답을 반환할 수 있게한다.\n하지만 여기서 끝내면 안되고 한가지 작업이 더 남았다. 바로 종료시점에 대한 고려가 필요하다. pod 가 준비되고 트래픽을 처리하기 까지는 위의 설정으로도 충분하지만, pod 가 종료될 때 즉각적으로 pod를 제거해버리면, 처리하고 있던 응답을 완료하기 전에 프로세스가 종료되어 사용자가 5XX 에러를 볼 수 있다. 이를 막기 위해서 preStop 설정을 추가해보자.\nStep 4 preStop 설정 추가 ports: - containerPort: 8080 lifecycle: preStop: exec: command: [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 10\u0026#34; ] readinessProbe: ... 생략 ... preStop 설정을 추가하면 Pod 가 종료를 처리할 때 즉각적으로 종료하지 않고 preStop 에 설정된 명령을 수행한 뒤에 pod 종료를 수행한다. 따라서 위와 같이 약 10초의 딜레이를 주어서 현재 처리중이던 응답을 반환할 충분한 시간을 주면 사용자에게는 5XX에러 상황이 발생하지 않을 수 있다.\n전체 시퀀스 도식화 말로만 하면 이해가 잘 안될것 같아, 그림으로 도식화 해보았다.\n전체 pod 전환 순서 최종 yml 파일 최종적으로 production 에서 사용하는 deployment yml 파일음 다음과 같은 형태를 가지고 있다.\napiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: replicas: 4 selector: matchLabels: stage: production app: my-app strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 50% template: metadata: labels: stage: production app: my-app spec: containers: - name: my-app image: my-org/my-app:latest imagePullPolicy: Always resources: requests: memory: 2Gi limits: memory: 2Gi env: - name: SPRING_PROFILES_ACTIVE value: \u0026#34;production\u0026#34; ports: - containerPort: 8080 livenessProbe: httpGet: path: /actuator/health/liveness port: 8080 initialDelaySeconds: 10 timeoutSeconds: 3 periodSeconds: 30 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /actuator/health/readiness port: 8080 initialDelaySeconds: 10 timeoutSeconds: 3 periodSeconds: 30 successThreshold: 1 failureThreshold: 5 lifecycle: preStop: exec: command: [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;sleep 10\u0026#34; ] terminationGracePeriodSeconds: 120 결론 K8s에서 pod를 배포하는 과정에서는 pod 가 시작할 때 준비과정(rediness), warmup (initialDelay)에 대한 고민이 필요하고 pod 가 종요할 때 처리하던 응답을 정상적으로 반환할 때까지 기다리기 위한 preStop 과 같은 설정이 필요하다. 이를 잘 이해하고 사용한다면 배포과정에서 사용자가 5XX 에러를 확인하는 일은 없을꺼라고 생각한다.\n참고자료 보다 정확한 내용을 이해하려면 pod 의 라이프사이클에 대한 이해가 필요하다. 다음의 참고자료에서 관련 내용을 확인할 수 있다.\n쿠버네티스 공식문서 - 파드 라이프사이클 도서 \u0026ldquo;Kubernetes in action\u0026rdquo; - 7.2 POD 라이프사이클 , 7.3 모든 클라이언트 요처이 올바르게 처리되도록 보장하기 챕터 "
},
{
	"permalink": "https://findstar.pe.kr/tags/build-cache/",
	"title": "Build Cache",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/05/13/gradle-docker-cache/",
	"title": "Gradle을 사용할 때 도커 빌드를 빠르게 하는 방법",
	"tags": ["docker", "build cache"],
	"description": "",
	"type": "post",
	"contents": "개요 Spring boot 프로젝트 개발할 때 빌드 과정에서 의존 패키지를 다운받는 작업은 시간이 오래 걸리기 때문에 도커 빌드 과정에서 이를 매번 반복하면 전체적인 도커 빌드 시간, 나아가 배포 시간이 길어지는 문제가 있다.\nGradle을 사용할 때 도커 캐시 레이어를 사용해서 이 도커 빌드 타임을 줄이는 방법을 정리해보았다. 만약 Maven을 사용한다면 이전 포스트를 참고하길 바란다.\n기본적인 도커 빌드 과정 Spring Boot 프로젝트를 배포하기 위한 Dockerfile 이 다음과 같을 때 빌드 과정을 살펴보자.\nFROM gradle:7.4-jdk-alpine WORKDIR /app COPY ./ ./ RUN gradle clean build --no-daemon CMD java -jar build/libs/*.jar 기반 이미지에서 gradle 을 사용하여 의존 패키지를 다운받는다. 애플리케이션 구동을 위한 jar 파일이 빌드된다. 문제점 도커 명령어 RUN gradle clean build --no-daemon 을 실행할 때마다 매번 의존 패키지를 다운받기 때문에 시간이 오래걸린다. 나의 경우에는 위의 Dockerfile을 사용해서 간단한 샘플 애플리케이션 코드를 도커 빌드하는데 184초가 걸렸다.\n해결방법 핵심 아이디어 한번 gradle 빌드를 하면 이 다음에 빌드할 때는 의존 패키지는 캐싱해두었다가 다시 사용되면 도커 빌드 타임이 줄어든다. 따라서 의존 패키지의 캐싱방법을 알아보았다.\n대안 1. 별도의 캐시 디렉토리를 지정하는 방법 만약 jenkins 와 같이 별도의 빌드 환경을 구축해서 사용한다면 캐시 디렉토리를 지정할 수 있다. gradle 6.1 버전 부터는 캐시 디렉토리를 지정할 수 있게 되었다. 6.1 의존 패키지 캐싱 디렉토리 지정 매뉴얼 따라서 빌드 할 때 이 캐시 디렉토리를 지정하면 이전에 다운로드된 의존 패키지는 다시 다운로드 받지 않는다.\n$GRADLE_HOME 변수를 지정할 수 있는데, 문제는 프로젝트마다 각기 다른 디렉토리를 지정해주어야 한다는 불편함이 있어서 다른 방법을 찾아보았다.\n2. gradle 의존성을 다운로드 받고 이를 도커 캐시에 저장하는 방법 gradle 에서는 의존성만 다운받아서 캐싱하는 명령어는 없다. 하지만 약간의 꼼수를 사용하면 가능하다. 다음과 같이 Dockerfile을 변경했다.\nFROM gradle:7.4-jdk-alpine WORKDIR /app ADD build.gradle.kts /app/ RUN gradle build -x test --parallel --continue \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true COPY . /app RUN gradle clean build --no-daemon CMD java -jar build/libs/*.jar 핵심은 RUN gradle build -x test --parallel --continue \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true 명령어다. 앞서 build.gradle.kts 파일만 복사했기 때문에 제대로된 빌드가 수행될 수 없다. 하지만 도커 빌드 이미지 내부에 의존 패키지는 다운로드가 된다. (참고로 그래들 의존 패키지가 저장되는 디렉토리는 ${HOME}/.gradle/ 이다.) 그리고 에러가 발생하지만 /dev/null 로 메세지를 보내버렸기 때문에 아무런 응답이 없고 || true 라는 파이프를 연결해서 강제로 성공처리해버렸다. 그 다음에 정상적으로 애플리케이션 소스 파일을 복사하고 다시 빌드하면 빌드 이미지 내부에 저장된 의존 패키지 덕분에 빌드가 빠르게 수행된다. 그리고 이 과정은 각각의 도커 빌드 명령어로 나뉘어졌기 때문에 다음번에 소스코드만 변경하게 된 경우에는 의존 패키지를 다시 다운받지 않게 되어 최종적으로 도커 빌드 시간이 줄어든다.\n최초 도커 파일 적용시 빌드 로그\n[+] Building 189.3s (11/11) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 262B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/gradle:7.4-jdk-alpine 1.1s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 36.87kB 0.0s =\u0026gt; [1/6] FROM docker.io/library/gradle:7.4-jdk-alpine@sha256:93f4bd52c3372e54e4262c5ec9f0e060747b598b9effd1153def7e89833009ba 0.0s =\u0026gt; CACHED [2/6] WORKDIR /app 0.0s =\u0026gt; CACHED [3/6] ADD build.gradle.kts /app/ 0.0s =\u0026gt; [4/6] RUN gradle build -x test --parallel --continue \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true 121.7s =\u0026gt; [5/6] COPY . /app 0.2s =\u0026gt; [6/6] RUN gradle clean build --no-daemon 63.2s =\u0026gt; exporting to image 3.0s =\u0026gt; =\u0026gt; exporting layers 2.9s =\u0026gt; =\u0026gt; writing image sha256:ad2b0af2a18dbf64ca93d427868de1562e3a1c3ff8d2c5968e89ba3bc5bc443c 0.0s 이후 소스 코드의 일부만 변경된 경우\n[+] Building 65.6s (12/12) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 37B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/gradle:7.4-jdk-alpine 2.4s =\u0026gt; [auth] library/gradle:pull token for registry-1.docker.io 0.0s =\u0026gt; [internal] load build context 0.1s =\u0026gt; =\u0026gt; transferring context: 41.38kB 0.1s =\u0026gt; [1/6] FROM docker.io/library/gradle:7.4-jdk-alpine@sha256:93f4bd52c3372e54e4262c5ec9f0e060747b598b9effd1153def7e89833009ba 0.0s =\u0026gt; CACHED [2/6] WORKDIR /app 0.0s =\u0026gt; CACHED [3/6] ADD build.gradle.kts /app/ 0.0s =\u0026gt; CACHED [4/6] RUN gradle build -x test --parallel --continue \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true 0.0s =\u0026gt; [5/6] COPY . /app 0.2s =\u0026gt; [6/6] RUN gradle clean build --no-daemon 62.3s =\u0026gt; exporting to image 0.5s =\u0026gt; =\u0026gt; exporting layers 0.4s =\u0026gt; =\u0026gt; writing image sha256:7cb34a440b507949715da716d9f4f41ce5ed0b8ada396b9b2f5630e27b91bc11 0.0s 자세히 보면 CACHED [4/6] RUN gradle build -x test --parallel --continue \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true와 같이 의존패키지를 다운받는 과정이 CACHED 라고 표시된다. 도커 레이어 캐시를 활용한 것이다.\n중간 결과 도커 빌드 시간이 확 줄어들었다. 189s -\u0026gt; 65s 의존 패키지의 변경이 있을 때만 (보다 정확히는 build.gradle.kts 파일이 변경 되었을 때만) 새롭게 의존성 패키지를 다운받는다. gradle 의존 패키지는 도커 레이어 캐시에 저장된다. 추가 작업 이렇게 하면 도커 빌드는 줄었지만, 최종 도커 이미지가 늘어난다. multi-stage 빌드로 바꾸고 약간 더 정리해보자.\nFROM gradle:7.4-jdk17-alpine as builder WORKDIR /build # 그래들 파일이 변경되었을 때만 새롭게 의존패키지 다운로드 받게함. COPY build.gradle.kts settings.gradle.kts /build/ RUN gradle build -x test --parallel --continue \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true # 빌더 이미지에서 애플리케이션 빌드 COPY . /build RUN gradle build -x test --parallel # APP FROM openjdk:17.0-slim WORKDIR /app # 빌더 이미지에서 jar 파일만 복사 COPY --from=builder /build/build/libs/my-app-0.0.1-SNAPSHOT.jar . EXPOSE 8080 # root 대신 nobody 권한으로 실행 USER nobody ENTRYPOINT [ \\ \u0026#34;java\u0026#34;, \\ \u0026#34;-jar\u0026#34;, \\ \u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;, \\ \u0026#34;-Dsun.net.inetaddr.ttl=0\u0026#34;, \\ \u0026#34;my-app-0.0.1-SNAPSHOT.jar\u0026#34; \\ ] 이렇게 하면 gradle 의 모든 의존 패키지가 다운로드된 도커 이미지의 최종 사이즈가 다음과 같이 1.11G -\u0026gt; 456MB 로 줄어든다.\nREPOSITORY TAG IMAGE ID CREATED SIZE \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7cb34a440b50 8 minutes ago 1.11GB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cc07b80891c1 About a minute ago 456MB 따라서 최종적으로 빌드 시간도 줄어들고, 사이즈도 줄여서 배포시간에도 긍정적인 효과를 가져올 수 있다.\n참고자료 Gradle 6.1 릴리즈 노트 https://localcoder.org/docker-cache-gradle-dependencies https://zwbetz.com/reuse-the-gradle-dependency-cache-with-docker/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/beautiful-goodbye/",
	"title": "Beautiful Goodbye",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/mm/",
	"title": "Mm",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/04/29/beautiful-goodbye-mm/",
	"title": "서비스 종료 넋두리",
	"tags": ["beautiful goodbye", "mm"],
	"description": "",
	"type": "post",
	"contents": "서비스 종료 운영하던 서비스를 종료했다. 앱의 실행을 막고, 사이트를 찾을 수 없다는 메세지를 보고 있으니 기분이 참 먹먹했다. 1년동안 서비스를 개발하며 잠도 못자고 코드를 써내려가던 일부터, 치명적인 버그로 씨름하던 일까지 머리속에 스쳐지나간다. 아쉬운 마음에 넋두리를 기록해 보았다.\n종료한 서비스는 사이트에 연결할 수 없습니다. 왜 서비스를 종료할까? 2021년 6월 부터 2022년 4월까지 11개월간 음(mm)서비스를 운영했다. 그 기간동안 우리팀은 음성 시장에서의 새로운 서비스를 도전했다. 그렇지만, 회사의 목표에 부합할만한 결과를 만들어내지 못했다. 기대에 미치지 못하는 서비스의 성장곡선과 시장의 확장성도 초기 기대보다 많이 낮아져 있었다.\n올해초 마스터플랜을 정하고 구체적인 분기별 스케줄링을 고민하고 있었을 때 상위 조직장에게 \u0026ldquo;서비스를 종료하기로 결정했다\u0026quot;는 의사결정을 전달받았다. 서비스가 종료하는데 꼭 한가지 이유가 있겠냐만은 목표치에 달성하지 못한 것이 제일 큰 종료의 원인이 되었다.\n그렇다! 우리는 목표달성에 실패한 것이다. 시장의 상황이 예상보다 좋지 않았건, 사용자들을 위한 기능이 부족했건, 또는 마케팅의 미스매치로 충분히 널리 알리지 못했건 우리 서비스는 목표를 달성하지 못했고 실패한 서비스로 이름을 올리게되었다.\n종료 준비 - 어떻게 해야 사용자들에게 잘(?!) 종료하겠다고 이야기 해야할까? 서비스를 종료하기로 한 결정을 되돌릴 수는 없었기에, 아쉬운 마음을 안고 종료 준비를 진행했다. 처음부터 하나하나 만들며 오픈한 서비스를 내손으로 종료하려니 다양한 감정들이 떠올랐다. 코드레벨에서 객체의 라이프사이클만 생각해봤지, 서비스의 라이프사이클을 생각하게 될줄이야.\n제일 먼저 \u0026lsquo;사용자들에게 어떻게 종료 소식을 전달해야할까\u0026rsquo; 라는 고민을 했다. 우선 진행하던 모든 기능 개발을 멈추고, 사용자들에게 전달할 종료 공지 초안을 작성했다. 기대에 부합될만큼 숫자는 크지 않더라도 우리의 서비스를 잘 사용해준 사용자들이 있었기 때문에 그 분들에게 종료 소식의 아쉬움을 전달하기 위한 논의를 진행했다. 그 과정에서 우리는 한 가지 프로그램을 진행하기로 했는데, 바로 \u0026lsquo;종료 타운홀\u0026rsquo; 이었다.\n종료 타운홀 운영 블로그에 공지글을 남기고, 이메일과 메시지로 그리고 또 앱 내부의 배너와 푸시로 종료 소식을 알리면서 아쉬웠던 건 바로 진솔한 이야기와 뉘앙스를 전달하기에는 글이 부족하다는 점이었다. 그래서 우리는 우리 서비스의 특성을 살려서 \u0026lsquo;종료 타운홀\u0026rsquo;을 열기로 했다.\n사용자들을 초대해서 글로써는 전달하지 못하는 \u0026lsquo;만든 사람들\u0026rsquo;의 아쉬움과 서비스의 애정을 담아 우리의 이야기를 전할 수 있다면 좋겠다는 바램을 담아서 프로그램을 진행하면 우리 서비스를 잘 사용해준 사용자분들도 웃으며 안녕할 수 있지 않을까 라고 생각했다.\n서비스 종료일을 2주일 정도 앞두고 종료 타운홀을 진행했다. 오후 8시에 시작한 음성 대화방에서 진행한 종료 타운홀에서는 무려 100분 넘게 참석해서 2시간 넘게 진행되었다. 서비스를 만들었던 우리들의 뒷(?) 이야기도 들어주시고, 또 자신들의 이야기를 들려주시며 함께 서비스의 종료를 아쉬워하며 작별인사를 해주셨다.\n이름을 나열할 수는 없지만, 서비스를 사용하며 자신의 삶이 바뀌는 경험을 하셨다는 분, 서비스 종료소식에 울먹이는 목소리로 아쉬움을 말씀해주신 분, 꼭 다시 서비스가 살아났으면 좋겠다고 이야기 해주신 분까지 많은 분들이 서비스 종료에 대한 아쉬움을 이야기 해주셨다.\n타운홀 행사가 진행되기 전까지 혹여나 속상한 마음에 덜컥 화를 내거나 원망하시는 분이 있으면 어떻게 대응해야할까 걱정하고 있었는데 너무나 부질없는 일이었다. 참석해주신 분들은 서비스를 잘 만들어 주어서 고맙다고, 그동안 수고했다며 우리팀에게 인사를 건네주셨다.\n타운홀 마지막 순간 서비스는 사용자들의 발자취를 먹고 산다. 어릴적 아버지에게 왜 매일 TV에서는 싸우고 다투고 안좋은 이야기만 뉴스로 나오는거냐고 물었던 적이 있다. 그 때 아버지는 \u0026ldquo;새상에는 좋은 사람도 있고 나쁜 사람도 있는데 나쁜 사람들만 TV에 나오고 좋은 사람들은 잘 나오지 않아서 그렇다\u0026quot;고 말씀해 주셨다. 그러면서 \u0026ldquo;그럼에도 세상이 굴러가는건 좋은 사람들이 더 많기 때문이다\u0026quot;라고 덧붙이셨던 기억이 난다.\n서비스를 만들고 운영하는 동안에는 사용자들을 직접적으로 대하기가 어려워 지는 순간들이 있는데, 지금 들어온 이 \u0026lsquo;CS-고객문의사항\u0026rsquo; 하나가 특정 한 사람의 문제인지 여러 사람의 문제인지 또 우선순위가 낮은지 높은지 생각들을 하게 된다. 개선의견을 듣게 되더라도 진행중인 일의 스케줄에 밀려서, 또 구현하기에는 너무 많은 리소스가 들어서라는 이유로 어느 한켠에 \u0026lsquo;VOC #56\u0026rsquo;같은 이름으로 저장되기 일쑤다. 누군가에게는 애정어린 목소리일 수도 있는데, 나에게는 일의 하나로 다가오눈 순간들이었다.\n그렇지만 종료 타운홀을 진행하고 나서 든 생각은 예전 아버지의 말씀과 같이 \u0026lsquo;서비스를 잘 사용해주시는 많은 사용자들이 있기 때문에 서비스가 유지되고 성장해나가고 계속 나아갈 수 있는거구나\u0026rsquo; 라고 새삼스럽게 느꼈다. 그 동안 CS라고 하면 대부분은 재현이 안되거나, 당장 대안을 찾기가 어려운 경우가 많아서 나도 모르게 해결하기가 어렵워 귀찮고 힘든일이라고 생각했었는데 대부분의 사용자들은 서비스를 잘 사용해주시고, 또 애정을 가지고 계시다는 것을 가슴 깊이 느끼게 되었다. 그러면서 더 잘 해내지 못한 미안함, 아쉬움. 순간순간 푸념하던 기억이 떠올라 얼굴이 화끈거렸다.\n그래서 기억하고 잊지 말기로 했다. 서비스는 사용자들의 발자취를 먹고 산다.\n종료 순간의 아쉬움. 공식적인 서비스 종료시각이 다되었을 때까지 서비스 사용자분들은 자리를 지켜주셨다. 셧다운 오퍼레이션을 진행하면서 시퀀스 하나하나를 처리해 나갈 때 괜시리 코끝이 찡했다. 비록 1년도 채우지 못하고 종료하는 서비스지만 많은 분들께 미처 몰랐던 사랑을 많이 받았구나라는 걸 느꼈다. 다시금 이런 서비스를 만들 수 있을까? 미래는 알 수 없지만, 그래도 다음번에는 실패로 기록되지 않는 서비스이길 바래본다.\n마지막 순간, 이 노래가 딱 우리 이야기인듯 귀에 감겨왔다. 그리고 2022년 4월 29일 15시. 서비스를 종료했다.\n그리고 남은 사람들 서비스는 공식적으로 종료했지만, 아직 몇가지 뒷정리가 남아 있다. 개인정보의 완전한 파기 작업부터 인프라 정리 까지 챙겨야할 일들이 무척이나 많이(?!) 남아 있다.\n그리고 함께한 동료들과도 인사를 나누고 다시 새로운 미션을 위해서 새로운 일을 시작해야한다. 누군가는 떠나가고, 누군가는 다시 새로운 기회를 찾아야 한다. 그래도 기존의 멤버 대부분은 다시 함께 일할 수 있게 되어서 다행이다.\n서비스가 종료하는 마당에 팀도 공중분해되는 것은 아닐까 하는 걱정이 들었지만 다행히도(!) 함께 새로운 일을 진행할 기회를 얻었다. 처음부터 마지막까지 좋은 동료들을 마주한 덕분에 울고 웃으며 힘들지만 즐겁게 서비스를 만들 수 있었다. 팀동료들에게 다시한번 감사의 인사를 전하며, 이제 다시 새로운 일을 마주해야한다. 못다한 이야기, 남은 그리움은 가슴한켠에 묻어두고.\n"
},
{
	"permalink": "https://findstar.pe.kr/2022/01/26/book-review-get-your-hands-dirty-on-clean-architecture/",
	"title": "[북리뷰] 만들면서 배우는 클린 아키텍처",
	"tags": ["book", "architecture", "만들면서 배우는 클린 아키텍처"],
	"description": "",
	"type": "post",
	"contents": "스프링 애플리케이션을 작성하면서 항상 아키텍처에 적합한 패키지 구조가 어떤 형태인지에 대한 의문이 있었다. 그러던중 우연찮게 이 책을 읽고나서 많은 부분에서 인사이트를 얻을 수 있었다. 특히 DDD 에서 말하는 도메인 객체를 설계할 때의 경계에 대한 고민을, 책에서 소개하는 \u0026ldquo;Hexagonal Architecture\u0026rdquo; 를 통해서 이해할 수 있었다.\n책 소개 이 책은 DDD 와 클린 아키텍처에서 언급되는 \u0026ldquo;Hexagonal Architecture\u0026rdquo; 에 대한 설명을 담고 있다. 기존의 전통적인 \u0026ldquo;Layered Architecture\u0026quot;를 살펴보고 이 아키텍처의 단점과 이를 개선하는 방법에 대해서 설명하고 있으며, 각각의 아키텍처 선택지에 대한 장단점을 이야기해주는 책이다. 가격은 1만 8천원으로, 내가 읽은 시점에는 yes24 평점이 없었지만, 상당히 만족하면서 읽은 책이다.\n책을 읽은 이유 스프링 애플리케이션을 작성할 때 가장 궁금한 것중 하나는 패키지 구조를 어떻게 구성하는 것이 좋을까에 대한 의문이었다. 애플리케이션의 전체적인 설계의도가 잘 드러나는 구조가 적합하다고 생각했지만, 작성되는 클래스는 항상 비슷비슷한 구조를 띄고 있었다. 그리고 대부분은 시작부터 내가 참여하지 않고 레거시 프로젝트를 맡아서 유지보수 하기 마련이라 기존에 구성된 패키지 구조를 그대로 답습하게 되었다. 이 말은 즉 기존의 아키텍처에서 크게 벗어난 형태로 개선하기가 어려웠다는 뜻이기도 하면서, 어떤 아키텍처가 현재에 적합한지 알기가 어려웠다는 뜻이기도 하다. 그러다가 2021년 새로운 팀에서 업무를 시작하면서 새로운 애플리케이션을 시작할 수 있는 기회가 생겼다. (이른바 first commit 을 내손으로!) 이왕 시작하는 거 아키텍처에 신경쓰면서 잘 정돈된 패키지 구조를 가지고 싶었는데 막상 2022년 새해를 맞이하면서 10개월 가까이 작성한 코드를 돌아보니 기존의 레거시 프로젝트의 구조와 별반 다를게 없어보였다. 그래서 좀 더 나은 구조는 어떤 것일까 궁금하여 책을 찾게되었고 마침 \u0026ldquo;만들면서 배우는 클린 아키텍처\u0026rdquo; 라는 책이 눈에 띄어 냉큼 구매해 보았다.\n가장 적절한 패키지 구조는 무엇일까라는 호기심에서 찾게된 책 클린 아키텍처라는 말에 현혹됨 일단 가격이 싸고 얇다! 소감 일단 책이 150페이지도 되지 않아서 읽기에 부담이 없어서 좋았다. 얇은 분량 덕분에 가볍게 마음먹고 시작할 수 있었고 이 책과 같이 번역서의 경우 항상 번역 퀄리티 문제가 있었는데 이 책은 용어의 낯선 느낌이 덜해서 읽는데 방해되는 일이 적었다. 군데 군데 일부 용어들은 현장에서 자주 사용되는 용어로 병기되어 있어서 좋았다. 무엇보다도 전통적인 애플리케이션의 구조를 설명하면서 단계적으로 문제점과 개선점을 소개하고 있는데 바로 그 전통적인 애플리케이션 구조가 내가 작성하던 코드와 유사해서 문제점과 개선점이라는 부분에서 공감이 많이 되었다. 평소에 어떻게 하면 좀 더 나은 구조를 만들 수 있을까 고민했던 부분들에 대한 해답을 찾은 느낌이랄까? 물론 책에서 설명하는 \u0026ldquo;Hexagonal Architecture\u0026quot;이 모든 상황에서의 정답은 아니기에 항상 적합하다고 볼수는 없지만, 그동안 피상적으로만 이해하고 있던 DDD, 클린 아키텍처에서 이야기 하는 내용들을 좀 더 잘 이해할 수 있겠다는 생각이 들었다.\n핵심적인 몇몇 내용들 UserService 대신 RegisterUserService 를 작성하라 : 이 부분은 상당히 공감이 많이 되었던 내용이었다. 특히나 고민없이 클래스를 만들었던 과거에 수많은 Service, Manager, Handler 를 양상해본 경험이 있어서 더욱더 깊은 반성이 되었던 내용이다. 평소 객체지향의 제일 쉬운 접근은 클래스를 잘게 쪼개 보는 방법이라고 생각하는데, 이렇게 서비스 객체를 UseCase 에 특화해서 만드는게 좋다고 생각했다. SRP - 컴포넌트를 변경하는 이유는 오직 하나뿐이어야 한다. : \u0026ldquo;하나의 클래스는 하나의 책임을 가져야 한다\u0026quot;로 알고 있었는데, 생각해보니 책의 말대로 변경의 이유가 하나여야 한다고 이해하는 것이 더 적합하다고 생각이 들었다. 둘은 미묘하게 다른데 역할 또는 책임이라는 것을 명시적으로 설명하기 어려운데 \u0026ldquo;변경 이유\u0026rdquo; 라고 정의하니 훨씬 더 잘 이해하기 쉽다고 느껴졌다. Hexagonal Architecture 의 패키지 구조 : 패키지 구조를 명시적으로 보여줌으로써, 각각의 레이어의 의존성 역전, CQRS, Domain-Jpa Mapping 구조까지 잘 보여주는 예제를 확인할 수 있었다 account ├── adapter │ ├── in │ │ └── web │ │ └── SendMoneyController.java │ └── out │ └── persistence │ ├── AccountJpaEntity.java │ ├── AccountMapper.java │ ├── AccountPersistenceAdapter.java │ ├── ActivityJpaEntity.java │ ├── ActivityRepository.java │ └── SpringDataAccountRepository.java ├── application │ ├── port │ │ ├── in │ │ │ ├── GetAccountBalanceQuery.java │ │ │ ├── SendMoneyCommand.java │ │ │ └── SendMoneyUseCase.java │ │ └── out │ │ ├── AccountLock.java │ │ ├── LoadAccountPort.java │ │ └── UpdateAccountStatePort.java │ └── service │ ├── GetAccountBalanceService.java │ ├── MoneyTransferProperties.java │ ├── NoOpAccountLock.java │ ├── SendMoneyService.java │ └── ThresholdExceededException.java └── domain ├── Account.java ├── Activity.java ├── ActivityWindow.java └── Money.java 매핑전략 설명 : 각각의 매핑전략을 설명하고 장단점을 설명한 부분이 좋았다. 의식적으로 지름길 선택하기 : 책에서 설명한 대로 모든 의존성 역전구조와, 경계별로 나뉘어진 모델 객체를 사용하려면 작성할 클래스가 아주 많아지고 보일러플레이트 코드가 많아질 수 밖에 없다. 그래서 저자는 이부분을 적절하게 구분하여 의식적으로 경계를 느슨하게 할 수 있는 전략을 설명해주고 있는데, 이런 경계에 대한 인식없이 그냥 작성하는 것과 알고서 의식적으로 선택하는 것은 차이가 있다고 생각한다. 예를 들면 JPA Entity 를 RegisterUserService 의 응답 객체로 활용할 수 있는데 이 경우 어떤 부분이 문제가 되는지 설명해주고 그럼에도 이렇게 사용할 수 있다는 점을 설명해주었는데 이런 내용들이 공감이 많이 되었다. 아키텍처는 절대적이고 전역적이어야할까? 책에서는 줄곧 \u0026ldquo;Hexagonal Architecture\u0026quot;를 기준으로 설명하고 있지만, 마지막까지 이야기 하는 내용을 요약하자면 다음 메세지가 아닐까 한다.\n아키텍터는 팀의 의사결정에 따라 결정된다. 이 때 결정되는 아키텍처는 한번 정하면 불변하는 것이 아니고, 지속적으로 개선(또는 선택)될 수 있다. 그리고 모든 부분에 일관된 하나의 아키텍처(스타일, 코딩 컨벤션이든 무엇이든)를 적용하지 않아도 된다\n결국 은빛 탄환은 없다.(No Silver Bullet)\n덧붙이며 이전에 읽은 POEAA(엔터프라이즈 애플리케이션 아키텍처 패턴)에서 도메인과 영속성 매핑에 관해서 강조했는데 코드가 너무 낡아서 이해하기가 어려웠었다. JPA 와 Domain 기반 코드를 작성하면서 이 책에서 예시로든 PersistenceAdapter 가 매핑구조를 이해하는데 도움이 되었다.\n한줄 요약 \u0026ldquo;레이어의 경계를 고민하고 있다면 적절한 지침서가 되는 책\u0026rdquo; 참고 yes24 링크 : https://www.yes24.com/Product/Goods/105138479 21년 6월의 책 - Get Your Hands Dirty On Clean Architecture : https://velog.io/@gyunghoe/21%EB%85%84-6%EC%9B%94%EC%9D%98-%EC%B1%85-Get-Your-Hands-Dirty-On-Clean-Architecture Get Your Hands Dirty on Clean Architecture 를 읽고 : https://namkyujin.com/post/20210401-get-your-hands-on-clean-arch/ [Line engineering] 지속 가능한 소프트웨어 설계 패턴: 포트와 어댑터 아키텍처 적용하기 : https://engineering.linecorp.com/ko/blog/port-and-adapter-architecture/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/book/",
	"title": "Book",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/%EB%A7%8C%EB%93%A4%EB%A9%B4%EC%84%9C-%EB%B0%B0%EC%9A%B0%EB%8A%94-%ED%81%B4%EB%A6%B0-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98/",
	"title": "만들면서 배우는 클린 아키텍처",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2022/01/22/book-review-poeaa/",
	"title": "[북리뷰] 엔터프라이즈 애플리케이션 아키텍처패턴",
	"tags": ["book", "POEAA", "엔터프라이즈 애플리케이션 아키텍처 패턴"],
	"description": "",
	"type": "post",
	"contents": "새로운 팀에서 신규 프로젝트를 작성하면서 애플리케이션의 구조에 대해서 고민하게 되었는데, 좀 더 나은 아키텍처를 위해서 참고할만한 서적을 찾던 중 책장에 사놓고 고이 모셔두었던 이 책을 찾아 읽어보았다.\n책 소개 위키북스에서 출판한 서적. 2015년 재출간한 서적이다. 저자는 그 유명한 \u0026ldquo;마틴 파울러\u0026rdquo;. 가격은 3만 1천 5백원이고, yes24 평점도 8.5점대로 괜찮은 평을 받고 있다.\n책을 읽은 이유 2021년 새로운 팀에서 업무를 시작하면서 이왕 새로 시작하는 프로젝트를 잘 해보고 싶은 마음에 아키텍처에 대한 고민을 많이 하게되었다. 그 동안 레거시 코드를 다루다 보니 신경쓰지 못한 것 같아 아키텍처를 어떻게 하면 잘 설계할 수 있을까 하고 생각하게 되었다. 그러던 중에 책장 한켠에 고이 모셔두었던(?) 한 번쯤은 읽어봐야지 하고 생각했던 \u0026ldquo;엔터프라이즈 애플리케이션 아키텍처 패턴\u0026rdquo; 책을 읽어보기로 했다. 한동안 패턴, 아키텍터에 대해서 관심 많을 때 사놓은 책인데, 그 유명한 \u0026ldquo;마틴 파울러\u0026quot;의 책이라 사놓고서는 앞장만 살짝 읽은 이후로 방치하고 있던 책이었다. 이왕 읽어보기로 하는 거, 팀동료들과 북 스터디를 진행해보았다.\n마틴 파울러의 책 그 동안 커뮤니티에서 한번 쯤 읽어두면 좋다는 이야기를 들어보았음 아키텍처 그 중에서도 아키텍처 패턴을 엔터프라이즈에서 고민한 책이라는 호감 소감 북 스터디를 겸하고 있어서 한주단위로 챕터별로 읽었다. 대략 약 8주가 안되는 시간이 소모되었다. 페이지가 500페이지가 넘기 때문에 분량이 꽤 많은데, 중간중간 코드가 있어서 그나마 수월했다. 다 읽은 뒤에 소감은 다음과 같다.\n장점 모던한 애플리케이션의 아키텍처들이 어떤 흐름을 기반으로 하여 패턴화 되었는지 배경을 이해할 수 있었다. 도메인 로직과, 객체-관계형 데이터베이스의 매핑에 대한 내용이 많이 언급되며 개념의 배경을 이해할 수 있다. 단점 원서가 출간된 시기가 2002년인 만큼, 최신의 애플리케이션 구조와는 맞지 않은 낡은 부분이 많았다. 특히나 XML, XSLT 에 대한 부분은 보면서도 읽기 힘들었던 부분이다. 용어번역은 마음에 들지 않는 부분이 많다. 특히나 패턴이름을 번역하고 다이어그램에 나오는 용어들도 번역해놓아 한참을 들여다 봐야 하는 부분이 많았다. 코드가 많이 나오지만, 현재의 애플리케이션 코드에 빗대면 실용적이라고 보기는 어렵다. 기억에 남는 부분 도메인 로직에 대한 설명을 하면서 도메인 모델을 2가지로 구분했는데 \u0026ldquo;Simple Domain Model\u0026rdquo;, \u0026ldquo;Rich Domain Model\u0026rdquo; 로 구분한 부분에서 도메인 모델을 설계할 때의 기준점을 생각해볼 수 있었다. 그리고 아키텍처가 꼭 하나를 선택한다고 해서 다른 아키텍처를 포기한다는 것이 아니라고 설명한 부분이 공감이 갔다. 각각의 패턴은 상호 배타적이지 않을 수 있으며, 필요와 복잡도에 따라서 구분하거나 혼용할 수도 있다는 점에 동의했다. 마지막으로 책에서 설명한 패턴들이 결국 최신 프레임워크의 기술들이 구현해놓은 것이라는 알게되는데 특히나 ORM 과 관련해서 JPA 를 더 공부하고 싶게 만드는 동기가 되었다.\n한줄 요약 \u0026ldquo;한번쯤은 읽어볼만한 책이지만, 만족스럽지 못한 용어 번역과, 오래된 스타일의 코드는 감수해야한다. 빠르게 읽어야 한다면 4장을 제외한 1부만 읽어보는것이 시간을 줄일 수 있을 것같다\u0026rdquo; 참고 yes24 링크 : http://www.yes24.com/Product/Goods/22384677 "
},
{
	"permalink": "https://findstar.pe.kr/tags/poeaa/",
	"title": "POEAA",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/%EC%97%94%ED%84%B0%ED%94%84%EB%9D%BC%EC%9D%B4%EC%A6%88-%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%ED%8C%A8%ED%84%B4/",
	"title": "엔터프라이즈 애플리케이션 아키텍처 패턴",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/forwarded-option/",
	"title": "Forwarded Option",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/nginx-ingress/",
	"title": "Nginx Ingress",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/08/22/nginx-ingress-controller-use-forwarded-for-option/",
	"title": "Nginx ingress 에서 X-forwarded header 사용법",
	"tags": ["nginx ingress", "forwarded option"],
	"description": "",
	"type": "post",
	"contents": "개요 Nginx ingress controller 를 사용하는 K8S 환경에서 네트워크 구성에 따라서 X-forwarded 값을 전달받아야 하는 경우가 있다. 이 때 적용할 수 있는 Nginx ingress controller 의 use-forwarded-for 옵션에 대해서 살펴보자.\nDSR LB 만 존재하는 경우 먼저 K8S 환경에서 Nginx ingress controller 를 사용한다고 가정하겠다. 일단 k8s 클러스터의 앞단에 LB 가 있고 이 LB 는 DSR 모드로 작동해서 외부의 트래픽을 바로 Ingress 로 전달한다고 생각하면 다음과 같은 형태로 트래픽이 유입된다.\n여기서는 Nginx ingress controller 가 바로 reverse proxy 역할을 하기 때문에 Service 로 전달되는 트래픽에는 Nginx ingress 에서 전달해주는 X-forwarded-* 값을 사용하게 된다. 이 경우에는 nginx ingress controller 의 use-forwarded-for 옵션이 false 로 지정한다. 만약 클라이언트가 보내는 X-forwarded-* 값이 있더라도 이를 무시하고 덮어쓴다.)\n앞단에 별도의 Reverse proxy 가 존재하는 경우 경우에 따라서는 L7 proxy를 k8s 클러스터 앞단에 위치 시킬수도 있는데 이 경우에는 다음과 같은 형태의 트래픽 유입이 일어난다. (나의 경우에는 Haproxy 를 두었다.)\n이 경우에는 실제로 클라이언트의 트래픽이 서비스까지 전달되려면 reverse proxy 를 2번 거치게된다. (첫 번째는 L7 Proxy, 두 번째는 Nginx ingress) 따라서 nginx ingress 입장에서는 L7 proxy 에서 전달되는 X-forwarded-* 값을 신뢰하고 연결되는 서비스로 넘겨줘야한다. 이 경우에는 nginx ingress controller 의 use-forwarded-for 옵션을 true 로 지정한다.\n적용 방법 nginx ingress controller 에 연결되는 config map 에 값을 설정하면 된다.\napiVersion: v1 kind: ConfigMap metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx name: ingress-forwarded namespace: ingress-nginx data: ## ... other values 생략.. ## use-forwarded-headers: \u0026#34;true\u0026#34; 요약 K8s 와 연결되는 네트워크 구성과 그 앞단의 프록시 유무에 따라서 use-forwarded-headers 값을 적절하게 사용할 수 있다.\n참고자료 https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-forwarded-headers "
},
{
	"permalink": "https://findstar.pe.kr/tags/hugo/",
	"title": "Hugo",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/08/16/hugo-rss-template/",
	"title": "Hugo에서 RSS 템플릿을 지정하는 방법",
	"tags": ["hugo", "rss template"],
	"description": "",
	"type": "post",
	"contents": "개요 지난주에 커뮤니티에서 친분이 있는 분이 이 블로그의 RSS 링크에서 포스트의 태그가 확인되지 않는다고 제보해 주셨다. 확인 결과 rss 링크 \u0026ldquo;https://findstar.pe.kr/index.xml\u0026quot; 에서 포스트에 등록한 태그가 노출되지 않고 있었다. 그래서 RSS 스펙을 확인하여 hugo 템플릿을 수정해서 해결하였다.\n템플릿 파일 생성하기 매뉴얼 을 확인하면 먼저 RSS 템플릿의 파일 매칭 순서를 확인할 수 있다. 내가 사용하는 테마파일에서는 별도의 RSS 템플릿을 지정하지 않았기 때문에 내장된 기본값이 사용된다. 기본값은 링크에서 내용을 확인할 수 있다.\n내장된 기본값은 다음과 같다.\n{{- $pctx := . -}} {{- if .IsHome -}}{{ $pctx = .Site }}{{- end -}} {{- $pages := slice -}} {{- if or $.IsHome $.IsSection -}} {{- $pages = $pctx.RegularPages -}} {{- else -}} {{- $pages = $pctx.Pages -}} {{- end -}} {{- $limit := .Site.Config.Services.RSS.Limit -}} {{- if ge $limit 1 -}} {{- $pages = $pages | first $limit -}} {{- end -}} {{- printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;rss version=\u0026#34;2.0\u0026#34; xmlns:atom=\u0026#34;http://www.w3.org/2005/Atom\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;{{ if eq .Title .Site.Title }}{{ .Site.Title }}{{ else }}{{ with .Title }}{{.}} on {{ end }}{{ .Site.Title }}{{ end }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;Recent content {{ if ne .Title .Site.Title }}{{ with .Title }}in {{.}} {{ end }}{{ end }}on {{ .Site.Title }}\u0026lt;/description\u0026gt; \u0026lt;generator\u0026gt;Hugo -- gohugo.io\u0026lt;/generator\u0026gt;{{ with .Site.LanguageCode }} \u0026lt;language\u0026gt;{{.}}\u0026lt;/language\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;managingEditor\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/managingEditor\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;webMaster\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/webMaster\u0026gt;{{end}}{{ with .Site.Copyright }} \u0026lt;copyright\u0026gt;{{.}}\u0026lt;/copyright\u0026gt;{{end}}{{ if not .Date.IsZero }} \u0026lt;lastBuildDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/lastBuildDate\u0026gt;{{ end }} {{- with .OutputFormats.Get \u0026#34;RSS\u0026#34; -}} {{ printf \u0026#34;\u0026lt;atom:link href=%q rel=\\\u0026#34;self\\\u0026#34; type=%q /\u0026gt;\u0026#34; .Permalink .MediaType | safeHTML }} {{- end -}} {{ range $pages }} \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/pubDate\u0026gt; {{ with .Site.Author.email }}\u0026lt;author\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/author\u0026gt;{{end}} \u0026lt;guid\u0026gt;{{ .Permalink }}\u0026lt;/guid\u0026gt; \u0026lt;description\u0026gt;{{ .Summary | html }}\u0026lt;/description\u0026gt; \u0026lt;/item\u0026gt; {{ end }} \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; 이제 나의 고유한 RSS 템플릿 파일을 작성해보자. 위의 내장된 기본값을 바탕으로 약간의 수정된 내용을 추가하면 된다. 나의 경우에는 layouts/_default/index.xml 파일을 생성하였다. 만약 값이 적용이 안된다면 매뉴얼에서 매칭 순서를 다시 확인해보자. 우선순위가 높은 파일이 이미 존재할 수도 있다.\n생성한 파일은 다음과 같이 \u0026lt;category\u0026gt; 태그를 추가하였다. 포스트에서는 TAG 이지만, 이를 표한혀는 RSS 스펙이 Category 이기 때문이다. 이를 Taxonomy 라고 한다.\n{{- $pctx := . -}} {{- if .IsHome -}}{{ $pctx = .Site }}{{- end -}} {{- $pages := slice -}} {{- if or $.IsHome $.IsSection -}} {{- $pages = $pctx.RegularPages -}} {{- else -}} {{- $pages = $pctx.Pages -}} {{- end -}} {{- $limit := .Site.Config.Services.RSS.Limit -}} {{- if ge $limit 1 -}} {{- $pages = $pages | first $limit -}} {{- end -}} {{- printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;rss version=\u0026#34;2.0\u0026#34; xmlns:atom=\u0026#34;http://www.w3.org/2005/Atom\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;{{ if eq .Title .Site.Title }}{{ .Site.Title }}{{ else }}{{ with .Title }}{{.}} on {{ end }}{{ .Site.Title }}{{ end }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;description\u0026gt;Recent content {{ if ne .Title .Site.Title }}{{ with .Title }}in {{.}} {{ end }}{{ end }}on {{ .Site.Title }}\u0026lt;/description\u0026gt; \u0026lt;generator\u0026gt;Hugo -- gohugo.io\u0026lt;/generator\u0026gt;{{ with .Site.LanguageCode }} \u0026lt;language\u0026gt;{{.}}\u0026lt;/language\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;managingEditor\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/managingEditor\u0026gt;{{end}}{{ with .Site.Author.email }} \u0026lt;webMaster\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/webMaster\u0026gt;{{end}}{{ with .Site.Copyright }} \u0026lt;copyright\u0026gt;{{.}}\u0026lt;/copyright\u0026gt;{{end}}{{ if not .Date.IsZero }} \u0026lt;lastBuildDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/lastBuildDate\u0026gt;{{ end }} {{- with .OutputFormats.Get \u0026#34;RSS\u0026#34; -}} {{ printf \u0026#34;\u0026lt;atom:link href=%q rel=\\\u0026#34;self\\\u0026#34; type=%q /\u0026gt;\u0026#34; .Permalink .MediaType | safeHTML }} {{- end -}} {{ range $pages }} \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;{{ .Permalink }}\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;{{ .Date.Format \u0026#34;Mon, 02 Jan 2006 15:04:05 -0700\u0026#34; | safeHTML }}\u0026lt;/pubDate\u0026gt; {{ with .Site.Author.email }}\u0026lt;author\u0026gt;{{.}}{{ with $.Site.Author.name }} ({{.}}){{end}}\u0026lt;/author\u0026gt;{{end}} \u0026lt;guid\u0026gt;{{ .Permalink }}\u0026lt;/guid\u0026gt; \u0026lt;description\u0026gt;{{ .Summary | html }}\u0026lt;/description\u0026gt; {{ range .Params.tags }} \u0026lt;category\u0026gt;{{ . }}\u0026lt;/category\u0026gt; {{ end }} \u0026lt;/item\u0026gt; {{ end }} \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; 실제 기본 템플릿 내용에서 추가한 부분은 다음과 같다.\n\u0026lt;item\u0026gt; ..생략.. {{ range .Params.tags }} \u0026lt;category\u0026gt;{{ . }}\u0026lt;/category\u0026gt; {{ end }} \u0026lt;/item\u0026gt; 템플릿 파일에서는 .Params 값을 사용할 수 있다. 템플릿에서 사용할 수 있는 값들은 매뉴얼을 참고하자. 추가한 내용을 해석하면 포스트의 tags 라는 값을 확인해서 category 라는 RSS 태그를 추가한 것이다. 따라서 category 는 여러개가 될 수 있다. 물론 위의 RSS 태그가 표시되려면 개별 포스트는 tags 라는 속성값이 존재해야한다.\n... tags: [\u0026#34;hugo\u0026#34;, \u0026#34;rss template\u0026#34;] ... 결과 확인 이제 rss 결과를 확인하자.\n\u0026lt;item\u0026gt; \u0026lt;title\u0026gt;Spring 프로젝트 Maven을 사용할 때 도커라이즈 캐싱방법\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://findstar.pe.kr/2021/08/06/dockerize-maven-project/\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;Fri, 06 Aug 2021 20:08:09 +0900\u0026lt;/pubDate\u0026gt; \u0026lt;guid\u0026gt;https://findstar.pe.kr/2021/08/06/dockerize-maven-project/\u0026lt;/guid\u0026gt; \u0026lt;description\u0026gt;spring 프로젝트에서 maven 을 사용할 때 도커라이즈에서 레이어를 캐싱하여 빌드 속도를 향상시키는 방법을 살펴보았다.\u0026lt;/description\u0026gt; \u0026lt;category\u0026gt;dockerize\u0026lt;/category\u0026gt; \u0026lt;category\u0026gt;spring boot\u0026lt;/category\u0026gt; \u0026lt;category\u0026gt;maven\u0026lt;/category\u0026gt; \u0026lt;/item\u0026gt; 참고 https://gohugo.io/templates/rss/#the-embedded-rss-xml https://gohugo.io/variables/page/ https://validator.w3.org/feed/docs/rss2.html "
},
{
	"permalink": "https://findstar.pe.kr/tags/rss-template/",
	"title": "Rss Template",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/dockerize/",
	"title": "Dockerize",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/maven/",
	"title": "Maven",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/spring-boot/",
	"title": "Spring Boot",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/08/06/dockerize-maven-project/",
	"title": "Spring 프로젝트 Maven을 사용할 때 도커라이즈 캐싱방법",
	"tags": ["dockerize", "spring boot", "maven"],
	"description": "",
	"type": "post",
	"contents": "개요 Spring boot 프로젝트 개발할 때 의존 패키지를 다운받는 과정이 소모적으로 느껴져 도커의 빌드 레이어를 사용하여 의존성을 다운받는 과정을 생략할 수 없을까 고민하게 되었다. 의존성 매니저 도구로 Maven 을 사용하고 있었기 때문에 몇번의 시행착오를 거쳐서 현재 적용하고 있는 방법을 정리해보았다.\n아이디어 Spring Boot 프로젝트를 Dockerfile 을 사용하여 도커라이즈 하면 매번 maven 의존성을 다운로드 하는 과정을 거친다. 이 과정을 Jar 패키징과 분리하여 처리하여 의존성을 매번 다운로드 받지 않아도 되기 때문에 좀더 빠른 도커 빌드가 가능해질꺼라고 생각되었다.\nmvn go-offline 을 사용하는 방법 작업하는 Spring 에서 Maven 을 사용하고 있었기 때문에 Maven 의 go-offline 기능을 사용해보려고 시도하였다. Dockerfile 은 다음과 같이 구성하였다.\nFROM maven:3.6.3-jdk-11 as builder # working directory 지정 WORKDIR /app # maven dependency 를 복제해서 캐싱하도록 시도 COPY pom.xml . # maven dependency 를 다운로드 받는다. RUN mvn dependency:go-offline # 소스코드 복사 COPY src/ /app/src/ # 메이븐 패키징 # RUN mvn -o package (이 명령어는 오프라인의 디펜던시를 활용하여 패키징 하는 명령어지만, 도커 빌드가 실패하여 사용이 불가능했다.) RUN mvn package # 멀티 스테이지 빌드 FROM adoptopenjdk/openjdk11:jre-11.0.10_9-alpine WORKDIR /app COPY --from=builder /app/target/*-SNAPSHOT.jar /app ENV TZ=Asia/Seoul RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone EXPOSE 8080 ENTRYPOINT [ \\ \u0026#34;java\u0026#34;, \\ \u0026#34;-jar\u0026#34;, \\ \u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;, \\ \u0026#34;/app/my-app.jar\u0026#34; \\ ] 그런데 나의 도커 빌드 환경은 로컬이나 별도의 CI 환경이 아니라 사내에서 운영하는 private docker registry 에서 제공하는 기능을 사용하고 있어서 kaniko 를 사용하고 있었다. 여기에서는 내가 원하는 대로 go-offline 이후에 의존성이 캐싱되지 않았다. 그리고 어떤 원인에서인지는 정확히 알 수 없지만 pom.xml 에 정의해둔 maven plugin + kotlin 설정으로 인해서 go-offline 을 통해서도 완전히 의존성이 다운로드 되지 않고 mvn package 에서 매번 추가 의존 패키지를 다운로드 받는 것을 확인하였다. 원래는 go-offline 이후 mvn -o package 를 사용하여 offline 에 다운로드 받은 의존성 패키지만을 사용하여 패키징 하도록 시도하였으나 이경우에는 아예 도커 빌드가 실패하였다.\nmvn vefiry 를 사용하는 방법 두 번째로는 mvn verify 명령어를 사용하는 방법이다.\nFROM maven:3.6.3-jdk-11 as builder # working directory 지정 WORKDIR /app # maven file copy COPY pom.xml . # Download maven dependency RUN mvn verify --fail-never # 소스코드 복사 COPY src/ /app/src/ # 메이븐 패키징 RUN mvn package # 멀티 스테이지 빌드 FROM adoptopenjdk/openjdk11:jre-11.0.10_9-alpine WORKDIR /app COPY --from=builder /app/target/*-SNAPSHOT.jar /app ENV TZ=Asia/Seoul RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone EXPOSE 8080 ENTRYPOINT [ \\ \u0026#34;java\u0026#34;, \\ \u0026#34;-jar\u0026#34;, \\ \u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;, \\ \u0026#34;/app/my-app.jar\u0026#34; \\ ] 이 경우에는 의도한 대로 조금 더 빠른 도커 빌드가 가능한 결과를 확인할 수 있었다.\n빌드 시간 비교 mvn verify 없이 통째로 빌드하는 경우 : 271.0s mvn verify 도입후 디펜던시 변경시 : 338.6s mvn verify 도입후 캐시 레이어 기반에서 소스만 변경시 99.7s 기타 만약 jenkins 환경이 별도로 구성되어 있고 도커 빌드 타이밍에 의존성 패키지를 다운로드 받는 캐시를 위한 볼륨 마운트가 가능하다면 https://daddyprogrammer.org/post/12542/springboot-docker-integration/ 에서와 같이 볼륨 마운트를 시도해 볼수도 있다.\n또는 Java 한정이긴 하지만 jib 이나 buildpack 을 시도해 볼 수도 있을 것 같다.\n요약 Maven 기반의 Spring boot 프로젝트를 도커라이즈 할 때 mvn verify 를 사용하면 의존 패키지를 도커 레이어에 캐싱 할 수 있어 전체 도커 빌드 타임이 줄어드는 효과를 얻을 수 있다.\n참고 https://ohjongsung.io/2019/10/20/spring-boot-%EB%8F%84%EC%BB%A4-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%B5%9C%EC%A0%81%ED%99%94 https://velog.io/@ojwman/docker-maven https://daddyprogrammer.org/post/12542/springboot-docker-integration/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/data.sql/",
	"title": "Data.sql",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/multi-datasource/",
	"title": "Multi Datasource",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/07/31/multi-datasource-data-sql-initialize/",
	"title": "멀티 데이터소스에서 data.sql 사용하기",
	"tags": ["spring boot", "multi datasource", "data.sql"],
	"description": "",
	"type": "post",
	"contents": "개요 spring boot 프로젝트를 개발하는 도중 data.sql 을 사용해서 테스트에 필요한 기초 데이터를 초기화 하는 작업을 수행했다. 그런데 연결되는 데이터베이스가 늘어나서 데이터소스 설정을 추가한 뒤에 각각의 데이터베이스 마다 data.sql 을 사용하여 초기화 하려고 하자 테스트가 계속 실패했다. 그 원인을 찾아서 수정해보았다.\n멀티 데이터소스와 Data.sql 연결 싱글 데이터소스 연결 spring boot 프로젝트에서 여러개의 데이터베이스에 연결하는 경우가 있다. 나의 경우에는 신규 프로젝트를 시작하면서 제일 처음 사용자의 계정 데이터베이스에 연결하는 데이터소스를 설정하기 위해서 다음과 같이 코드를 작성하였다.\n다음은 AccountDataSourceConfiguration 이다.\n@Configuration @EnableJpaRepositories(basePackages = { \u0026#34;me.home.domain.account\u0026#34; }, entityManagerFactoryRef = \u0026#34;accountEntityManagerFactory\u0026#34;, transactionManagerRef = \u0026#34;accountTransactionManager\u0026#34;) public class AccountDataSourceConfig { public static final String[] packages = new String[]{ \u0026#34;me.home.domain.account\u0026#34; }; private final JpaProperties jpaProperties; private final HibernateProperties hibernateProperties; public AccountDataSourceConfig(JpaProperties jpaProperties, HibernateProperties hibernateProperties) { this.jpaProperties = jpaProperties; this.hibernateProperties = hibernateProperties; } ....이하 생략... 이 상태에서 data.sql 을 작성한 뒤 테스트를 실행하면 바로 이 AccountDataSource 에 작성한 SQL 스크립트가 실행된다.\n멀티 데이터소스 연결 위의 상황에서 추가적으로 비지니스 로직을 구현하면서 필요한 콘텐츠를 담아두는 데이터소스를 추가한다고 생각해보자.\n다음은 ContentDataSourceConfiguration 이다.\n@Configuration @EnableJpaRepositories(basePackages = { \u0026#34;me.home.domain.post\u0026#34;, \u0026#34;me.home.domain.comment\u0026#34;}, entityManagerFactoryRef = \u0026#34;contentEntityManagerFactory\u0026#34;, transactionManagerRef = \u0026#34;contentTransactionManager\u0026#34;) public class ContentDataSourceConfig { private final JpaProperties jpaProperties; private final HibernateProperties hibernateProperties; public ContentDataSourceConfig(JpaProperties jpaProperties, HibernateProperties hibernateProperties) { this.jpaProperties = jpaProperties; this.hibernateProperties = hibernateProperties; } ...이하 생략... 이 다음에는 content data source 에서 실행하고자 하는 data.sql 스크립트 내용을 추가로 작성해야하는데 이를 구분해서 적용하는 방법이 매뉴얼에는 안내되어 있지 않다. (매뉴얼에는 While we do not recommend using multiple data source initialization technologies 라고 하며 멀티 데이터소스에서 초기화 스크립트를 실행하는 것을 권장하지 않는다고 나와있다.) 이 상태에서 테스트를 실행하면 IllegalArgumentException 이 발생하거나 DataSourceInitializationConfiguration 에서 정상적으로 처리되지 않는다. 따라서 다음의 순서대로 적용해보았다.\nDataSourceInitializer 역할 확인 org.springframework.boot.autoconfigure.jdbc.DataSourceInitializer 파일을 확인해보면 initSchema() 메서드에서 runScripts 메서드를 호출하는 부분을 찾을 수 있는데 이부분이 바로 data.sql 을 실행하는 부분이다.\nList\u0026lt;Resource\u0026gt; scripts = getScripts(\u0026#34;spring.datasource.data\u0026#34;, this.properties.getData(), \u0026#34;data\u0026#34;); if (!scripts.isEmpty()) { if (!isEnabled()) { logger.debug(\u0026#34;Initialization disabled (not running data scripts)\u0026#34;); return; } String username = this.properties.getDataUsername(); String password = this.properties.getDataPassword(); runScripts(scripts, username, password); } 따라서 각각의 datasource configuration 에 다음과 같이 bean 을 등록해주는 코드를 추가해주었다.\nBean 등록 @Bean @Profile(\u0026#34;local\u0026#34;) // 테스트에서만 runScript 에 필요한 sql 스크립트이기 때문에 \u0026#39;local\u0026#39; profile 에서만 bean 을 등록한다. public DataSourceInitializer dataSourceInitializer(@Qualifier(\u0026#34;accountDataSource\u0026#34;) DataSource datasource) { ResourceDatabasePopulator resourceDatabasePopulator = new ResourceDatabasePopulator(); resourceDatabasePopulator.addScript(new ClassPathResource(\u0026#34;account-data.sql\u0026#34;)); // 여기에서는 data.sql 을 사용하지 않고 account-data.sql 파일을 사용하였다. DataSourceInitializer dataSourceInitializer = new DataSourceInitializer(); dataSourceInitializer.setDataSource(datasource); dataSourceInitializer.setDatabasePopulator(resourceDatabasePopulator); return dataSourceInitializer; } 그리고 application.yml 파일에 다음과 같이 설정해주었다.\ndatasource.account: jdbcUrl: jdbc:h2:mem:testdb driverClassName: org.h2.Driver username: sa password: maximumPoolSize: 20 poolName: account-hikari data: account-data.sql datasource.content: jdbcUrl: jdbc:h2:mem:testdb driverClassName: org.h2.Driver username: sa password: maximumPoolSize: 20 poolName: content-hikari data: content-data.sql 정리 나의 경우에는 테스트에서 필요한 기초 데이터를 입력하기 위한 data.sql 파일을 사용하고 있었는데, 멀티 데이터소스 구조로 변경되면서 각각의 데이터베이스에 분리된 별도의 data.sql 을 실행할 필요가 있었다. 따라서 DataSourceInitializer Bean 을 직접 추가하므로써 분리된 각각의 account-data.sql, content-data.sql 이 실행될 수 있도록 하였다.\n참고 DataSource Configuration은 이름 순서대로 Component Scan 하므로 DataSource Configuration 파일의 이름이 중요할 수 있다. @Primary 어노테이션은 Data.sql 을 실행하는데 아무런 관계가 없다. application.yml 을 어떤게 작성하여 datasource bean 을 등록하느냐에 따라서 위의 코드는 달라질 수 있다. 참고자료 공식 매뉴얼 스택오버플러오 "
},
{
	"permalink": "https://findstar.pe.kr/tags/alpine-linux/",
	"title": "Alpine Linux",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/07/25/timezone-pkg-on-alpine-linux/",
	"title": "Alpine 리눅스에서 timezone 설정하기",
	"tags": ["alpine linux", "timezone"],
	"description": "",
	"type": "post",
	"contents": "Alpine 리눅스에서 Timezone 설정하기 도커를 빌드할 때 주로 사용하는 것이 바로 Alpine 리눅스이다. 도커를 사용할 때 경량의 이미지를 기반으로 하면 이미지 pull 속도가 빨라 빠른 배포에 도움이 되어 자주 사용하고 있다. 다만 너무 경량이다 보니 필요한 패키지를 추가로 설치해줘야 하는 경우가 있는데 이미지에 timezone 을 추가로 지정하는 방법을 살펴보았다.\nAlpine Linux Alpine 리눅스는 보안, 단순함, 적은 자원을 사용하는 리눅스로 위키 링크 에서 대부분의 매뉴얼을 찾을 수 있다. 도커 허브에서는 베이스 이미지로 제공하고 있으며 로컬 머신에서 이미지를 다운로드 받아보면 얼마나 작은 사이즈를 유지하는지 확인할 수 있다. 다음처럼 확인했을 때 불과 5.6MB 에 이미지 사이즈를 확인할 수 있었다.\n$ docker image pull alpine:latest Using default tag: latest latest: Pulling from library/alpine 5843afab3874: Already exists Digest: sha256:234cb88d3020898631af0ccbbcca9a66ae7306ecd30c9720690858c1b007d2a0 Status: Downloaded newer image for alpine:latest docker.io/library/alpine:latest $ docker images alpine 3.14.0 d4ff818577bc 5 secods ago 5.6MB timezone 설정하기 정확한 내용은 위키 문서를 참고하였다. 일단 alpine 에서는 yum, apt 와 같은 패키지 매니저를 사용할 수 있는데 바로 apk 명령어다. 타임존을 설정하기 위해서는 tzdata 패키지가 필요하다.\napk add tzdata 패키지를 설치하는 것에 더해 /etc/timezone 을 지정해줘야한다.\necho \u0026#34;Asia/Seoul\u0026#34; \u0026gt; /etc/timezone 도커에서 설정하기 도커파일에서는 다음과 같이 사용하고 있다.\nENV TZ=Asia/Seoul RUN apk add --no-cache tzdata \u0026amp;\u0026amp; \\ cp /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; \\ echo $TZ \u0026gt; /etc/timezone 참고자료 알파인 리눅스 도커 허브 이미지 timezone 지정 위키 문서 "
},
{
	"permalink": "https://findstar.pe.kr/tags/timezone/",
	"title": "Timezone",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/fuse.js/",
	"title": "Fuse.js",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/07/10/fuse-search-on-hugo/",
	"title": "fuse.js를 사용하여 hugo 블로그에 검색 기능 추가 하기",
	"tags": ["fuse.js", "search on hugo"],
	"description": "",
	"type": "post",
	"contents": "fuse.js 로 Hugo 블로그에 검색 기능 추가하기 그 동안 이 블로그에는 검색기능이 빠져 있었다. 😱 콘텐츠를 제공하는 사이트라면 기본적으로 제공하는 기능인데, 정적사이트에서 어떻게 하는지 몰라서 못하고 있었다. (사실 사이트 유입의 대부분인 구글 검색에 의존하고 있었다.) 그러다가 hugo 를 기반으로한 정적사이트에서 fuse.js를 사용하면 된다는 사실을 알게되어 바로 적용해보았다.\nfuse.js fuse.js 는 작고 강력한 fuzzy search 라이브러리로 다른 라이브러리 의존성이 없다는 특징이 있다. 따라서 적용이 아주 쉬운 장점이 있다. fuse.js 데모 에서 확인해보면 한글 검색도 잘 지원한다.\nfuzzy search fuzzy search 란 검색의 일종으로 완벽하게 일치하는 검색이 아닌 흐린(fuzzy) 검색이라는 뜻으로, 쉽게 생각해서 검색어가 완벽하게 일치하지 않더라도 대상 콘텐츠를 찾을 수 있는 검색을 의미한다. 유사 검색이라고도 하는데, 이를 구현하기 위한 알고리즘은 여러가지가 있고 세부적인건 복잡하기 때문에, 아주 쉽게 예를 들어 \u0026ldquo;떡국\u0026rdquo; 이라는 단어를 찾기 위해서 \u0026ldquo;떡\u0026rdquo; 이라는 검색어로 찾을 수 있는 경우가 바로 이 fuzzy search 라고 이해할 수 있다.\nhugo 에 적용하기 fuse.js 가 아주 간단하게 fuzzy 검색 기능을 제공하지만 hugo 에 적용하려면 몇가지 작업이 필요하다. 직접 다 만들기는 어려우니 hugo theme 로 만들어진 자료를 참고하자. 참고 를 확인하여 다음과 같은 순서로 적용할 수 있다.\n1. index.json 템플릿을 적용 config.toml 의 output 에 json 타입을 추가한다. (콘텐츠를 index.json 주소로 접근 가능하게 한다.) layouts/_default/index.json 에 json 으로 표시될 콘텐츠의 템플릿을 적용한다. [{{ range $index, $page := .Site.Pages }} {{- if ne $page.Type \u0026#34;json\u0026#34; -}} {{- if and $index (gt $index 0) -}},{{- end }} { \u0026#34;permalink\u0026#34;: \u0026#34;{{ $page.Permalink }}\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;{{ htmlEscape $page.Title}}\u0026#34;, \u0026#34;tags\u0026#34;: [{{ range $tindex, $tag := $page.Params.tags }}{{ if $tindex }}, {{ end }}\u0026#34;{{ $tag| htmlEscape }}\u0026#34;{{ end }}], \u0026#34;description\u0026#34;: \u0026#34;{{ htmlEscape .Description}}\u0026#34;, \u0026#34;contents\u0026#34;: {{$page.Plain | jsonify}} } {{- end -}} {{- end -}}] 2. 검색 랜딩 페이지 구성 content/search/index.md 파일을 생성하여 검색 랜딩 페이지를 구성한다. --- title: Search Result layout: search --- 참고자료의 fuse.js, search.js 를 적용한다. \u0026lt;!--검색 결과가 출력되는 부분 id=\u0026#34;search-results\u0026#34; 에 추가된다.--\u0026gt; \u0026lt;div class=\u0026#34;inner\u0026#34;\u0026gt; \u0026lt;article class=\u0026#34;post-full post page no-image\u0026#34;\u0026gt; \u0026lt;header class=\u0026#34;post-full-header\u0026#34;\u0026gt; \u0026lt;h1 class=\u0026#34;post-full-title\u0026#34;\u0026gt;Search Result\u0026lt;/h1\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;section class=\u0026#34;post-full-content\u0026#34; id=\u0026#34;search-results\u0026#34;\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;/article\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!--검색 결과가 출력되는 템플릿 search.js 에 의해서 다음과 같은 포맷으로 표시된다.--\u0026gt; \u0026lt;template id=\u0026#34;search-result-template\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;search_summary\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;post-title no-text-decoration\u0026#34;\u0026gt;\u0026lt;a class=\u0026#34;search_link search_title\u0026#34; href=\u0026#34;\u0026#34;\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em class=\u0026#34;search_snippet\u0026#34;\u0026gt;\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;small\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr class=\u0026#34;search_iftags\u0026#34;\u0026gt; \u0026lt;td\u0026gt;\u0026lt;strong\u0026gt;Tags\u0026lt;/strong\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;search_tags\u0026#34;\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr class=\u0026#34;search_ifcategories\u0026#34;\u0026gt; \u0026lt;td\u0026gt;\u0026lt;strong\u0026gt;Categories\u0026lt;/strong\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;search_categories\u0026#34;\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/small\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;!--필요한 js 파일을 추가한다.--\u0026gt; {{ $assetBusting := not .Site.Params.disableAssetsBusting }} \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;{{\u0026#34;js/libs/fuse.js/3.2.1/fuse.min.js\u0026#34; | relURL}}{{ if $assetBusting }}?{{ now.Unix }}{{ end }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;{{\u0026#34;js/libs/mark.js/9.0.0/mark.min.js\u0026#34; | relURL}}{{ if $assetBusting }}?{{ now.Unix }}{{ end }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;{{\u0026#34;js/search.js\u0026#34; | relURL}}{{ if $assetBusting }}?{{ now.Unix }}{{ end }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; css 스타일링은 적당히 변경한다. 3. header 영역에 검색 입력창 추가 theme 에서 사용하는 header 부분에 검색 입력창을 추가한다. submit 하면 추가한 검색 랜딩 페이지로 연결되도록 한다. \u0026lt;div id=\u0026#34;search\u0026#34;\u0026gt; \u0026lt;form method=\u0026#34;get\u0026#34; action=\u0026#34;/search\u0026#34;\u0026gt; \u0026lt;fieldset \u0026gt; \u0026lt;div class=\u0026#34;field\u0026#34;\u0026gt; \u0026lt;label\u0026gt;\u0026lt;span class=\u0026#34;hide\u0026#34;\u0026gt;검색\u0026lt;/span\u0026gt; \u0026lt;i class=\u0026#34;fa fa-search\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;searchtext\u0026#34; class=\u0026#34;input_text\u0026#34; name=\u0026#34;q\u0026#34; value=\u0026#34;\u0026#34; size=\u0026#34;15\u0026#34; \u0026gt; \u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/fieldset\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; 4. 검색 기능이 잘 되는지 확인한다. 기타 fuse.js 말고도 다른 방법을 사용하여 hugo 사이트에 검색 기능을 추가할 수 있다. 나의 경우에는 처음에 lunr 를 도입시도해보다가 한글 지원지 잘 되지 않아서 포기하고 fuse.js 로 검색되도록 작업을 완료하였다. 다른 검색 기능을 살펴본다면 공식 사이트 매뉴얼을 살펴보자.\n소감 hugo 로 변경한 뒤에 내심 검색기능을 구글에만 의존하고 있어서 자체 검색을 추가해야지 해야지 하면서 시간만 보내고 있었는데 간단하게 작업을 완료하고 나니 한결 보기 좋아졌다는 기분이 든다. 그 동안 왜 못한다고 생각하고 있었던 걸까?;; 앞으로 방문자들에게 조금이나마 더 편하게 글을 찾을 수 있도록 도움되길 바란다.\n참고자료 hugo 공식 매뉴얼의 검색 기능 안내 fuse.js hugo fuse.js 도입 참고 "
},
{
	"permalink": "https://findstar.pe.kr/tags/search-on-hugo/",
	"title": "Search on Hugo",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/developer/",
	"title": "Developer",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/how-do-i-work/",
	"title": "How Do I Work",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/06/30/how-do-I-work/",
	"title": "나는 어떻게 일하는 사람인가?",
	"tags": ["developer", "how do I work"],
	"description": "",
	"type": "post",
	"contents": "배경 올 초에 회사에서 진행하는 TF 조직에 급하게 합류하게 되면서 4년 6개월동안 일했던 조직을 인수인계도 없이 떠나게되었다. 조직이동 후 인수인계를 제대로 못해놓은 것이 내심 마음에 걸렸는데, 감사하게도 뒤늦게 나마 인수인계겸 개인 회고 자리가 마련되었다. 인수인계 이외에 개인 회고를 어떻게 진행할까 고민하다가 내가 어떻게 일하는 사람인지 정리해서 공유해보자고 마음먹었다. 그 동안 내가 조직에서 무슨 일을 했고, 어떻게 일하려고 노력했는지, 아쉬운건 어떤 것들이 있었는지 정리해 발표를 했다. 회고 발표 \u0026ldquo;나는 어떻게 일하는 사람인가?\u0026rdquo; 시작은 회고를 하기 위함이었지만, \u0026ldquo;나는 어떻게 일하는 사람인가?\u0026ldquo;라는 아이디어를 떠 올리고 보니 이런 주제는 그 동안 의식적으로 생각해보지 못했다는 걸 알았다. 이번기회에 정리를 통해서 내가 어떻게 일하려고 노력하는지 돌이켜 보았다.\n1. 비지니스를 이해하는 개발자 비지니스를 이해하는 개발자라는 건 내가 지향하는 방향이다. 조금 더 풀어서 이야기 해보자면, 내가 맡은 프로덕트가 어떻게 해서 진행되는지 깊게 이해하려고 노력한다는 뜻이다.\n프로덕트가 어떤 산업 영역에 포함되어 있고 어떤 포지션을 차지하고 있으며 이 비지니스에서 제일 중요한 목표는 무엇인지 그리고 이 비지니스를 진행하고 있는 조직(팀 또는 회사)이 현재 집중하고 있는 부분은 어디이며 따라서 내가 어떤 역할을 수행해야 팀과 프로덕트에 기여 할 수 있는지에 대해서 가능한 구체적으로 이해하기 위해 노력하는 것이다. 이렇게 하는 이유는 내가 개발자 이기 때문에 실제 제품의 구현에 집중하게 되는데 이 때 디테일에 몰입되기 쉬워 프로덕트의 목표를 잊기 쉽기 때문이다.\n그래서 항상 전체 그림을 잊지 않고 팀의 방향과 목표를 잊지 않으려고 노력하기 위해서 목표를 정의했다.\n행동양식 비지니스를 이해하는 개발자라는 목표를 이루기 위해서는 몇가지 행동양식이 필요하다.\n첫째로 항상 그림을 그려보자 라는 생각을 바탕으로 가능한 다이어 그램 형태로 (꼭 컴퓨터가 아니더라도 손으로 그리기도 한다.) 함축적으로 표현해보는 노력을 들인다. 그림으로 잘 표현이 되지 않으면 뭔가 명확하게 알고 있지 않은 부분이 있다고 생각하고 그런 부분을 좀 더 파악하려고 노력을 한다.\n두번째로 100번 생각해보기 라는 행동양식을 지향하고 있는데, 이건 새로운 문제를 해결해야 할 때\n아이디어가 실제로 이 문제를 해결해 줄 수 있는지 리소스를 고려할 때 실현 가능한지 이슈가 될 부분은 없는지 기존의 구현에서 사이드 이펙트가 발생할 부분은 없는지 추가 리소스가 필요하다면 어떤 리소스인지 정도를 생각해 보는 것이다. 아이디어를 구체화 나가는 과정에서 다양한 생각을 계속 해보면서 사고를 넓히려고 노력하는 방법이다. 2. Be proactive 개인적인 측면에서 비지니스를 이해하고 역할을 명확히 하고자 하였다면 거기에서 그치지 않고 조직에 기여할 수 있어야 한다고 생각한다. 이때 기여를 잘 하기 위해서는 팀/조직 안에서는 동료들과의 협업을 잘 하는 것이 중요한데 이 때 내가 어떤 태도를 지향하는가가 중요하다고 생각한다. Be proactive 라는 것은 원활한 협업/커뮤니케이션/신뢰를 위해 내가 갖추어야할 태도라고 생각한다.\nProactive? 한국어로 딱 맞아 떨어지는 단어가 떠오르지 않는데 영어로 Proactive 라는 단어가 있다. Reactive 와 대비되는 말이라고 이해할 수 있는데, 풀어서 이야기 하자면 주변의 상황에 반응하지 말고 먼저 행동하자는 뜻이된다. active 가 활동적이다라는 의미로 해석한다면 pro 가 붙었으니 그보다 더 적극적이고 능동적이라고 해석할 수 있다.\n행동양식 Proactive 하게 조직에 기여하는 방식도 몇가지 행동 양식을 정했다. 알고 있는 것을 공유한다.\n내가 알게된 지식/노하우를 공유해야한다. 나만 알고 있는 정보가 있다면 내가 자리를 비웠을 때 문제가 발생할 수 있다. 그리고 공유를 통해서 타인에게 설명할 수 있어야 100% 이해하는 지식이다 라는 인식을 가지고 있다. 나의 이슈를 알린다.\n내가 무엇을 하고 있는지, 내가 어떤 어려움이 있는지 말하지 않으면 상대가 눈치껏 알아서 도와줄 수 없다. 동료에게 알리고 함께 고민하면 새로운 아이디어도 얻을 수 있고 상대방의 고민도 들어주면서 각자의 신뢰도가 높아진다고 생각한다. 어슬렁 거리기\n딱히 뭐라 말할 수 없지만 생각을 환기시키거나 이슈의 중간중간 시간에 동료들과 커피를 함께 하거나 다양한 이야기를 나누려고 노력한다. (물론 인터럽트를 거는건 아닌지 주의하며) 도움이 필요한 동료들이 없는지 살펴보고 관심을 가지려고 노력한다. 일거리가 보이면 그냥한다.\n팀에서 일하다보면 누구나 하기 싫은 일이 생기기 마련이다. 가급적이면 내가 그 일을 하는 것이 가장 빠르게 일을 처리할 수 있는 방법일 수도 있다. 동료들에게 도움이 되도록 일할 거리가 보이면 그냥 하려고 노력한다. 이 때 스트레쓰/귀찮음 그 자체를 생각하지 않으려고 노력한다. 3. 남들 보다 느려도 여유있게 나는 학습 속도가 빠르다던가, 머리가 비상해서 한번에 문제를 해결하는 그런 능력은 가지고 있지 않다. 따라서 나는 역량을 효율적으로 그리고 꾸준히 사용해야만\n동료들과 보조를 맞추어 목표를 이루어나갈 수 있다. 그렇기 때문에 나는 보다 많이 고민하고 전체를 이해하고 시작하려고 하고 실수를 덜 하려고 노력한다. 그래야만 조금 느리더라도 덜 시행착오를 거치니까 말이다. 하나의 목표를 정하고 머리속에 목표의 전체 모습을 그려보고 하나씩 살을 붙이는 방식으로 일하려고 노력한다. 그렇게 해야만 느린 나의 속도를 가지고서도 적절한 시간 내에 결과를 만들어 낼 수 있다고 생각한다. 물론 남들 보다 느려도 여유있게 를 목표로 하지만 실상은 고민에 고민을 거듭해야 남들만큼 인것은 비밀. 😭\n행동양식 다음은 느리더라도 여유있게 일 하기 위한 행동양식이다. 왜 필요한가 생각한다.\n이 이슈의 근본적인 목표와 원인이 무엇인지 생각해본다. 무엇이 중요한지 생각한다.\n왜를 알게 되었으면 무엇이 중요한 것인지 생각해본다. 사용성/기능성/내구성/유지보수 용이성등을 고민해본다. 어떻게 해결할지 생각한다.\n코드를 추가할 것인지? 정책을 추가해야할 것인지 고민할 때가 있다. 때로는 코드 없이 정책을 바꾸는게 더 문제의 근본 원인을 해결하는 방향일수 있기 때문이다. 운영 리소스의 효율성을 생각한다.\n새로운 기능을 추가할 때 유지보수가 용이하게 구성해야한다. 또한 사용하는데 리소스가 적게 들어야 커뮤니케이션 비용을 낮게 유지할 수 있다. 해결의 결과를 어떻게 측정할지 생각한다.\n문제 해결을 완료한다는 것을 어떻게 확인할 것인지 고민해본다. 무엇이 해결인지 정의하기 위해서는 왜 필요한지 고민하는 게 꼭 필요하다. 뒷정리는 어떻게 할것인지 생각한다.\n문제를 해결하고 나면 시행착오 과정에서 발생한 불필요한 찌꺼기들이 남기 마련이다. 뒷정리가 깔끔해야 나중에 나의 발목을 잡지 않는다. 요약 다음은 내가 어떻게 일하려고 노력하는지에 대한 요약이다.\n비지니스를 이해하는 개발자가 되기 위해서 Be proactive 하게 조직에 기여하고 남들보다 느려도 여유있게 가려고 노력한다. 그리고 위의 목표를 위해서 다양한 행동양식을 정의하고 이를 따르려고 노력한다.\n회고 발표 후기 위의 내용을 정리해서 업무와 관련된 내용을 덧붙여 전 조직에 회고 발표를 진행했다. 그 동안 조직내에서 기술공유는 종종 했었지만 개인적인 이야기는 해본적이 없어서 당일날 발표하기 전에 문득 이런 내용을 발표해도 되는가라는 불안감이 들었다. 평소에는 잘 떨지 않는 다고 자부했지만 이날 발표는 덜덜덜 떨면서 발표를 진행했는데 다행히도(!) 동료들의 \u0026ldquo;좋은 자극이 되었다\u0026quot;는 코멘트가 많아 감사한 마음이 들었다. 그렇게 4년 6개월간 함께 일한 동료들 앞에서 개인 회고 발표를 진행하며 나 자신에 대한 생각을 정리할 수 있었다. 나 스스로가 어떻게 일하는 사람인가 알아 갈 수록 자기 자신을 더 잘 이해하게 되었다는 느낌이 들었다. 블로그를 빌려 지난 4년 6개월간 함께한 동료들에게 다시금 감사의 인사를 전한다. 🙇🏻\n\u0026ldquo;제일 좋은 복지는 좋은 동료. 좋은 동료들 덕분에 즐겁게 일할 수 있었다. 언젠가 다시 함께 할 수 있기를 바라며!\u0026rdquo;\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/cronjob/",
	"title": "Cronjob",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/curl/",
	"title": "Curl",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/06/13/curl-cronjob-k8s/",
	"title": "curl 을 사용하여 K8S(Kubernetes) 에서 cronjob 등록하기",
	"tags": ["curl", "k8s", "cronjob"],
	"description": "",
	"type": "post",
	"contents": "k8s 에서 curl 을 사용하여 cronjob 등록하기 배경 종종 쿠버네티스 클러스터에서 cronjob 을 활용할 일이 있는데 간단하게 curl 명령어 한번만 실행하면 되는 경우가 있다.\n이럴 때 사용하기 위한 alpine 기반의 curl 이미지를 사용하는 방법에 대해서 알아보았다.\nk8s cronjob 쿠버네티스의 오브젝트 중에서 cronjob 오브젝트는 yml 에 정의된 스케줄에 맞게 지정한 이미지를 기반으로 명령어를 실행하는 오브젝트다. 나는 다음과 같은 형식으로 사용하고 있다.\napiVersion: batch/v1beta1 kind: CronJob metadata: name: my-cronjob-name spec: schedule: \u0026#34;*/5 * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: my-cronjob-name-pod-name image: curlimages/curl:7.77.0 imagePullPolicy: IfNotPresent command: - \u0026#34;/bin/sh\u0026#34; - \u0026#34;-c\u0026#34; - | curl -X \u0026#34;POST\u0026#34; \u0026#34;somehost\u0026#34; \\ -H \u0026#39;header-key: value\u0026#39; \\ -H \u0026#39;Content-Type: application/json; charset=utf-8\u0026#39; restartPolicy: Never 설명 먼저 apiVersion 에 맞게 CronJob 을 등록한다.\nschedule 는 리눅스의 cronjob 을 등록하는 포맷과 동일한데 위의 예제에서는 5분 마다 실행하라고 정의하였다.\nimage 는 curlimages/curl:7.77.0 을 사용하였는데 필요하다면 alpine 기반으로 apk --no-cache add curl 을 추가하여 직접 이미지를 생성하여 사용해도 된다.\ncommand 부분에서 | 로 이어지는 부분으로 연결하면 curl 명령어를 raw 하게 정의할 수 있다. 콘솔에서 입력하는 형태 그대로 정의하자.\n실행 기록 kubectl get pods 로 확인해보면 다음과 같이 언제 완료되었는지 확인이 가능하고 필요하다면 실행 결과를 로그로 확인할수도 있다.\n$ kubectl get pods my-cronjob-name-1623488400-tb4zs 0/1 Completed 0 16s $ kubectl logs my-cronjob-name-1623488400-tb4zs Response OK 참고 https://nieldw.medium.com/using-a-kubernetes-cronjob-the-team-city-api-to-trigger-a-regular-backup-17dfb076d83a\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/confluence/",
	"title": "Confluence",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/customizing/",
	"title": "Customizing",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/sidebar-search/",
	"title": "Sidebar Search",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/04/21/confluence-sidebar-search/",
	"title": "컨플루언스 사이드바에 검색 기능 추가하기",
	"tags": ["confluence", "sidebar search", "customizing"],
	"description": "",
	"type": "post",
	"contents": "컨플루언스 사이드바에 검색 기능 추가하기 배경 신규 프로젝트를 진행하면서 위키로 사용중인 컨플루언스에 새로운 스페이스를 생성하게 되었다. 동료들의 권한설정을 마치고 몇가지 컨벤션을 정한 다음 스페이스의 레이아웃을 살펴보았다. 뭔가 허전함을 느껴서 그게 뭘까 하고 생각하던 중에, 이전 스페이스에서 사용하던 사이드바 검색 기능이 빠져있는 것을 발견하였다. 막상 설정하려다 보니, 금방 할 줄 알았는데 한참이나 헤메게 되어서 나중에 까먹지 않으려고 정리해보았다.\n사이드바 검색이 필요한 이유 컨플루언스 위키에는 기본적으로 상단 헤더영역에 검색기능이 있지만, 여기서 제공하는 검색은 전체 통합 검색이다. 그래서 스페이스가 많을 때는 특정 스페이스만 지정하여 검색하려면 불편하다. 따라서 사이드바에 검색기능을 추가하면 해당 스페이스의 문서만 검색이 쉬워지므로 사용성이 좋아진다.\n설정 방법 먼저 스페이스에 진입후 왼쪽 하단의 공간 도구를 눌러 레이아웃(모양새)를 선택하자\n다음으로 사이드바 영역을 선택하자 사이드바 설정에 다음과 같이 매크로를 등록하자 끝이다. 이제 사이드바에 검색 기능이 잘 나오는지 확인한다. 참고 https://community.atlassian.com/t5/Confluence-questions/Sidebar-Search-in-a-confluence-Space/qaq-p/888718\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/istio/",
	"title": "Istio",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/04/14/istio-traffic-management/",
	"title": "istio의 트래픽 관리 기능 살펴보기",
	"tags": ["istio", "traffic management", "kubernetes"],
	"description": "",
	"type": "post",
	"contents": "istio의 트래픽 관리 기능 살펴보기 배경 istio 스터디의 두번째 내용으로 istio 의 트래픽 관리 기능을 살펴보았다. 매뉴얼을 참고로 하여 gateway, virtual service, destination rule 을 살펴보았다.\n구동 환경 istio 1.9.1 k8s on docker desktop istio on docker desktop 참고 트래픽 관리 기능 istio 는 envoy 를 사용하여 sidecar 가 적용된 pod 의 트래픽을 관리할 수 있다. 순수하게 k8s 의 기능을 사용하면 ingress, deployment 만 사용하게 되었을 텐데 istio 를 사용하면 gateway, virtualService, destinationRule 을 다루게 된다.\nGateway 먼저 Gateway istio 서비스 메쉬로 유입되는 관문을 나타낸다. 게이트웨이는 노출되는 포트, 프로토콜, 호스트, TLS 정보를 담고 있다. 다음은 이전 포스트에서 예제로 실행한 bookinfo-gateway.yml 파일에 들어 있는 내용이다.\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; 위의 yml 을 해석해보자면 먼저 selector 에 나타나 있는 istio: ingressgateway 는 로드밸런서로 동작하는 istio-ingressgateway 에서 실행되라는 의미가 된다. operator 를 통해서 설치한 ingressgateway 컴포넌트를 통해 실행된 pods 의 라벨을 확인해보자\n$ kubectl get pods/istio-ingressgateway-67d647b4-b6k84 --show-labels -n istio-system # (pods 이름은 그때 그때 각기 다를 수 있다.) istio-ingressgateway-67d647b4-b6k84 1/1 Running 0 31m app=istio-ingressgateway,chart=gateways,heritage=Tiller,install.operator.istio.io/owning-resource=unknown,istio.io/rev=default,istio=ingressgateway,operator.istio.io/component=IngressGateways,pod-template-hash=67d647b4,release=istio,service.istio.io/canonical-name=istio-ingressgateway,service.istio.io/canonical-revision=latest,sidecar.istio.io/inject=false 결과에서 보면 istio=ingressgateway 라는 라벨을 확인할 수 있다. ingressgateway 에서 실행되는 논리적인 게이트웨이 나타낸것이 바로 Gateway 컴포넌트이다. 게이트웨이는 VirtualService 와 함께 사용되어 트래픽을 제어하게 된다.\nVirtualService VirtualService 는 트래픽 라우팅과 관련된 설정이다. gateway 와 결합하여 트래픽을 제어한다. 다음은 bookinfo 예제에서 사용한 yml 이다.\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \u0026#34;*\u0026#34; gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 spec 을 살펴보면 연결할 게이트웨이를 gateways 에 bookinfo-gateway로 표시해주었다. http 설정은 http 프로토콜만을 설명하는 것은 아니고 HTTP, HTTP2, GRPC 를 포함한다. 그 다음으로 매칭 룰을 지정하는데 (match) exact 는 정확히 매칭되어야 하는 경우 prefix 는 uri 의 앞부분만 매칭되면 되는경우로 해석된다. route 에서는 연결할 대상을 지정하는데 host 가 productpage 로 되어 있는 것은 앞선 예제에서 등록한 Service 컴포넌트로 연결하라는 의미가 된다. (k8s 내부에서 서비스명으로 바로 접속할 수 있기 때문)\nGateway 와 VirtualService 는 실제로 별도의 pods 를 실행하는 것은 아니고 ingressgateway 의 envoy 설정을 변경한다.(중요) ingressgateway 의 라우트 설정은 다음의 명령어로 확인할 수 있다. 자세히 보면 prefix 로 지정한 uri 는 match 뒤에 * 표시가 붙어 있는 것을 확인할 수 있다.\n$ istioctl pc routes istio-ingressgateway-67d647b4-b6k84.istio-system NOTE: This output only contains routes loaded via RDS. NAME DOMAINS MATCH VIRTUAL SERVICE http.80 * /productpage bookinfo.default http.80 * /static* bookinfo.default http.80 * /login bookinfo.default http.80 * /logout bookinfo.default http.80 * /api/v1/products* bookinfo.default * /healthz/ready* * /stats/prometheus* VirtualService 에는 라우팅을 위한 좀 더 디테일한 설정도 가능하다\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews-route spec: hosts: - reviews.prod.svc.cluster.local http: - headers: request: set: test: \u0026#34;true\u0026#34; route: - destination: host: reviews.prod.svc.cluster.local subset: v2 weight: 25 - destination: host: reviews.prod.svc.cluster.local subset: v1 headers: response: remove: - foo weight: 75 대표적으로 header를 수정할 수도 있고, destination 에 weight 값을 지정할 수도 있다. (가중치 값은 합쳐서 100이 되어야 한다.) 위의 yml 에서는 test:true 라는 헤더 값을 추가하고 v1 subset 에 대해서만 응답에서 foo 라는 헤더값을 제거한다.\nDestinationRule VirtualService 가 어느 서비스로 트래픽을 라우팅할지 결정했다면, DestinationRule 은 어디로 라우팅할지 결정된 이후에 트래픽에 적용되는 정책을 정의하는 기능이다. (트래픽이 어디로 갈지 결정된 이후에 트래픽을 어떻게 보낼지 정의한 것이라고 이해하면 된다.) 이렇게 이야기 하면 감이 잘 오지 않는데 간단한 예시로 pods 의 라벨에 따라서 부하를 분산 시킬 때 사용하는 것이 바로 DestinationRule 이다.\n다음은 bookinfo 예제에서 사용하는 DestinationRule yml 이다.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: productpage spec: host: productpage subsets: - name: v1 labels: version: v1 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: ratings spec: host: ratings subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v2-mysql labels: version: v2-mysql - name: v2-mysql-vm labels: version: v2-mysql-vm --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: details spec: host: details subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- 자세히 보면 subsets 라고 보이는데 pods 의 label 을 그룹핑한 단위라고 보면 된다. pods 를 하나의 subset 으로 묶은 다음에 트래픽을 제어하는데 별도의 정책이 지정되지 않았으므로 라운드 로빈이 지정된다. 설정 가능한 옵션은 문서를 참고하자. 다음과 같이 기본 정책을 지정하고 특정 subset 에 대해서만 다른 정책을 취할 수도 있다.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings spec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_CONN # 커넥션이 더 적은 쪽으로 연결한다. subsets: - name: testversion labels: version: v3 trafficPolicy: loadBalancer: simple: ROUND_ROBIN # 라운드 로빈으로 연결한다(기본값) 다음은 user 라는 쿠키값을 해싱한 결과값을 기반으로 연결을 설정한다.\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings spec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: consistentHash: httpCookie: name: user ttl: 0s 기타 옵션 다음은 istio 공식 사이트에서 제공한는 Gateway 예제이다.\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: my-gateway namespace: some-config-namespace spec: selector: app: my-gateway-controller servers: - port: number: 80 name: http protocol: HTTP hosts: - uk.bookinfo.com - eu.bookinfo.com tls: httpsRedirect: true # HTTP 로 접근시 301 리다이렉트 반환 - port: number: 443 name: https-443 protocol: HTTPS hosts: - uk.bookinfo.com - eu.bookinfo.com tls: mode: SIMPLE # 이 포트에 대해서 HTTPS 활성화 serverCertificate: /etc/certs/servercert.pem privateKey: /etc/certs/privatekey.pem - port: number: 9443 name: https-9443 protocol: HTTPS hosts: - \u0026#34;bookinfo-namespace/*.bookinfo.com\u0026#34; tls: mode: SIMPLE # 이 포트에 대해서 HTTPS 활성화 credentialName: bookinfo-secret # k8s secret 에 등록된 이름 - port: number: 9080 name: http-wildcard protocol: HTTP hosts: - \u0026#34;*\u0026#34; - port: number: 2379 # 내부 서비스를 2379포트 번호를 통해서 외부에 노출한다. name: mongo protocol: MONGO # 프로토콜은 HTTP, HTTPS, GRPC, HTTP2, MONGO, TCP, TLS 중 하나여야 한다. hosts: - \u0026#34;*\u0026#34; 트래픽의 실제 연결 흐름 새로운 용어가 계속 나오기 때문에 실제 트래픽이 어떻게 제어되는지 헷갈려서 정리해보았다.\n로드밸런서로 부터 트래픽이 유입된다. ($ kubectl get svc -n istio-system 결과값을 생각해보자.) 로드밸런서 타입의 서비스(ingressgateway controller)는 실제로는 istio-ingressgateway 라는 pods 를 실행시킨다. k8s 의 ingress controller 가 실제로 nginx pods 를 띄우는 것과 동일하다. 다만 다른점은 envoy 를 띄운다는점 IngressGateway 의 pods 는 Gateway 와 VirtualService 에 의해서 configuration 이 결정된다. $ istioctl pc routes istio-ingressgateway-67d647b4-b6k84.istio-system 와 같이 등록된 라우팅 설정을 확인할 수 있다. Gateway 는 포트, 프로토콜, TLS 등 인증을 설정한다. VirtualService 는 어떤 서비스로 연결될 것인지 라우팅을 설정한다. 이것만으로도 트래픽이 연결되지만 트래픽 연결 정책을 추가하고 싶은 경우 DestinationRule 을 지정할 수 있다. 여기서는 pods 의 라벨을 하나로 그룹핑하여 Subset 이라고 한다. 참고 https://istio.io/latest/docs/reference/config/networking/gateway/ https://istio.io/latest/docs/reference/config/networking/virtual-service/ https://istio.io/latest/docs/reference/config/networking/destination-rule/ https://istio.io/latest/docs/ops/diagnostic-tools/proxy-cmd/ https://istio.io/latest/docs/examples/bookinfo/ https://blog.jayway.com/2018/10/22/understanding-istio-ingress-gateway-in-kubernetes/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/traffic-management/",
	"title": "Traffic Management",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/docker-desktop/",
	"title": "Docker Desktop",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/03/25/install-istio-on-docker-desktop/",
	"title": "Docker desktop 에서 istio 설치하기",
	"tags": ["istio", "docker desktop", "kubernetes"],
	"description": "",
	"type": "post",
	"contents": "Docker desktop 에 istio 를 설치하는 방법 배경 최근 istio 스터디에 참여하고 있는데 istio 를 설치하기 위해서 kubernetes 클러스터 환경이 필요했다. 문득 로컬 docker desktop 에 kubernetes 지원이 있었던 기억이 나서 이 옵션을 활성화하여 docker desktop 에서 istio 를 설치해보았다.\n설치 환경 istio 1.9.1 docker desktop 설치된 환경 (RAM 이 넉넉해야한다.) Docker desktop 준비사항 먼저 resource 에서 CPU 4cpu 이상, RAM 할당을 8G 로 늘려주자. 그 다음으로 kubernetes 클러스터를 활성화 시켜주자. 클러스터가 잘 활성화 되었다면 다음과 같이 확인할 수 있다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION docker-desktop Ready master 2h v1.19.3 istioctl 설치 brew 를 사용해서 istioctl 을 설치해두자.\n$ brew install istioctl $ istioctl version 1.9.1 istio sample 예제 clone $ git clone https://github.com/istio/istio.git $ cd istio istio 설치 istio 를 활성화한 kubenetes cluster 에 설치해보자.\noperator init 이전 버전과 다르게 최근 istio 에서는 operator 를 사용한 설치가 가능하다. 편리하게 사용해주자. (airflow 도 operator 로 설치했던 기억이..)\n# --tags 를 붙여서 다른 버전을 설치할 수도 있다. 기본은 최신버전이 설치됨 # istio-operator controller 설치 + IstioOperator CRD(Custom Resource Definition) 설치됨 $ istioctl operator init Installing operator controller in namespace: istio-operator using image: docker.io/istio/operator:1.9.1 Operator controller will watch namespaces: istio-system ✔ Istio operator installed ✔ Installation complete IstioOperator apply ingress gateway 를 활성화 하기 위해서 demo profile 로 IstioOperator 를 apply 하자.\n$ kubectl create ns istio-system $ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: install.istio.io/v1alpha1 kind: IstioOperator metadata: namespace: istio-system name: example-istiocontrolplane spec: profile: demo # profile demo 로 하면 컴포넌트를 쭈루룩 설치한다. components: pilot: k8s: resources: requests: memory: 3072Mi # pilot 에는 메모리를 넉넉하게 할당하였다. egressGateways: - name: istio-egressgateway # 외부로 나가는 트래픽을 egressgateway 를 통하도록 활성화 한다. enabled: true EOF profile 에 따라서 설치되는 컴포넌트의 차이가 난다. 지금은 동작 확인을 위해서 demo profile 로 설치하지만 production 에서는 customization 이 필요할 수도 있다. 자세한 프로필 옵션은 매뉴얼 링크를 참고하자. 위의 예시로 설치하면 다음의 컴포넌트가 설치된다. istio-egressgateway istio-ingressgateway istiod istio-ingressgateway envoy 로 되어 있으며 들어오는(ingress) 트래픽을 전달하는 역할을 수행한다. istio 가 없을 때 ingress controller (주로 nginx 로 구동되는)가 이 역할을 수행한다. (경우에 따라 nginx ingress controller + envoy istio ingress gateway 조합으로도 구성하기도 한다고..)\nistio-egressgateway 마찬가지로 envoy 로 동작하며 밖으로 나가는(egress) 트래픽을 제어하는 역할을 수행한다. 어떤 경우에 쓰이냐면, 클러스터 외부에 있는 인프라에서 제한된 ACL 을 필요로 할 때 egress-gateway 가 구동되는 위치를 일부 node들로 제한하고 이 Node 들에 대해서만 ACL 을 열어주는 용도로 사용될 수 있다.\nistiod istio 의 컨트롤 플랜 (pilot - discovery , citadel - certification, galley - configuration management)\n추가 컴포넌트 설치 이전 버전에서는 demo 프로필로 설치하면 kiali, 등등의 컴포넌트가 많이 설치되었는데 최신버전에서는 추가 컴포넌트는 따로 설치해줘야 한다. (이렇게 변경된 이유는 kiali, zipkin, Prometheus와 같은 컴포넌트는 istio 의 core 컴포넌트가 아니고 이 녀석들도 버전업이 빨라서 demo profile 에 같이 포함시켰더니 버전업 대응하기가 어려웠다고 한다. 따라서 그 때그때 필요한 사람들이 add components 하라고 안내하고 있다.) $ cd istio/samples/addons $ kubectl apply -f prometheus.yml $ kubectl apply -f kiali.yml $ kubectl apply -f grafana.yml $ kubectl apply -f jaeger.yaml 설치 확인 Service\n$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) grafana ClusterIP 10.96.62.176 \u0026lt;none\u0026gt; 3000/TCP istio-egressgateway ClusterIP 10.97.243.254 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP istio-ingressgateway LoadBalancer 10.96.161.146 localhost 15021:30511/TCP,80:30544/TCP,443:31492/TCP,31400:30865/TCP,15443:32030/TCP istiod ClusterIP 10.103.241.162 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP jaeger-collector ClusterIP 10.106.227.172 \u0026lt;none\u0026gt; 14268/TCP,14250/TCP kiali ClusterIP 10.109.82.172 \u0026lt;none\u0026gt; 20001/TCP,9090/TCP prometheus ClusterIP 10.106.47.148 \u0026lt;none\u0026gt; 9090/TCP tracing ClusterIP 10.99.41.58 \u0026lt;none\u0026gt; 80/TCP zipkin ClusterIP 10.105.51.116 \u0026lt;none\u0026gt; 9411/TCP Pods\nNAME READY STATUS RESTARTS grafana-f766d6c97-5gj64 1/1 Running 0 istio-egressgateway-5d748f86d5-2drv7 1/1 Running 0 istio-ingressgateway-67d647b4-xhjh5 1/1 Running 0 istiod-69ccd7b848-9vwck 1/1 Running 0 jaeger-7f78b6fb65-xp4sm 1/1 Running 0 kiali-59c8574c55-zvlcr 1/1 Running 0 prometheus-69f7f4d689-4cswz 2/2 Running 0 네임스페이스에 istio-injection 라벨 추가 사이드카를 활성화할 default namespace 에 istio-injection=enable 라벨을 적용한다.\n$ kubectl label namespace default istio-injection=enabled namespace/default labeled BookInfo 예제 실행 sample yml 예제 적용 $ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml service/details created serviceaccount/bookinfo-details created deployment.apps/details-v1 created service/ratings created serviceaccount/bookinfo-ratings created deployment.apps/ratings-v1 created service/reviews created serviceaccount/bookinfo-reviews created deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created service/productpage created serviceaccount/bookinfo-productpage created deployment.apps/productpage-v1 created bookinfo 서비스 확인 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.110.253.197 \u0026lt;none\u0026gt; 9080/TCP 7s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2d productpage ClusterIP 10.109.81.216 \u0026lt;none\u0026gt; 9080/TCP 7s ratings ClusterIP 10.109.38.26 \u0026lt;none\u0026gt; 9080/TCP 7s reviews ClusterIP 10.101.176.215 \u0026lt;none\u0026gt; 9080/TCP 7s pods 확인 pod 상태를 확인하자. docker desktop 에서 띄우려니 사이드카 뜨는데 시간이 좀 걸린다. ready 2/2 보려고 조금 기다렸다. $ kubectl get pods -w NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-v7wq7 2/2 Running 0 3m27s productpage-v1-6b746f74dc-sgwxh 2/2 Running 0 3m26s ratings-v1-b6994bb9-mjhkg 2/2 Running 0 3m26s reviews-v1-545db77b95-p9lkm 2/2 Running 0 3m27s reviews-v2-7bf8c9648f-gmckn 2/2 Running 0 3m27s reviews-v3-84779c7bbc-qtrc8 2/2 Running 0 3m27s curl 요청 결과 확인 다음과 같이 Simple Bookstore App 이라는 문자열을 확인할 수 있으면 된다.\n$ kubectl exec \u0026#34;$(kubectl get pod -l app=ratings -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; -c ratings -- curl -sS productpage:9080/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; \u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; 네트워킹 설정 추가 아직 브라우저에서 편하게 접속하기가 어렵다. 네트워킹 설정을 추가하자\n$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml gateway.networking.istio.io/bookinfo-gateway created virtualservice.networking.istio.io/bookinfo created 설치된 gateway 를 확인할 수 있다.\n$ kubectl get gateway NAME AGE bookinfo-gateway 6s 접속 확인 External IP 확인 현재 Docker desktop 환경에서는 LoadBalancer 의 External IP 가 localhost 로 나온다.\n$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.96.62.176 \u0026lt;none\u0026gt; 3000/TCP 12h istio-egressgateway ClusterIP 10.97.243.254 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP 3h16m istio-ingressgateway LoadBalancer 10.96.161.146 localhost 15021:30511/TCP,80:30544/TCP,443:31492/TCP,31400:30865/TCP,15443:32030/TCP 3h16m istiod ClusterIP 10.103.241.162 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 3h16m jaeger-collector ClusterIP 10.106.227.172 \u0026lt;none\u0026gt; 14268/TCP,14250/TCP 12h kiali ClusterIP 10.109.82.172 \u0026lt;none\u0026gt; 20001/TCP,9090/TCP 12h prometheus ClusterIP 10.106.47.148 \u0026lt;none\u0026gt; 9090/TCP 12h tracing ClusterIP 10.99.41.58 \u0026lt;none\u0026gt; 80/TCP 12h zipkin ClusterIP 10.105.51.116 \u0026lt;none\u0026gt; 9411/TCP 12h 따라서 현재 istio-ingressgateway 가 LoadBalancer 타입으로 활성화 되어 localhost 를 통해서 접속이 가능하다는 것을 의미한다. 만약 minikube와 같이 다른 환경인 경우에는 각각의 external IP 또는 NodePort 를 활용하여 접속해야하므로 추가 작업이 필요하다. 매뉴얼 링크\n브라우저 확인 정상적으로 작업이 완료되었다면\nhttp://localhost/productpage 로 접속이 잘 된다. 여러번 새로고침해보면서 rating 영역이 바뀌는 것을 볼 수 있다.\n추가 컴포넌트 확인 Kiali $ istioctl dashboard kiali Jaeger $ istioctl dashboard jaeger garafana $ istioctl dashboard garafana 주의사항 만약 진행에서 잘 되지 않는 부분이 있다면 istioctl analyze 를 실행해서 문제가 없는지 살펴볼 수 있다.\nistioctl analyze 기타 이건 스터디에서 안승규님이 알려주신 내용이다.\ndemo profile 에서는 jaeger의 메트릭 데이터가 inmemory 에 저장되지만 실제 production 에서는 별도의 스토리지(es) 등에 적재할 수 있다.\npilot 이 컨트롤 플랜에서 주요해서 메모리를 넉넉하게 주는게 좋다.\nIstioOperator 를 설치할 때 spec 에 revision 을 지정해주어야 다음버전으로 올라갈 때 수월하다.\napiVersion: isntall.istio.io/v1alpha1 kind: IstioOperator ... spec: revision: \u0026#34;1-9-1\u0026#34; # 이부분이 리비전 지정 profile: default .... 리비전을 지정하면 istio-injection=enabled 이 동작하지 않는다.\n# 다음처럼 지정해야된다. $ kubectl label ns default istio.io/rev=1-9-1 IstioOperator Kind 를 여러개 설치해서 (profile : default / profile: empty 와 같이)\nistiod / ingressgateway / egressgateway 컴포넌트를 각각 별도로 관리할 수 있다.(개별 리소스 / HPA 를 따로 지정) jager 와 kiali 의 다른점은? - jaeger 는 call trace 를 보는데 집중(Open telemetry), kiali 는 서비스의 graph 를 보는데 중점을 둔다\nkial 는 워크로드를 확인하고 jaeger 와 연결해서 trace 정보를 볼 수도 있다. kubectl get istiooperators.install.istio.io -n istio-system 을 입력하면 현재 설치된 istiod 컴포넌트를 확인할 수 있다.\n새로운 operator 버전업은 다음과 같이 진행된다.\n새로운 버전의 istio operator 를 revision 지정하여 활성화 하고 injection 설정을 새롭게 overwrite kubectl label no default istio.io/rev=1-9-2 애플리케이션들을 전체 restart 하여 새로운 istio sidecar 가 구동되어 새로운 operator 에 연결되도록 확인 (istioctl proxy-status) kubectl delete istiooperators.install.istio.io {name} --revision=1-9-1 로 기존 컴포넌트 삭제 참고 https://istio.io/latest/docs/setup/install/operator/ https://istio.io/latest/docs/examples/bookinfo/ https://istio.io/latest/docs/setup/getting-started/#determining-the-ingress-ip-and-ports https://istio.io/latest/docs/setup/additional-setup/config-profiles/ "
},
{
	"permalink": "https://findstar.pe.kr/2021/03/21/kubernetes-nginx-ingress-controller-regexp-path/",
	"title": "nginx ingress 에서 정규표현식 경로 지정하기",
	"tags": ["kubernetes", "nginx regexp"],
	"description": "",
	"type": "post",
	"contents": "nginx ingress controller 에서 regexp location 지정하는 방법 배경 k8s 에서 nginx ingress controller 의 path 을 지정할 때에 기본적으로는 string match 방식으로 path를 구분한다. 대부분의 경우에는 별 문제가 없지만 특정 요구사항에 따라서 따라서 path 를 식별하는데 regexp 를 적용하고 싶은 경우가 있다. ingress controller 로 동작하는 nginx 의 configuration을 바로 수정하기는 어려우니 ingress controller 에서 지원하는 방식으로 해법을 찾아보았다.\n방법 간단히 말해서 다음과 같이 nginx.ingress.kubernetes.io/use-regex annotation 을 적용하면 된다.\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/use-regex: \u0026#34;true\u0026#34; annotation 을 지정한 다음에 다음과 같이 path 지정을 했다면\nspec: rules: - host: test.com http: paths: - path: /foo/.* backend: serviceName: test servicePort: 80 nginx ingress controller 에 적용되는 nginx config 는 다음과 같은 형태가 된다.\nlocation ~* \u0026#34;^/foo/.*\u0026#34; { ... } 주의사항 주의 할점은 regexp 가 적용된 path 의 경우에 매칭되는 패턴이 여러개라면 첫 번째 일치하는 패턴이 적용된다는 점이다. 패턴을 등록할 때 주의하자.\n참고 https://kubernetes.github.io/ingress-nginx/user-guide/ingress-path-matching/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/nginx-regexp/",
	"title": "Nginx Regexp",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2021/03/15/curl-cnd-lookup-time/",
	"title": "curl 로 측정하는 dns lookup time",
	"tags": ["curl"],
	"description": "",
	"type": "post",
	"contents": "curl 로 dns lookup time 까지 얼마나 걸리는지 확인하기 배경 k8s 클러스터 서비스들 사이에 dns lookup time 이 얼마나 되는지 확인해야될 일이 있었다. curl 이 가장 익숙해서 curl 로 dns lookup time 을 확인할 수 있는 방법을 알아보았다.\n방법 curl은 --write-out 옵션을 가지고 있는데 이 옵션을 사용하면 http 요청의 다양한 결과값을 확인할 수 있다.\nhttp_code http 응답의 상태 코드 time_appconnect 요청이 시작되어 SSL/SSH/etc 연결 또는 핸드쉐이크가 완료되었을 때까지의 시간 (초단위) time_connect 요청이 시작되어 TCP 연결이 되었을때까지의 시간 (초단위) time_namelookup DNS 조회가 완료되었을 때까지의 시간 (초단위) time_pretransfer 파일 전송이 시작되었을때까지의 시간(초단위) time_starttransfer 첫번째 바이트가 전송되었을 때까지의 시간(초단위) time_total 전체 작업이 완료되었을 때까지의 시간(초단위) 이 중에서 dns lookup time 까지의 시간을 측정하려면 time_namelookup 을 사용하면 된다.\n다음과 같이 실행할 수 있다.\ncurl --write-out \u0026#39;%{time_namelookup}\u0026#39; https://google.com \u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;meta http-equiv=\u0026#34;content-type\u0026#34; content=\u0026#34;text/html;charset=utf-8\u0026#34;\u0026gt; \u0026lt;TITLE\u0026gt;301 Moved\u0026lt;/TITLE\u0026gt;\u0026lt;/HEAD\u0026gt;\u0026lt;BODY\u0026gt; \u0026lt;H1\u0026gt;301 Moved\u0026lt;/H1\u0026gt; The document has moved \u0026lt;A HREF=\u0026#34;https://www.google.co.kr/\u0026#34;\u0026gt;here\u0026lt;/A\u0026gt;. \u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt; 0.001519% 요청 결과로 확인되는 응답은 보고 싶지 않으니 다음과 같이 결과 값을 표시하지 않도록 변경한다. -o /dev/null 을 추가한다.\ncurl -o /dev/null --write-out \u0026#39;%{time_namelookup}\u0026#39; https://google.com % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 220 100 220 0 0 814 0 --:--:-- --:--:-- --:--:-- 814 0.002023% 응답 결과를 제거하였지만 진행 상황을 나타내는 프로그레스가 아직 출력되어 지저분하다. 이것도 줄여보자. -s 을 추가한다. (silent)\ncurl -o /dev/null -s --write-out \u0026#39;%{time_namelookup}\u0026#39; https://google.com 0.001426% 단 한번의 응답 값으로는 확인하기 어려우니 여러번 수행한 결과를 확인해보자. 간단하게 스크립트를 작성했다.\n#!/bin/sh URL=${1} LIMIT=${2:-1000} if [[ -z \u0026#34;$URL\u0026#34; ]]; then echo \u0026#34;ERROR: Useage : {url} {times} \u0026#34; exit 1 fi echo \u0026#34;$LIMIT times dns_time check for $URL\u0026#34; function get_time() { url=$1 t=$(curl -s -o /dev/null -w \u0026#39;%{time_namelookup}\u0026#39; $url) echo $t } sum=.0 count=0 while [ \u0026#34;$count\u0026#34; -le \u0026#34;$LIMIT\u0026#34; ] do t=$(get_time $URL) count=$((count + 1)) sum=$(echo \u0026#34;$sum + $t\u0026#34; | bc| sed \u0026#39;s/^\\\\./0./;s/0*$//;s/\\\\.$//\u0026#39;) if [ \u0026#34;$count\u0026#34; -ge $LIMIT ] then break fi done total_time=$(echo \u0026#34;scale=10; $sum * 1000\u0026#34; | bc | sed \u0026#39;s/^\\\\./0./;s/0*$//;s/\\\\.$//\u0026#39;) avg_time=$(echo \u0026#34;scale=10; $total_time / $count \u0026#34; | bc | sed \u0026#39;s/^\\\\./0./;s/0*$//;s/\\\\.$//\u0026#39;) echo \u0026#34;total taken: $total_time ms, avg_time : $avg_time ms\u0026#34; 커맨드라인에서 다음과 같이 입력하면 결과를 확인할 수 있다.\n./time.sh http://google.co.kr 10 10 times dns_time check for http://google.co.kr total taken: 16.338 ms, avg_time : 1.6338 ms dns lookup time만을 확인하기 위해서는 다른 좋은 방법이 있겠지만 나의 경우에는 k8s 클러스터 내부에서 각 서비스들 사이의 http 요청에 대해서 dns를 변경했을 때 내부에서 소요되는 시간을 측정하는게 목표라서 위와 같이 curl 로 간단하게 확인해 보았다. 게다가 alpine 리눅스라 이것저것 해볼 여지가 없었다.\n참고 https://ops.tips/gists/measuring-http-response-times-curl/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/haproxy/",
	"title": "Haproxy",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/haproxy-acl-logging/",
	"title": "Haproxy Acl Logging",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/haproxy-custom-variable-logging/",
	"title": "Haproxy Custom Variable Logging",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2019/09/08/haproxy-acl-logging/",
	"title": "Haproxy에서 acl을 log에 기록하기",
	"tags": ["haproxy", "haproxy acl logging", "haproxy custom variable logging"],
	"description": "",
	"type": "post",
	"contents": "Haproxy 의 ACL 설정 reverse proxy 로 활용하면서 header 의 값을 판단하거나, source 의 IP 대역을 확인하거나, 또는 기타 특정 backend 연결을 위해서 acl을 정의한다.\n# ACL \u0026#34;vpc-network\u0026#34; 선언 acl vpc-network src 10.10.1.0/16 # ACL \u0026#34;allow_url\u0026#34; 선언 # whitelist url 목록 파일 지정 acl accept_url url_beg -i -f /etc/haproxy/accept_url.list # ACL \u0026#34;found-header\u0026#34; 선언 acl found-header req.hdr_val(my_custom_header) -m found 조건에 따른 분기 처리를 위한 변수 획득 예를들어 header 의 특정 값을 변수로 획득하여 조건에 따라 다른 백엔드 서버로 요청을 처리를 하고 싶다고 가정해보자. 이 값을 내부 변수로 획득하기 위해서는 몇가지 설정을 통해서 처리할 있다.\napache 에서는 다음과 같이 처리할 수 있다.\n#apache # my_custom_header 라는 값이 존재하는지 확인하고 이 값을 # CUSTOM_HEADER 라는 변수에 할당한다. SetEnvIf my_custom_header (.+) CUSTOM_HEADER=$1 # 로그에 CUSTOM_HEADER 라는 변수값을 기록한다. LogFormat \u0026#34;%{CUSTOM_HEADER}e %{Host}i %D [%{%FT%T%z}t] \\\u0026#34;%r\\\u0026#34; %\u0026gt;s %b \\\u0026#34;%{Referer}i\\\u0026#34; \\\u0026#34;%{User-Agent}i\\\u0026#34;\u0026#34; combined nginx 에서는 다음과 같이 처리할 수 있다.\n#nginx # http_my_custom_header 라는 값을 찾아 custom_header 라는 변수에 할당한다 # 값이 없다면 공백으로 지정 # 값지 존재한다면 해당값을 변수에 저장한다. map $http_my_custom_header $custom_header { \u0026#34;-\u0026#34; \u0026#34;\u0026#34;; default $http_my_custom_header; } # custom_header 라는 변수값을 로그에 기록한다. log_format main \u0026#39;$custom_header $forward_ip $http_host $request_time [$time_iso8601] \u0026#39;; haproxy document 를 확인하면 애석하게도 acl 을 곧바로 로그에 남기는 방법이 존재하지 않는다. apache 나 nginx 보다는 조금 불편한 방법을 거쳐야 한다. 다음과 같이 해보자.\n#haproxy # request header 에 들어 있는 my_custom_header 라는 변수값을 찾아 이를 # req.my_custom_variable 이라는 값으로 지정한다. http-request set-var(req.my_custom_variable) req.hdr(my_custom_header) # 특정 acl 에 따라서 string 값으로 지정할 수 있다. http-request set-var(req.my_custom_variable) str(\u0026#34;other_value\u0026#34;) if my_acl # 길이 제한 10으로 이 값을 capture 하자. http-request capture var(req.my_custom_variable) len 10 # 위에서 캡처한 값을 로그에 기록한다. # 캡쳐한 값이 여러개라면 index 번호는 0, 1, 2.. 순으로 늘어난다. log-format \u0026#34;%[capture.req.hdr(0)]\\ %Tt [%trl]\\ \\\u0026#34;%HM\\ %HU\\ %HV\\\u0026#34;\\ %ST\\ %B\\\u0026#34; 이렇게 하면 원하는 값을 acl 에 따라서 지정할 수 있고, 이를 로그에서 확인할 수 있다. 나의 경우에는 내부 내트워크 에서 접속인지 아닌지 로그에서 바로 확인하는 용도로 사용한다. 테스트는 haproxy 1.9 버전에서 정상적으로 동작하는 것을 확인했다.\n요약 haproxy 에서 acl 값을 log 에 바로 남기는 방법을 알아보았다. apache 처럼 바로 acl 을 변수값으로 확인해서 기록하는 방법이 있으면 편하겠지만. 조금 불편한 방법을 거쳐야 한다. 또한 capture 한 값을 index 값처럼 0, 1, 2.. 로 확인하려니 설정이 지저분해지는 단점이 있다.\n참고 크롬 브라우저에서 header 에 특정 값을 추가하는 extension : https://chrome.google.com/webstore/detail/modheader/idgpnmonknjnojddfkpgkljpfnnfcklj stat haproxy 매뉴얼 : https://cbonte.github.io/haproxy-dconv/1.8/management.html#9 socket 으로 haproxy stat 획득하기 : https://makandracards.com/makandra/36727-get-haproxy-stats-informations-via-socat stat 수집 script : https://gist.github.com/bpaquet/7153979 stat 설명 : https://gist.github.com/alq666/20a464665a1086de0c9ddf1754a9b7fb "
},
{
	"permalink": "https://findstar.pe.kr/2019/04/21/kmooc-review-microservice-design-and-implementation/",
	"title": "[리뷰] Kmooc-Microservice 설계 및 구현 수강 후기",
	"tags": ["kmooc", "Microservice 설계 및 구현"],
	"description": "",
	"type": "post",
	"contents": "[kmooc] Microservice 설계 및 구현 수강 후기 배경 우연찮게 [kmooc] Microservice 설계 및 구현 강의를 신청해서 듣게 되었다. 큰 기대를 하지 않고 시작했는데 예상보다 개념을 정리하는데 도움이 되었다.\nkmooc? kmooc는 Korean MOOC(Massive, Open, Online, Course의 줄임말로 \u0026lsquo;오픈형 온라인 학습 과정\u0026rsquo;) 의 약어이다. 2015년도 부터 시작했다고 하는데, 주요 강좌들은 대학과 연계한 내용이 많아 보였다. 주로 교수님들의 온라인 강의를 수강할 수 있도록 정리되어 있었다. 해외의 coursera 한국어판 이라고 생각하면 된다. 나도 이번에 처음 알게되었다.\n\u0026lsquo;Micronservice 설계 및 구현\u0026rsquo; 강의 나는 kmooc 사이트에 관심을 가지고 있었던 것도 아니였고, 페이스북에서 우연히 \u0026ldquo;Microservice 설계 및 구현\u0026rdquo; 강의를 오픈한다는 광고(?)를 보고나서 최근에 마이크로 서비스 설계의 고민을 해결하는데 도움이 될까 해서 신청하게 되었다. 기간은 19.02.18. ~ 19.04.28. 강의 8주 + OT + 기말시험의 구성이었다. 정해진 기간안에 수강을 완료하면 수강증을 준다는데, 차후에도 청강형태로 영상을 볼 수 있는 시스템이었다. 수강증 보다는 내용이 도움이 되기를 바라면서 수강을 시작했다.\n시작하기 전에 사실, 강의 시작전에 약간의 불안감(?)이 있었다. 왜냐하면, kmooc의 대상 유저가 글로벌(?)이 아닌 한국어 가능자만을 대상으로 하다보니, 콘텐츠의 내용이 충분히 퀄리티가 좋을까? 내용이 만족스러울까 하는 우려가 있었기 때문이다. 특히나 내가 관심있는 IT 주제들은 영어로된 자료가 나오고 나서 한~~~참 뒤에서야 한국어로 번역되어서 전파되기 마련이라, 너무 뒤쳐진 이야기나 수박 겉핥기 내용만 나오지 않을까 걱정스러웠다. 그렇지만 또 한편으로는 마이크로서비스 라는 키워드는 이미 많은 곳에서 이야기 되고 있었기에 내용 괜찮기를 바랬었다.\n강좌는 무료이기 때문에 부담없이 신청이 가능했지만, 무료인만큼 동기부여가 잘 되지 않으면 어떻하지 하는 걱정도 있었다. (유료라.. 돈이 아깝지 않으면 안듣게 되더라.. 하는..걱정)\n시작하고 나서. 강좌를 시작하고 나서 약간 놀란 것은 강좌를 진행하시는 분이 교수님이 아니라, SK에서 실무에 종사하는 분들(!) 이라는 것이었는데, 두명의 수석 or 책임 연구원(?)정도의 직책에 계신분들이 전반/후반을 나눠서 진행을 하셨다. 두분의 목소리뿐만 아니라, 얼굴(!)도 볼 수 있는데, 약간 굳어 있는 표정에서, (발표도 어려운데, 영상에서 얼굴이 나온다니..) 묘하게 안쓰러움이\u0026hellip;.\n강의는 후반의 약간의 코드를 보여주는 부분 이외에는 이론적 개념설명이 주를 이룬다. \u0026ldquo;설계 및 구현\u0026ldquo;에서 구현 코드 따라하기를 기대했던 나는 잘못 생각한 것이었다(쳇..)\n강의 초반에는 솔직히 지루한 느낌이었다. 뻔한 이야기를 그대로 답습하는 느낌이랄까.. 솔직히 교재만 보고 후다닥 넘겨 버릴까 했었지만, 분량 자체가 그렇게 길지 않아서 바로바로 해치워 버린다(!)라는 생각으로 다 봤다. 강의 후 주차별로 나오는 퀴즈는 난이도가 높지 않아 강의를 듣지 않고도 풀 수 있는 정도이다. 초반의 지루함을 이겨내고 매주 수강을 이어갈 수 있었던건. 꼬박꼬박 월요일 마다 날아오는 강의 주차 알림 이메일 덕분이 아니였을까..\n8주차 중에서 4~5 주차 정도 지나서 어느정도 적응하고 나니, 내용이 제법 도움이 된다는 생각이 짙어졌다. 안그래도 업무에서 DDD, Event Driven에 고민이 있었는데 강의의 내용이 딱 그런 내용들이었다. 그동안에 몸으로 느끼고 있었던 것들의 이론적 배경을 되짚어보는 느낌이랄까. 개념들의 핵심 키워드를 빠르게 인지 할 수 있고, 생각들을 환기 시킬 수 있어 괜찮았다.\n용어 설명은 너무 개괄적인 한계가 있었다. 짧은 시간에 무언가 깊이를 기대하기 어려운게 사실이고, 핵심 키워드 또한 한번더 검색을 해보는게 도움이 되었다.\n후기 kmooc 에 대한 걱정반 기대반으로 시작한 강좌였지만, 나름 만족스러웠다. 결국 이런 제도들은 콘텐츠가 핵심인데, 적어도 나에게는 도움이 되었다. 앞으로도 괜찮은 강좌가 개설되면 더 찾아서 들어볼 생각이다. 일단 부담이 없으니까. 그렇지만 더 재미있는 내용이 많았으면 좋겠다.\n실제 강좌에는 DDD 가 절반, 나머지 절반은 마이크로 서비스 설계인것 같다. 마이크로 서비스 보다 DDD 의 개념 정리하는데 도움이 되었다.\nDDD와 관련된 복잡한 용어들이 많이 나오지만, 결국 자세히 생각해 보면, 당연한 이야기를 어렵게 한다는 느낌이 들었다. 용어에 매몰되기 보다, 현장에서의 적용을 생각하면 좀 더 쉽게 이해된다. 한편으로는 과거 디자인 패턴도 익히고 나니 결국 \u0026ldquo;코드의 패턴에 이름을 붙인것\u0026rdquo; 이라는 생각이 들었듯이, DDD 도 어떻게 설계할 것인가 이미 실무에서 진행중인 방법에 이름을 붙인거라는 생각이 든다. 따지고 보니, 이미 나는 유비쿼터스 언어로 서로 대화하고 있었고, 기획, 개발, 디자인, Product Owner 가 모두 모여 함께 설계하고 이야기 했었다. 기획이 기획만 하고 개발이 개발한 하는건 나에게는 이미 아닌 이야기.\n전체 러닝 타임이 5시간이 조금 못 미치는데, 이 정도 시간을 투자해서 주요 용어들을 빠르게 정리해서 훑어 볼 수 있었다는 데 만족스럽다. 다만 DDD, 마이크로 서비스에 대해서 아무것도 모르는 사람이라면, 크게 도움이 되지 않을것 같다.\n요약 kmooc 생각보다 나쁘지 않았다. 코드 실습은 없다. 이미 어느정도 알고 있는 사람이 듣는다면, 개념과 용어 정리에 적합하다. 5시간 정도 투자해서 빠르게 훑어 볼 수 있는 시간이 괜찮음 DDD, 마이크로 서비스 처음 익히는 사람이라면 도움되지 않을 듯. 참고 Microservice 설계 및 구현 강좌 링크 : http://www.kmooc.kr/courses/course-v1:KAISTk+2018_K14+CS490/course/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/kmooc/",
	"title": "Kmooc",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/microservice-%EC%84%A4%EA%B3%84-%EB%B0%8F-%EA%B5%AC%ED%98%84/",
	"title": "Microservice 설계 및 구현",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2019/03/03/book-review-kubernetes-up-running/",
	"title": "[북 리뷰] 쿠버네티스 시작하기(kubernetes up &amp; running)",
	"tags": ["book", "쿠버네티스 시작하기", "kubernetes"],
	"description": "",
	"type": "post",
	"contents": "[북 리뷰] 쿠버네티스 시작하기(kubernetes up \u0026amp; running) 책 소개 원서는 오라일리에서 출간한 Kubernetes Up \u0026amp; Running 라는 제목이다. 에이콘 출판사에서 번역해서 내놓은 책인데, 번역서 특유의 번역체(?)가 좀 거슬린다. 가격은 2만원으로 저렴한 편이라고 생각된다. 책을 구매당시에는 평점 확인이 힘들었는데, 지금 보니 yes24 평점이 4점이다. 평점이 낮을만한 이유는 아래에서 설명하겠다.\n책 구매 이유 도커와 함께 쿠버네티스를 실무에서 사용하기 위해서 공부가 필요했다. 쿠버네티스를 주제로한 외부 스터디에서 참석했었던 경험이 있었는데 업무가 바빠져 완료를 하지 못했었다. 아쉬움에 팀 동료들이 쿠버네티스 스터디를 진행하기로 하여 이 책을 선정해서 함께 요약발표하기로 했다. 다음은 선정 기준.\n최대한 최신 버전의 쿠버네티스를 이야기 할것! 상대적으로 책이 얇을것.(익숙해 지는 단계라는 점을 고려했다.) 대상 독자가 쿠버네티스 입문자일것. 소감 책을 다 읽기 까지는 1주에 2~3시간씩 약 5주정도 소요되었다. 초반에는 그냥 무난했는데, 뒤로 가면 갈수록 번역체 특유의 거슬림(?)이 있다. 특히나 챕터 7장의 서비스 탐색 이라는 용어는 정말이지 끝까지 적응안되는 용어였는데, 그냥 원문 Service discovery 라고 쓰는게 나았다고 생각한다. 후반부의 급격한 번역의 퀄리티 저하는 그렇다 치고, 원저자가 의도한 것인지는 모르겠지만 Deployment 개념을 12장에 가서야 설명한다. 그런데 정작 12장 이전에 군데군데 계속 개념적으로 연결된 내용들이 나온다. 따라서 12장에 도달하기도 전에 이미 Deployment는 구글링을 통해서 알게된 상태가 된다.\n마지막으로 쿠버네티스는 그 개념이 요소요소 마다 방대한데, 책에서 챕터별로 이를 축약하다보니, 생략된 부분이 많고, 아무리 가볍게 보려고 해도 핵심적인 개념전달 보다, 장황하게 풀어쓴 느낌이 들었다. 그래서 책을 다 읽은 뒤에도 뭔가 시간낭비한 느낌이 들었다. 책에서 그나마 나은점을 꼽으라면 실습을 위한 yml 파일 모음 을 제공한다는 점이다.\n차라리 이전에 읽었던 원서 : The DevOps 2.3 Toolkit: Kubernetes가 더낫다.\n한줄 요약 \u0026ldquo;쿠버네티스 입문용으로 아쉬운 책. 비추천\u0026rdquo; 참고 에이콘 링크 : http://acornpub.co.kr/book/kubernetes-up-and-running "
},
{
	"permalink": "https://findstar.pe.kr/tags/%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0/",
	"title": "쿠버네티스 시작하기",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2019/01/27/learning-in-2019/",
	"title": "2019년에 배우고자 하는 것들",
	"tags": ["learning in 2019"],
	"description": "",
	"type": "post",
	"contents": "2019년에 배우고자 하는 것들 주제 정하기 재미있어 보이고, 평소에 생각만 했던것들, 또 해봐야지 하고 생각했던 일들은 참 많다. 그렇지만 나 스스로에게 꼭 필요하고, 쉽게 접근할 수 있으며, 목표가 뚜렷한 일들을 정해보았다.\n1. 영어 매년 고정적으로 나의 \u0026quot;wish list\u0026quot; 에 등장하는 단어! 그렇다 영어다. 하지만 올해는 야심차게 영어 스터디를 가입해서 년초부터 고군분투 하고 있다. 지금은 감명 깊었던 TED 의 한 영상 스크립트를 3주째 외우는 중인데 처음보다는 조금 나아진걸 느끼고 있다. 스스로가 조금 나아진걸 느껴지니 영어가 재미있어 지기 시작했다!(이건 정말 스스로에게 대견한 일이다) 마지막 까지 잘 마무리 해보도록 스스로에게 응원 중이다.\n바램 : 영어에 대한 거부감을 지금보다 낮추기 목표 : 10분간 영어로 자기소개 막힘없이 말할 수 있기 계획 : 영어와 친숙해 지기 위해서 스터디 참석, 단어는 출퇴근 시간에 외우기, 스크립트 외우기 2. Spring Framework Java 그리고 Spring Framework 는 언제나 나에게는 큰 숙제같은 느낌이었다. 코드를 수정하고, 이슈를 치는 순간순간에도 무언가 내게 맞지 않는 옷같은 느낌이랄까. 올해는 업무적으로 아키텍처 개선이 예상되고 있어 더이상 얕게 알아서는 안되는 스킬이 됬다. 다행히도 Spring Boot 라는 좋은 소재가 있어서 차근차근 하나씩 이해하려고 노력중이다.\n바램 : 스프링 프레임워크에 대한 두려움 없애기 목표 : 스프링으로 micro service 코드 처음부터 끝까지 완성하기 계획 : 책 하나 완독, 동영상 강좌 수강, 토이 프로젝트 완성 3. Kotlin 아직 업무에서 적용할지 안할지 모르겠지만, 이왕이면 새로운 언어로 프로젝트를 시작해보고 싶은 마음에 선정해봤다. 주변에서도 제법 코틀린 이야기가 많이 들려오는 걸 봐서는 올해에는 새롭게 배워야 할 언어로 코틀린을 꼽아 봤다. 작년에는 golang을 써봤지만 아직 내가 쓰기에는 조금 더 시간이 필요하다는 느낌이었기에 올해에는 코틀린을 좀 더 잘 살펴보고 싶다.\n바램 : 코틀린 언어 새롭게 친숙해지기 목표 : 신규 프로젝트 코틀린으로 진행하기 계획 : 책 세권이상 독파, 동영상 강좌 수강, 실무에서 활용. 4. 요리 대학교 입학하고 자취를 몇년이나 했었지만, 먹는데 취미가 없어서인지, 요리는 항상 나에게 먼나라 이야기였다. 그렇지만 결혼을 하고, 아이가 생기고 주말에 가족과 함께 식사하는 시간에 매번 아내에게 요리를 맡기는게 미안했었다. 뭔가 조금씩이나마 내가 만든 요리를 아내와 아이에게 해줄 수 있는 멋진 아빠가 되고 싶다.\n바램 : 요리 할줄 아는 아빠되기 목표 : 주말에 한끼는 내가 만들어 가족에게 대접하기. 계획 : 집밥 백선생 독파, 주말에 요리 연습 해보기, 장볼 때 음식 재료 관심 가지기. 그 밖에 것들 아직 정확하게 어떻게 해야 할지 계획을 세우지는 못했지만 그래도 희망사항으로 배워보았으면 하는 것들도 있다.\nData Flow(Pipeline) 에 대한 노하우 : 업무에서 kafka를 사용하면서 data 의 흐름에 대한 관심, kafka message를 잘 만들려면 어떻게 해야하는지 고민이 많다. 이런건 어떻게 노하우를 습득할 수 있을까? (nifi?) GraphQL \u0026amp; Micro service architecture : entity model 의 relation 과 API 의 연관관계를 고민하다보니 GraphQL 이 눈에 들어왔다. micro service architecture 를 설계하는데 도움이 되지 않을까? 다짐 시작이 반이다. 기록으로 남겼으니, 올해 말에 스스로에게 부끄럽지 말자. "
},
{
	"permalink": "https://findstar.pe.kr/tags/learning-in-2019/",
	"title": "Learning in 2019",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/brew-cask/",
	"title": "Brew Cask",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2019/01/20/install-openjdk-by-homebrew/",
	"title": "homebrew로 openjdk 설치하기 ",
	"tags": ["openjdk", "brew cask"],
	"description": "",
	"type": "post",
	"contents": "homebrew로 openjdk 설치하기 OpenJDK OpenJDK(Open Java Development Kit)는 자바 플랫폼, 스탠더드 에디션 (자바 SE)의 자유-오픈 소스 구현체이다. 최근 자바가 유료화 되면서 한층 주목받고 있는데, (자바가 유료라니..) 유료화에 대한 반발(?)로 주변의 많은 사람들이 OpenJDK를 설치하는 모습을 볼 수 있었다. 나는 새롭게 개발 환경을 구성하면서 OpenJDK를 설치해보고자 하였는데, 처음에는 생각없이 brew 명령어를 실행했었다.\nbrew install openjdk 하지만 보기 좋게 실패했다.(!!!)\n찾아보니 아직은 공식적으로 brew 를 통해서 설치가 불가능하다. brew issue link\n따라서 OpenJDK 를 brew로 설치하려면 공식이 아닌 비공식(?) 경로를 통해서 설치해야 한다.\nAdoptOpenJDK AdoptOpenJDK는 사전에 prebuild 형태로 java binary를 제공하는 커뮤니티 그룹이다. 홈페이지 Mac 뿐만 아니라 윈도우, 리눅스 환경도 제공하고 있다. 공식적으로 OpenJDK를 설치하는건 직접 빌드해서 사용하는 방법이 있지만, 빌드 이외에도 자잘한 JAVA_HOME (빌드해서 사용했더니 \u0026ldquo;/usr/libexec/java_home\u0026rdquo; 이 동작하지 않았다..) 설정 문제라던가 버전업을 편하게 하기 위해서 homebrew를 사용해서 AdoptOpenJDK를 설치하도록 했다.\n다음 Github 에서 설치방법을 확인할 수 있다.\nbrew tap AdoptOpenJDK/openjdk brew cask install \u0026lt;version\u0026gt; OpenJDK 버전은 다음중 하나를 선택하면 된다.\nOpenJDK8 - adoptopenjdk8 OpenJDK9 - adoptopenjdk9 OpenJDK10 - adoptopenjdk10 OpenJDK11 - adoptopenjdk11 OpenJDK11 w/ OpenJ9 JVM - adoptopenjdk11-openj9 나는 JDK8를 사용하기 위해서 다음과 같이 진행했다.\nbrew tap AdoptOpenJDK/openjdk brew cask install adoptopenjdk8 이후 빌드했을 때와 다르게 JAVA_HOME 문제도 없고, 다른 경로 문제도 없이 잘 동작하는 것을 확인할 수 있었다.\n참고 openjdk란 위키 : https://ko.wikipedia.org/wiki/OpenJDK OpenJDK 홈페이지 : https://openjdk.java.net/ AdoptOpenJDK 홈페이지 : https://adoptopenjdk.net/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/openjdk/",
	"title": "Openjdk",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2019/01/13/book-report-begining-docker/",
	"title": "[북 리뷰] 시작하세요 도커",
	"tags": ["book", "시작하세요 도커", "begining docker"],
	"description": "",
	"type": "post",
	"contents": "도커 공부를 시작 한지는 오래되었지만, 제대로 한권을 끝까지 읽은건 이책이 처음이었다. 위키북스에서 출간한 \u0026ldquo;시작하세요 도커\u0026rdquo; 책의 간략한 소감이다.\n책 소개 위키북스에서 출판한 서적. 2017년 04월 07일 출판했는데 빠른 버전업을 고려해도 기본적인 내용을 잘 다룬책이다. 저자인 \u0026ldquo;용찬호\u0026quot;님께서는 경의대 대학원에서 석사과정을 진행중이라고 한다. (blog.naver.com/alice_k106) 가격은 2만 8천원으로 무난한편. yes24 평점도 8점대로 괜찮은 평을 받고 있다.\n책 구매 이유 도커는 실제 업무 환경에서 사용하고 있기도 하고, 몇차례 스터디에서 공부하기도 했었지만, 책을 하나 잡고 진득하니 독파해본적은 없었다. 마침 팀내에서 진행하는 스터디에서 책을 하나 선정해서 함께 정리해가는 방식으로 진행해보자는 이야기가 나와서 이 책을 구매하게 되었다. 다음은 선정 기준.\n최대한 최신 버전의 도커를 이야기 할것!(도커 CE/EE 구분된 이후 버전을 담은 책을 기준으로 정했다.) 번역서적이 아닌 집필 서적일것!(번역은 호불호가 갈리는 편이라서..) 오케스트레이션 내용은 가볍게 담고 순수 도커에만 집중한 책 기타 주변분들의 추천을 고려 소감 책을 다 읽기 까지는 1주에 2~3시간씩 약 3주정도 소요되었었는, 전체적으로 이해하기 쉽게 설명되어 있어서 좋았다. 3장 \u0026ldquo;도커 스웜\u0026quot;의 경우에는 쿠버네티스를 바로 공부할 예정이라서 생략 했었고, 6장 \u0026ldquo;플러그인\u0026rdquo; 도 크게 집중해서 읽지는 않았다. 7장에서 오케스트레이션 툴을 소개해준건 괜찮았다.(물론 내용은 거의 없는게 당연) 초반에 2장이 좀 분량이 많다고 생각이 들었는데, 읽고 보니 2장이 제일 핵심적인 내용이자, 이 책의 이유(?!)인 챕터이다. 심지어 나중에 느낀 소감으로는 2장만 읽어도 괜찮았었겠다라는 느낌이 들었다. 종종 명령어가 생각안날 때 다시 2장 챕터를 몇번 더 찾아보게되었다. 말 그대로 기본기를 \u0026ldquo;익히기 좋은 책\u0026rdquo;.\n한줄 요약 \u0026ldquo;도커의 기본기를 익히기 좋은 책. 시간이 없다면 2장만 읽으세요!\u0026rdquo; 참고 yes24 링크 : http://www.yes24.com/24/goods/38261523 용찬호 저자의 블로그 : blog.naver.com/alice_k106 "
},
{
	"permalink": "https://findstar.pe.kr/tags/begining-docker/",
	"title": "Begining Docker",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/%EC%8B%9C%EC%9E%91%ED%95%98%EC%84%B8%EC%9A%94-%EB%8F%84%EC%BB%A4/",
	"title": "시작하세요 도커",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/haproxy-metric/",
	"title": "Haproxy Metric",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/haproxy-stat/",
	"title": "Haproxy Stat",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2019/01/05/haproxy-stat-metric/",
	"title": "Haproxy 의 Statistics 정보 모니터링 하기",
	"tags": ["haproxy", "haproxy stat", "haproxy metric"],
	"description": "",
	"type": "post",
	"contents": "Haproxy를 사용한 시스템을 운영중인데, 여기에서 확인할 수 있는 다양한 상태값을 모니터링 해보고 싶었다. 간단하게는 Haproxy가 살아 있는지에서 부터 현재 연결된 커넥션 갯수를 확인하는 것 까지 작업을 수행해보았다.\nHaproxy 의 statistics 를 얻을 수 있는 방법 Haproxy 의 statistics를 확인할 수 있는 방법은 다음의 2가지 방법을 확인가능하다.\n1. HTTP 상에서 확인할 수 있는 방법 haproxy.cfg 에 다음과 같이 stats 설정을 지정하면 http 를 통해서 확인이 가능하다\nlisten stats # \u0026#34;stats\u0026#34;라는 이름으로 listen 지정 bind :9000 # 접속 포트 지정 stats enable stats realm Haproxy\\ Statistics # 브라우저 타이틀 stats uri /haproxy_stats # stat 를 제공할 URI stats auth Username:Password # 인증이 필요하면 추가한다 이렇게 설정한 뒤에 http://haproxy_host:9000 으로 접속하면 다음과 같은 화면을 확인할 수 있다.\n화면으로만 보는 것도 의미가 있지만 모니터링을 위해서는 metric 정보만 필요할 수도 있다. 주로 CSV 포맷으로 보게 되는데 URI 뒤에 ;csv를 붙이면 된다.\n위와 같은 설정이라면 stats URI 를 /haproxy_stats;csv 라고 접근하면 된다.\n2. unix socket 를 사용하는 방법 http 를 사용할 수도 있지만 unix socket 을 사용할 수도 있다. 먼저 haproxy.cfg 에 다음과 같이 설정되어 있어야 한다.\nstats socket /var/run/haproxy.sock 그리고 socat 명령어를 사용할 수 있어야 한다. 명령어가 설치되어 있지 않다면 yum 또는 apt 를 사용해서 설치해야한다.\n이제 다음 명령어로 stat 을 확인할 수 있다.\necho \u0026#34;show stat\u0026#34; | socat unix-connect:/var/run/haproxy.sock stdio http-in,FRONTEND,,,0,85,2000,4655,380562,991165,0,0,14,,,,,OPEN,,,,,,,,,1,2,0,,,,0,0,0,3305,,,,0,0,0,14,4641,0,,0,3305,4655,,,0,0,0,0,,,,,,,, appname,lamp1,0,0,0,0,,0,0,0,,0,,0,0,0,0,DOWN,1,1,0,1,1,134,134,,1,3,1,,0,,2,0,,0,L4TOUT,,2002,0,0,0,0,0,0,0,,,,0,0,,,,,-1,,,0,0,0,0, appname,lamp2,0,0,0,0,,0,0,0,,0,,0,0,0,0,DOWN,1,1,0,1,1,133,133,,1,3,2,,0,,2,0,,0,L4TOUT,,2002,0,0,0,0,0,0,0,,,,0,0,,,,,-1,,,0,0,0,0, appname,BACKEND,0,0,0,77,200,4641,380562,988533,0,0,,4641,0,0,0,DOWN,0,0,0,,1,133,133,,1,3,0,,0,,1,0,,3292,,,,0,0,0,0,4641,0,,,,,0,0,0,0,0,0,-1,,,0,0,0,0, 여기서 출력되는 정보는 haproxy.cfg 에 설정된 backend, frontend 에 따라서 조금씩 달라진다. 그리고 여기서 확인되는 정보는 다음과 같다.\n0. pxname [LFBS]: proxy name 1. svname [LFBS]: service name (FRONTEND for frontend, BACKEND for backend, any name for server/listener) 2. qcur [..BS]: current queued requests. For the backend this reports the number queued without a server assigned. 3. qmax [..BS]: max value of qcur 4. scur [LFBS]: current sessions 5. smax [LFBS]: max sessions 6. slim [LFBS]: configured session limit 7. stot [LFBS]: cumulative number of connections 8. bin [LFBS]: bytes in 9. bout [LFBS]: bytes out 10. dreq [LFB.]: requests denied because of security concerns. - For tcp this is because of a matched tcp-request content rule. - For http this is because of a matched http-request or tarpit rule. 11. dresp [LFBS]: responses denied because of security concerns. - For http this is because of a matched http-request rule, or \u0026#34;option checkcache\u0026#34;. 12. ereq [LF..]: request errors. Some of the possible causes are: - early termination from the client, before the request has been sent. - read error from the client - client timeout - client closed connection - various bad requests from the client. - request was tarpitted. 13. econ [..BS]: number of requests that encountered an error trying to connect to a backend server. The backend stat is the sum of the stat for all servers of that backend, plus any connection errors not associated with a particular server (such as the backend having no active servers). 14. eresp [..BS]: response errors. srv_abrt will be counted here also. Some other errors are: - write error on the client socket (won\u0026#39;t be counted for the server stat) - failure applying filters to the response. 15. wretr [..BS]: number of times a connection to a server was retried. 16. wredis [..BS]: number of times a request was redispatched to another server. The server value counts the number of times that server was switched away from. 17. status [LFBS]: status (UP/DOWN/NOLB/MAINT/MAINT(via)...) 18. weight [..BS]: total weight (backend), server weight (server) 19. act [..BS]: number of active servers (backend), server is active (server) 20. bck [..BS]: number of backup servers (backend), server is backup (server) 21. chkfail [...S]: number of failed checks. (Only counts checks failed when the server is up.) 22. chkdown [..BS]: number of UP-\u0026gt;DOWN transitions. The backend counter counts transitions to the whole backend being down, rather than the sum of the counters for each server. 23. lastchg [..BS]: number of seconds since the last UP\u0026lt;-\u0026gt;DOWN transition 24. downtime [..BS]: total downtime (in seconds). The value for the backend is the downtime for the whole backend, not the sum of the server downtime. 25. qlimit [...S]: configured maxqueue for the server, or nothing in the value is 0 (default, meaning no limit) 26. pid [LFBS]: process id (0 for first instance, 1 for second, ...) 27. iid [LFBS]: unique proxy id 28. sid [L..S]: server id (unique inside a proxy) 29. throttle [...S]: current throttle percentage for the server, when slowstart is active, or no value if not in slowstart. 30. lbtot [..BS]: total number of times a server was selected, either for new sessions, or when re-dispatching. The server counter is the number of times that server was selected. 31. tracked [...S]: id of proxy/server if tracking is enabled. 32. type [LFBS]: (0=frontend, 1=backend, 2=server, 3=socket/listener) 33. rate [.FBS]: number of sessions per second over last elapsed second 34. rate_lim [.F..]: configured limit on new sessions per second 35. rate_max [.FBS]: max number of new sessions per second 36. check_status [...S]: status of last health check, one of: UNK -\u0026gt; unknown INI -\u0026gt; initializing SOCKERR -\u0026gt; socket error L4OK -\u0026gt; check passed on layer 4, no upper layers testing enabled L4TOUT -\u0026gt; layer 1-4 timeout L4CON -\u0026gt; layer 1-4 connection problem, for example \u0026#34;Connection refused\u0026#34; (tcp rst) or \u0026#34;No route to host\u0026#34; (icmp) L6OK -\u0026gt; check passed on layer 6 L6TOUT -\u0026gt; layer 6 (SSL) timeout L6RSP -\u0026gt; layer 6 invalid response - protocol error L7OK -\u0026gt; check passed on layer 7 L7OKC -\u0026gt; check conditionally passed on layer 7, for example 404 with disable-on-404 L7TOUT -\u0026gt; layer 7 (HTTP/SMTP) timeout L7RSP -\u0026gt; layer 7 invalid response - protocol error L7STS -\u0026gt; layer 7 response error, for example HTTP 5xx 37. check_code [...S]: layer5-7 code, if available 38. check_duration [...S]: time in ms took to finish last health check 39. hrsp_1xx [.FBS]: http responses with 1xx code 40. hrsp_2xx [.FBS]: http responses with 2xx code 41. hrsp_3xx [.FBS]: http responses with 3xx code 42. hrsp_4xx [.FBS]: http responses with 4xx code 43. hrsp_5xx [.FBS]: http responses with 5xx code 44. hrsp_other [.FBS]: http responses with other codes (protocol error) 45. hanafail [...S]: failed health checks details 46. req_rate [.F..]: HTTP requests per second over last elapsed second 47. req_rate_max [.F..]: max number of HTTP requests per second observed 48. req_tot [.F..]: total number of HTTP requests received 49. cli_abrt [..BS]: number of data transfers aborted by the client 50. srv_abrt [..BS]: number of data transfers aborted by the server (inc. in eresp) 51. comp_in [.FB.]: number of HTTP response bytes fed to the compressor 52. comp_out [.FB.]: number of HTTP response bytes emitted by the compressor 53. comp_byp [.FB.]: number of bytes that bypassed the HTTP compressor (CPU/BW limit) 54. comp_rsp [.FB.]: number of HTTP responses that were compressed 55. lastsess [..BS]: number of seconds since last session assigned to server/backend 56. last_chk [...S]: last health check contents or textual error 57. last_agt [...S]: last agent check contents or textual error 58. qtime [..BS]: the average queue time in ms over the 1024 last requests 59. ctime [..BS]: the average connect time in ms over the 1024 last requests 60. rtime [..BS]: the average response time in ms over the 1024 last requests (0 for TCP) 61. ttime [..BS]: the average total session time in ms over the 1024 last requests 이제 stat 를 확인하는 스크립트를 구성한다. www라는 frontend 의 scur(current sessions)를 출력하는 스크립트로 구성했다.\n#!/bin/sh sock=\u0026#39;/var/run/haproxy.sock\u0026#39; host=\u0026#34;$(hostname -f)\u0026#34; pause=1 time=\u0026#34;$(date +%s)\u0026#34; echo \u0026#39;show stat\u0026#39; | socat - UNIX-CLIENT:$sock \\ |while IFS=\u0026#39;,\u0026#39; read pxname svname qcur qmax scur smax slim stot bin bout dreq dresp ereq econ eresp wretr wredis status weight act bck chkfail chkdown lastchg downtime qlimit pid iid sid throttle lbtot tracked type rate rate_lim rate_max check_status check_code check_duration hrsp_1xx hrsp_2xx hrsp_3xx hrsp_4xx hrsp_5xx hrsp_other hanafail req_rate req_rate_max req_tot cli_abrt srv_abrt comp_in comp_out comp_byp comp_rsp; do if [ \u0026#34;$svname\u0026#34; = \u0026#39;FRONTEND\u0026#39; ]; then if [ \u0026#34;$pxname\u0026#34; = \u0026#39;www\u0026#39; ]; then name=$(echo $svname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;) echo \u0026#34;${scur:-0}\u0026#34; fi fi done 이 결과를 이제 collectd 에 연결해서 그래프로 그리면 된다.\nLoadPlugin \u0026#34;exec\u0026#34; \u0026lt;Plugin \u0026#34;exec\u0026#34;\u0026gt; Exec \u0026#34;haproxy:haproxy\u0026#34; \u0026#34;/usr/local/bin/haproxy-stats\u0026#34; \u0026lt;/Plugin\u0026gt; 두가지 방법중에 나의 경우는 unix socket 을 사용했다.\n요약 haproxy 의 stat를 확인하여 모니터링하는 방법은 unix socket 을 활용해서 필요한 정보(여기서는 current session)를 얻는 스크립트를 작성하고 이를 주기적으로 실행해서 (collectd 또는 별도의 방법을 사용해도 된다.) 수집된 데이터를 전달하여 그래프로 그리는 것이다. 그래프는 grafana 같은 그래프로 연결해 놓으면 보기도 편하고 이상이 있을 때 알림을 걸 수도 있어서 편리하다.\n참고 stat 수집 획득하는 방법 : https://www.datadoghq.com/blog/how-to-collect-haproxy-metrics/ stat haproxy 매뉴얼 : https://cbonte.github.io/haproxy-dconv/1.8/management.html#9 socket 으로 haproxy stat 획득하기 : https://makandracards.com/makandra/36727-get-haproxy-stats-informations-via-socat stat 수집 script : https://gist.github.com/bpaquet/7153979 stat 설명 : https://gist.github.com/alq666/20a464665a1086de0c9ddf1754a9b7fb "
},
{
	"permalink": "https://findstar.pe.kr/2019/01/01/2019-new-year/",
	"title": "2019년 새해 목표",
	"tags": ["happy new year"],
	"description": "",
	"type": "post",
	"contents": "2019년 새해가 밝았다, 작년 중반 이후에는 뜸해진 블로그에 간만에 따뜻한(?) 온기를 불어넣어보고자, 2019년 한해의 다짐을 기록해 본다.\n매년 새해가 되면, 새롭게 무언가 시작해보고자 하얀 백지 위에 이것저것 목표들을 채워넣는다. 작년에도, 그 전해에도 그랬지만, 이번에는 \u0026ldquo;끝내~ 이루리라!\u0026rdquo; 라고 의지를 불살라 본다.\n제일먼저, 의지 충만하게 시작하지만 중간에 흐지부지 되고마는 목표들(영어라던가..영어라던가..영어라던가..), 해가 갈수록 건강에 신경써야지 했던 생각들이 떠오른다. 가족과의 시간도 중요하고, 또 기술을 갈고 닦는 시간도 중요하다.\n뭔가 회사 업무 이외에 개인적인 성취도 도전해보고 싶고, 그러고 보니 대출금을 줄이기 위해서 절약도 해야한다. 두서없이 적다보니 끝이 없을것 같아, 요약에 요약에 요약을 더해 간추려 보았다.\n욕심내지 말고, 하나씩 해나가자는 다짐을 해본다.\n건강 치료 : 매번 악순환되는 손목 부상, 알레르기성 질환 관리 (1, 2월은 치료) 운동 : 암벽등반 꾸준히 하기 (부상 치료후 3월 부터) 체력 : 집에서 꾸준히 맨손 운동 하기 다이어트 : 더 찌지만 말자.(빼는건 ㅠ) 수면 : 12시 이전에 취침하기. 영어 스터디 : 영어 스터디 참가 (2019.01.03 시작!) 도전 : \u0026ldquo;영어책 한권 외워 봤어?\u0026rdquo; 책을 읽고 따라하기 기술 spring boot 익숙해지기 매주 기술 스터디 : java, spring boot 준비중 (2019.01.07 시작!) kafka data pipeline 설계 MSA 구조 구현 도전 가족 주말에 한끼는 스스로 요리해서 밥 준비하기 하반기 가족과 함께 미국 한달 여행 업적 강의 만들기 도전! 매주 1개의 블로깅 1달에 1권 책 독후감 작성하기 이렇게 정리해놓으니 한결 보기 편하긴 한데, 잘 할 수 있을까 라는 걱정이 앞선다. 물론 시작이 반이니까 이미 반은 한셈인가. 매년 다짐하는 새해 목표지만, 올해는 좀 더 잘 해보고 싶다. 스터디도 벌려놨고, 실천 계획을 하나씩 세우고 차근차근 해나가는게 중요하지만 너무 조바심 내서 스스로 스트레스 받지는 않기로 했다. 어찌되었든 올 한해도 화이팅!\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/happy-new-year/",
	"title": "Happy New Year",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/lets-encrypt/",
	"title": "Let&#39;s Encrypt",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/09/08/lets-encrypt-certificates-rate-limit/",
	"title": "Let&#39;s encrypt 의 인증서를 생성할 때 주의사항",
	"tags": ["let&#39;s encrypt", "letsencrypt"],
	"description": "",
	"type": "post",
	"contents": "Let\u0026rsquo;s encrypt를 활용해서 SSL 인증서를 생성할 때에는 몇가지 주의해야할 사항이 있다. 인증서를 무한대로 생성할수 없는 것이 당연하고, 생성에 대한 제약사항을 정리해보았다.\nLet\u0026rsquo;s encrypt? Let\u0026rsquo;s encrypt 는 https 접속을 지원하기 위한 인증서를 무료로 생성할 수 있는 서비스라고 생각하면 이해하기 쉽다. 개인 블로그의 경우 인증서를 직접 구매하기 어려운데, let\u0026rsquo;s encrypt를 사용하면 무료로 인증서를 발급 받을 수 있다. 모질라, 아카마이, 시스코, 크롬, 페이스북, automattic 과 같은 회사들이 스폰서로 있어서 서비스가 지원을 중단할 걱정도 없다고 할 수 있다. 이 블로그의 경우에는 github page 에서 제공해주는 ssl 서비스를 사용하고 있는데 여기에서 사용하는 인증서도 let\u0026rsquo;s encrypt를 사용한 것이다. 국내에서는 티스토리에서 개인도메인에 대한 ssl 인증서를 let\u0026rsquo;s encrypt를 사용해서 지원하고 있다.\n인증발급의 각 단계 인증서는 발급/갱신/취소 의 세가지 단계가 있다. 발급은 새로운 인증서 파일을 생성하는 것, 갱신은 생성된 인증서의 유효기간이 30일 이하로 남은 경우 이를 새롭게 90이상 사용가능하도록 하는 것, 마지막으로 취소는 생성한 인증서 파일을 유효하지 않게 하는 동작이다.\n생성된 인증서의 유효기간 let\u0026rsquo;s encrypt를 통해서 생성한 인증서는 기본적으로 3개월(90일)동안 사용할 수 있다. 일반적으로 사설 업체를 통해서 구매한 인증서가 최소 1년이라는 것을 감안하면 기간이 짧다고 할 수 있다. 그렇지만 재갱신 또한 무료로 가능하기 때문에, 제 때에 갱신하는 것을 깜빡하지(?) 않다면, 인증서를 계속 갱신해서 https 접속을 유지할 수 있다.\n인증서를 발급 할 때 제약사항 인증서를 발급하는 방법은 자료가 많으니, 인증서를 발급할 때 제약사항에 대해서 알아보자. 일단 공식 사이트 에서 자세하게 나와 있긴 한데 처음에는 뭐가뭔지 헷갈린다. 이를 정리해 보았다.\n인증서는 하나의 Registered Domain 기준으로 1주에 50개까지 인증서 발급이 가능하다.\nRegistered Domain 이라는 것은 일반적으로 우리가 도메인을 신규로 구매할 때 결정되는 도메인이라고 생각하면 된다. \u0026ldquo;www.example.com\u0026rdquo; 에서 Registered Domain 도메인은 \u0026ldquo;example.com\u0026rdquo; 부분을 말한다. 만약 서브 도메인이 \u0026ldquo;new.blog.example.co.kr\u0026rdquo; 이라면 Registered Domain 은 \u0026ldquo;example.co.kr\u0026rdquo; 이 된다. Registered Domain 에 대한 판단은 Public Suffix 를 사용해서 판단된다.\n하나의 인증서는 최대 100개의 서브 도메인 인증 정보를 포함시킬 수 있다.\n하나의 인증서에는 단 하나의 도메인 인증 정보만 담을 수 있는 것은 아니다. 두개 이상의 도메인 인증 정보를 담을 수 있는데, (이러한 인증서를 멀티 도메인 인증서 라고 한다) let\u0026rsquo;s encrypt를 사용해서 발급하는 인증서 파일 하나에 담을 수 있는 도메인 인증정보는 파일당 최대 100개의 제한이 있다. 따라서 앞서 첫번째 제한사항과 함께 고려한다면, 일주일에 최대 5000개의 서브 도메인에 대한 인증정보를 생성할 수 있다.\n동일한 도메인에 대한 인증정보 그룹(domain set)은 일주일에 최대 5개까지 발급이 가능하다.\n인증서를 생성할 때에는 동일한 도메인 인증정보 그룹(domain set)에 대해서는 일주일에 최대 5개까지 발급이 가능하다. 인증정보 그룹(domain set)이라는 것은 [\u0026ldquo;www.example.com\u0026rdquo;, \u0026ldquo;example.com\u0026rdquo;] 와 같이 한번에 인증서 발급을 위해서 요청하는 도메인들(하나 또는 그이상)의 집합이라고 할 수 있다. 일반적으로는 하나의 도메인에 대해서 인증서를 발급 요청하기 때문에 이 제약사항은 그냥 동일한 도메인에 대해서는 인증서 발급은 일주일에 5번으로 제한된다고 이해하면 된다. 만약 인증정보 그룹(여러개의 도메인에 대한 인증서)이 동일한 인증서 발급 요청을 한다면, 일주일 동안에는 최대 5개까지만 가능하다는 의미다. 단, 인증정보 그룹을 추가한다면 추가적으로 인증서 발급이 가능하다 일주일 사이에 [\u0026ldquo;www.example.com\u0026rdquo;, \u0026ldquo;example.com\u0026rdquo;] 으로 5번 인증서를 생성했더라도, [\u0026ldquo;www.example.com\u0026rdquo;, \u0026ldquo;example.com\u0026rdquo;, \u0026ldquo;blog.example\u0026rdquo;] 으로 인증서를 추가적으로 생성할 수 있다.\n인증서를 갱신할 때 제약사항 인증서를 생성할 때만 제약이 있는 것은 아니고, 인증서를 갱신할 때에도 제약사항이 있다.\n생성 limit 에 해당되더라도, 갱신은 가능하다.\n만약 일주일에 50개의 인증서를 발급하여, 생성 limit에 해당되더라도, 기존에 생성한 인증서의 갱신(renewal)은 가능하다. 따라서 일단 인증서를 생성했다면, 갱신하는데는 문제가 없다. (그렇지만, 언제나\u0026hellip; 유저 불량이 발생한다.)\n갱신을 하기 전에 만료 기간에 해당하는지 확인해야한다.\n생성된 인증서는 90일 동안 유효하고, 30일이 남은 시간 부터 갱신이 가능하다, 만약 그 이전에 동일한 인증정보 그룹(domain set)으로 갱신 요청을 한다면, 이때는 동일한 도메인 인증서 발급 요청으로 취급되어, 동일한 도메인 인증정보 발급의 제약을 받는다. 그러니, 갱신을 하기전에. 만료기간에 해당하는지 확인하고 요청을 하도록 하자.\n인증서 갱신을 활용하면 사용가능한 하나의 도메인의 서브 도메인별 인증서는 계속 늘어날 수 있다.\n인증서는 기본적으로 생성에 제약이 있고, 갱신에는 제약이 없기 때문에 이를 잘 활용한다면, 1주에 50개씩 (하나의 도메인씩 늘려간다고 가정할 때) 계속 사용가능한 인증서를 늘려갈 수 있다. 만약 하나의 도메인에 여러개의 서브도메인을 모두 인증서를 발급해야 한다면, 이 정책을 사용하면 시간이 걸릴지언정, 언젠가는 모두 인증서 발급이 가능하다.\n인증서 취소의 참고사항 인증서 취소(revoke)는 인증서 생성 limit 을 초기화 시키지 않는다\n인증서 생성단계에서 어떠한 제약사항에 해당되어(주로 동일한 도메인 인증서를 계속해서 생성하는 경우) 기존 인증서를 취소(revoke)하면 limit count가 초기화되거나, 감소되지 않는지 궁금할 수도 있다. 공식 사이트에서는 명시적으로 인증서 취소는 limit count를 초기화 시키지 않는다고 알려주고 있다. 그러니, 대부분은 그냥 기다리는게 상책이다.\n인증서 발급 실패와 관련된 제약사항 계정별, 호스트별, 시간당 5번의 실패까지만 허용된다. 호스팅 업체, 대규모 도메인 등록 서비스를 진행하는 경우, 또는 개인 이더라도 인증서 발급을 자동화하도록 구현하는 과정에서 여러가지 테스트를 수행하게 된다. 이 때, 잘못하면 fail vadliation 제약에 해당되어서 한 시간 동안 기다려야 되는 불행한(!) 사태가 발생한다. 그러니, 인증서 발급을 요청하기 전에, DNS 설정은 잘 되었는지, \u0026ldquo;/.well-known/acme-challenge/\u0026rdquo; 경로는 잘 허용되었는지(발급을 위한 validation 과정은 방식에 따라 차이가 있을 수 있다) 확인을 잘 하고 발급 요청을 진행해야 한다.\n\u0026ldquo;new-reg\u0026rdquo;, \u0026ldquo;new-authz\u0026rdquo;, \u0026ldquo;new-cert\u0026rdquo; API는 초당 최대 20개까지 허용된다.\n일반 사용자는 대부분 고려할 필요가 없는 사항인데, 대규모 인증서 발급이 필요한 경우에는 해당 API는 초당 20개까지 가능하다는 점을 염두해둬야 한다. (그렇지만 이렇게 까지 많은 API를 사용하려면 얼마나 많은 도메인을 관리해야 하는걸까..)\n\u0026ldquo;/directory\u0026rdquo; \u0026ldquo;/acme\u0026rdquo; API는 초당 최대 40개까지 허용된다.\n위와 마찬가지로 해당 API는 초당 최대 40개까지 가능하다\nIP별 생성 제한 인증서를 발급하는데는 IP에 대한 제약사항도 존재한다.(이렇게 놓고보니, 참 많은것 같다.)\nIP별로 인증서 생성을 위한 계정을 3시간에 10개까지 생성이 가능하다\nlet\u0026rsquo;s encrypt를 통해서 인증서를 생성하려면 계정이 필요한데 이 계정은 하나의 IP에서 3시간에 10개까지 생성이 가능하다\nIP별로 인증서 생성은 3시간에 500개까지 가능하다\n하나의 IP에서 인증서 생성은 3시간에 500개까지 가능하다. 이걸 넘는 경우는 아마 대규모 호스팅 업체의 경우일 것이다.\n제약에 대한 개인 경험 나의 경우에는 대부분 인증서 발급 자동화를 테스트 하다가 실패 제한(fail validation)에 해당되어서 한시간 기다려야 되는 경우를 빈번하게 경험했다. 또한 대규모 인증서 발급을 해본 경험으로는 let\u0026rsquo;s encrypt client를 개발하다가(자동화 과정에서) IP 당 발급 제한(3시간에 500개)에 해당되는 경우가 많았다. 따라서 let\u0026rsquo;s encrypt를 통해서 대규모로 사용하건/개인용으로 사용하건 제약사항을 잘 알고나서 사용해야 한다. 다행히도 요즘엔 직접 let\u0026rsquo;s encrypt를 사용하기 보다는 다양한 툴들을 통해서 이러한 제약에 해당되지 않고도 테스트를 할 수 있게 잘 구성되어 있는 편이다(stage environment를 잘 활용하자). 그리고 제발 같은 도메인으로 인증서 신규 발급 계속하지 말자. (오늘도 여전히 RTFM)\n참고 https://letsencrypt.org/docs/rate-limits/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/letsencrypt/",
	"title": "Letsencrypt",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/haproxy-1.8/",
	"title": "Haproxy 1.8",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/08/15/seamless-reload-haproxy/",
	"title": "Haproxy를 다운타임 없이 재시작(reload)하기",
	"tags": ["haproxy", "seamless reload", "zero downtime", "haproxy 1.8"],
	"description": "",
	"type": "post",
	"contents": "앞서 Haproxy를 reload fail 해결하기을 살펴보았다. 이 과정에서 Haproxy가 실제로 reload 될 때 seamless 하게 작동할 수 있는가에 대한 확인을 해보았다.\nSeamless reload? Haproxy 를 구성한 뒤에 서비스를 재시작(restart) 하는 것이 아니라, 다시 로드(reload) 하는 순간이 있다. 단순히 설정파일의 일부 값을 변경하고 이를 다시 읽어 들이거나, 연결한 인증서 파일을 갱신/추가/삭제 하고 다시 로드하기 위함 뿐만 아니라, 최근의 도커 기반의 오케스트레이션 시스템을 사용한다면, 서비스는 더 빈번하게 끊김없이 다시 로딩되어야할 필요가 있는 것이다. 이를 seamless reload 라고 하고 (다르게는 gracefully restart 라고도 하는 것 같다) Haproxy 에서 적용한 방법이다.\n리서치 이 주제는 논의가 제법 이전부터 이루어 져왔었고 메일링 리스트 참조, 작년(2017.05)에 공식사이트의 블로그에서 소개되면서 정리가 되었다. Haproxy 의 재기동 절차 공식 블로그에 따르면 Haproxy 가 다시 시작하기 위해서는 다음의 과정을 거친다고 한다.\n새로운 프로세스가 필요한 전체 포트(port)를 수신할 수 있는지(listen) 확인한다. 만약 그렇지 않다면 이전 프로세스(old process)에게 임시적으로(temporarily) 포트 수신은 해제하라는 신호를 보낸다. 새로운 프로세스는 계속해서 다시 확인한다. 포트 수신이 가능한지 확인하는 과정이 계속된다면 일정 횟수 이후에는 새로운 프로세스가 구동하기를 포기하고(give up) 이전 프로세스(old process)에게 유입되는 커넥션을 계속 담당하라고 신호를 보낸다. 만약 새로운 프로세스가 필요한 전체 포트를 수신할 수 있게 되었다면 이전 프로세스(old process)에게 현재의 커넥션에 대한 처리가 종료되고 나서 프로세스를 종료하라고 신호를 보낸다. 여기서 문제가 되는 부분은 2번과 3번 사이인데, 실제로 포트가 해제된 후에(release) 신규 프로세스가 수신대기(listen)하기 까지 아주 약간의 시간이 필요하며 이 과정에서 유입되는 커넥션에 대한 처리가 불가능해진다는 점이다.\n블로그에서는 이 문제를 수정하기 위해서 어떠한 변경사항이 있었는지 리눅스 커널 패치 이야기 까지 하면서 설명해 놓았다. 하나하나 읽어 보는 것도 재미나지만, 간략하게 정리하면 다음과 같다.\nHaproxy 의 Seamless reload 를 위한 기술적 내용 이전 프로세스에서 file descriptor 를 기반으로한 socket 을 통해서 현재 관리되고 있는 커넥션을 새로운 프로세스로 전달한다. 이과정에서 file socket(unix socket) 에서 확인되는 커넥션은 끊기지 않는다. 프로세스는 master-worker 로 동작하면서 이 작업을 수행한다. 요약하자면 unix socket을 사용해서 커넥션 상태를 유지하고, 이를 이전 프로세스와 새로운 프로세스간에 전달함으로써, 커넥션의 유실을 막는다는 것이다.\n참고로 haproxy 의 동작모드는 다음과 같고 Systemd 를 사용해서 reload 를 하려면 -Ws 모드를 사용해야 한다. (또한 빌드할 때 USE_SYSTEMD 가 활성화 되어 있어야 한다)\n-D : start as a daemon. The process detaches from the current terminal after forking, and errors are not reported anymore in the terminal. It is equivalent to the \u0026#34;daemon\u0026#34; keyword in the \u0026#34;global\u0026#34; section of the configuration. It is recommended to always force it in any init script so that a faulty configuration doesn\u0026#39;t prevent the system from booting. -W : master-worker mode. It is equivalent to the \u0026#34;master-worker\u0026#34; keyword in the \u0026#34;global\u0026#34; section of the configuration. This mode will launch a \u0026#34;master\u0026#34; which will monitor the \u0026#34;workers\u0026#34;. Using this mode, you can reload HAProxy directly by sending a SIGUSR2 signal to the master. The master-worker mode is compatible either with the foreground or daemon mode. It is recommended to use this mode with multiprocess and systemd. -Ws : master-worker mode with support of `notify` type of systemd service. This option is only available when HAProxy was built with `USE_SYSTEMD` build option enabled. 설정 파일의 변경 이 기능은 Haproxy 1.8 버전에서 정식으로 적용되었고, 이를 사용하기 위해서는 expose-fd listeners 설정을 추가해주어야 한다. 이 옵션의 의미하는 것은 file descriptor 를 통해서 connection listener 하는 것을 노출(expose)하겠다라는 뜻이다.\n# seamless reload 를 위한 status socket 운영 stats socket /var/lock/subsys/haproxy mode 777 level admin expose-fd listeners 이제 sudo systemd reload haproxy 를 실행하면 정상적으로 reload 가 수행된다.\n테스트 그렇다면 이렇게 seamless reload 를 구성했으니, 정말 커넥션 손실이 없는가 결과를 확인해 보아야 한다. ab(apache bench) 툴을 사용해서 확인해보았다.\n먼저 정상적으로 서비스가 동작하고 있는 상태에서 아무런 액션을 취하지 않았을 때\n$ ab -r -c 100 -n 100000 -l http://haproxy-lb.host/ ab: illegal option -- l This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking haproxy-lb.host (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: Apache Server Hostname: haproxy-lb.host Server Port: 80 Document Path: / Document Length: 32987 bytes Concurrency Level: 100 Time taken for tests: 35.472 seconds Complete requests: 100000 Failed requests: 0 Write errors: 0 Total transferred: 3340500000 bytes HTML transferred: 3298700000 bytes Requests per second: 2819.15 [#/sec] (mean) Time per request: 35.472 [ms] (mean) Time per request: 0.355 [ms] (mean, across all concurrent requests) Transfer rate: 91966.57 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 1 0.9 1 6 Processing: 22 34 11.2 34 1052 Waiting: 19 33 11.1 32 1051 Total: 22 35 11.2 35 1053 Percentage of the requests served within a certain time (ms) 50% 35 66% 37 75% 39 80% 39 90% 42 95% 44 98% 47 99% 50 100% 1053 (longest request) systemd restart haproxy.service 실행한 경우\n$ ab -r -c 100 -n 100000 -l http://haproxy-lb.host/ ab: illegal option -- l This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking haproxy-lb.host (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: Apache Server Hostname: haproxy-lb.host Server Port: 80 Document Path: / Document Length: 32987 bytes Concurrency Level: 100 Time taken for tests: 36.536 seconds Complete requests: 100000 Failed requests: 104 (Connect: 0, Receive: 2, Length: 100, Exceptions: 2) Write errors: 0 Total transferred: 3337159500 bytes HTML transferred: 3295401300 bytes Requests per second: 2737.02 [#/sec] (mean) Time per request: 36.536 [ms] (mean) Time per request: 0.365 [ms] (mean, across all concurrent requests) Transfer rate: 89198.02 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 2 31.7 1 1004 Processing: 3 35 6.9 34 214 Waiting: 0 33 6.8 33 213 Total: 3 36 33.2 35 1078 Percentage of the requests served within a certain time (ms) 50% 35 66% 37 75% 38 80% 39 90% 41 95% 44 98% 47 99% 50 100% 1078 (longest request) systemctl reload haproxy : socket 설정을 추가하기 전 reload\n$ ab -r -c 100 -n 100000 -l http://haproxy-lb.host/ ab: illegal option -- l This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking haproxy-lb.host (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: Apache Server Hostname: haproxy-lb.host Server Port: 80 Document Path: / Document Length: 32987 bytes Concurrency Level: 100 Time taken for tests: 35.514 seconds Complete requests: 100000 Failed requests: 3 (Connect: 0, Receive: 1, Length: 1, Exceptions: 1) Write errors: 0 Total transferred: 3340466595 bytes HTML transferred: 3298667013 bytes Requests per second: 2815.79 [#/sec] (mean) Time per request: 35.514 [ms] (mean) Time per request: 0.355 [ms] (mean, across all concurrent requests) Transfer rate: 91855.95 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 1 0.9 1 8 Processing: 0 35 7.1 34 239 Waiting: 0 33 6.9 32 238 Total: 0 35 7.0 35 239 Percentage of the requests served within a certain time (ms) 50% 35 66% 37 75% 38 80% 39 90% 42 95% 44 98% 47 99% 50 100% 239 (longest request) systemctl reload haproxy : socket expose-fd listeners 설정을 추가한 뒤의 reload\n$ ab -r -c 100 -n 100000 -l http://haproxy-lb.host/ ab: illegal option -- l This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1430300 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking haproxy-lb.host (be patient) Completed 10000 requests Completed 20000 requests Completed 30000 requests Completed 40000 requests Completed 50000 requests Completed 60000 requests Completed 70000 requests Completed 80000 requests Completed 90000 requests Completed 100000 requests Finished 100000 requests Server Software: Apache Server Hostname: haproxy-lb.host Server Port: 80 Document Path: / Document Length: 32987 bytes Concurrency Level: 100 Time taken for tests: 36.077 seconds Complete requests: 100000 Failed requests: 0 Write errors: 0 Total transferred: 3340500000 bytes HTML transferred: 3298700000 bytes Requests per second: 2771.86 [#/sec] (mean) Time per request: 36.077 [ms] (mean) Time per request: 0.361 [ms] (mean, across all concurrent requests) Transfer rate: 90423.65 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 0 1 0.9 1 8 Processing: 22 35 6.9 34 223 Waiting: 20 34 6.8 33 222 Total: 22 36 6.9 35 224 Percentage of the requests served within a certain time (ms) 50% 35 66% 37 75% 39 80% 40 90% 43 95% 46 98% 49 99% 52 100% 224 (longest request) 요약 (동시 100개 10만 커넥션)\nTest case Time taken for tests Failed connection nomal 35.472s 0 restart 36.536s 104 reload without socket 35.514s 3 reload with socket 36.077s 0 이슈 해결 소감. Haproxy 는 초기 설계에서 잦은 재시작을 고려하지 않았었다. 그러나 Haproxy를 사용하는 환경이 변화하면서 seamless reload 에 대한 사용자의 필요가 더 많아졌고, 결국 1.8에서 완전한 해결책을 제시하고 있다. 나의 경우에는 처음에 접한 버전이 1.8 버전대 였지만, 공식 매뉴얼을 찾기 보다 레퍼런스를 구해서 설치하다가, 시행착오를 겪은 케이스이다.\n마지막에 테스트까지 완료하면서 이상없이 재시작되는 것을 보니 마음이 참 뿌듯했다. 또한 개발팀에서 이 하나의 이슈를 완전히 해결하기까지, 무려 10여년동안 내부 아키텍처를 개선해오면서 노력했다는 점이 가장 인상깊다. 나도 어떤 문제를 끝까지 해결하기 위해서 노력하는 개발자가 되어야겠다는 마음가짐을 다시금 새겨본다.\n참고 https://www.haproxy.com/blog/truly-seamless-reloads-with-haproxy-no-more-hacks/ http://git.haproxy.org/?p=haproxy-1.8.git;a=blob_plain;f=contrib/systemd/haproxy.service.in "
},
{
	"permalink": "https://findstar.pe.kr/tags/seamless-reload/",
	"title": "Seamless Reload",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/zero-downtime/",
	"title": "Zero Downtime",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/haproxy-reload-fail/",
	"title": "Haproxy Reload Fail",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/08/14/fix-haproxy-reload-fail/",
	"title": "Haproxy를 reload fail 해결하기",
	"tags": ["haproxy", "haproxy reload fail", "systemd reload", "haproxy 1.8"],
	"description": "",
	"type": "post",
	"contents": "Haproxy를 처음 설치했을 때 로그를 위해서 rsyslog 와 logrotate 를 함께 설정했었다. 그런데 새벽만 되면 (정확히는 logrotate가 실행되고 나서) Haproxy 프로세스가 죽은 다음에 살아나지를 않았다. 이를 해결하기 위해서 수행했던 몇일간의 삽질(!)아닌 삽질기를 정리해보았다.\n문제의 발단. 처음에는 분명히 서비스가 동작하고 있었던것 같은데 다음날 작업해 보려고 하면 서비스가 죽어 있었다. 내가 어제 마지막에 프로세스를 내려 놓았었나? 하고, conf 파일 구성에 신경쓰느라 인지를 못했던 것이었다. 원하는 haproxy 설정을 마무리 하고, 실제 production 에 적용하기 전에, log 파일 및 logrotate 설정을 살펴보고 있었는데, 다음날만 되면 어김없이, haproxy 프로세스가 죽은 상태로 다시 살아나지를 않았다.\n원인 찾기 무엇이 문제일까 고민하다가 일단 journalctl 을 뒤져보기로 했다.\n# journalctl -xe 그랬더니 아래와 같은 메세지를 확인할 수 있었다.\n7월 24 03:41:01 haproxy-lb CROND[18789]: (root) CMD (/usr/lib64/sa/sa1 1 1) 7월 24 03:41:03 haproxy-lb sshd[19005]: Connection closed by 10.41.20.81 port 45638 [preauth] 7월 24 03:41:55 haproxy-lb sshd[19231]: Connection closed by 10.41.20.81 port 48616 [preauth] 7월 24 03:42:01 haproxy-lb CROND[19235]: (root) CMD (/usr/lib64/sa/sa1 1 1) 7월 24 03:42:01 haproxy-lb CROND[19236]: (root) CMD (/opt/SE/update_k5login) 7월 24 03:42:01 haproxy-lb anacron[1389]: Job `cron.daily\u0026#39; started 7월 24 03:42:01 haproxy-lb run-parts(/etc/cron.daily)[19244]: starting logrotate 7월 24 03:42:01 haproxy-lb haproxy[19255]: Shutting down haproxy: [FAILED] 7월 24 03:42:01 haproxy-lb logrotate[19272]: ALERT exited abnormally with [1] 7월 24 03:42:01 haproxy-lb run-parts(/etc/cron.daily)[19274]: finished logrotate 7월 24 03:42:01 haproxy-lb run-parts(/etc/cron.daily)[19276]: starting man-db.cron 7월 24 03:42:01 haproxy-lb run-parts(/etc/cron.daily)[19285]: finished man-db.cron 7월 24 03:42:01 haproxy-lb run-parts(/etc/cron.daily)[19287]: starting mlocate 7월 24 03:42:02 haproxy-lb run-parts(/etc/cron.daily)[19296]: finished mlocate 7월 24 03:42:02 haproxy-lb anacron[1389]: Job `cron.daily\u0026#39; terminated (produced output) 새벽에 cron.daily 작업에 걸려 있던 logrotate 작업이 비정상적으로 종료된것을 확인했다. ALERT exited abnormally 라는 문구가 바로 그것이다. sudo logrotate -f /etc/logrotate.d/haproxy 와 같이 테스트해본 결과 증상이 재현되었다. 구동중인 haproxy 가 재시작 되지 않고 죽어버렸다.\nlogrotate 의심하기 무엇이 문제일까? logrotate 에 설정되어 있는 haproxy /etc/logrotate.d/haproxy을 살펴보았다.\n/var/log/haproxy/*log { create 0644 root root daily rotate 90 missingok notifempty compress sharedscripts postrotate /bin/kill -USR1 `cat /run/haproxy.pid 2\u0026gt;/dev/null` 2\u0026gt;/dev/null || true chown -R root:root /var/log/haproxy/*.log 2\u0026gt;/dev/null endscript } 처음 구성되어 있던 logrotate 설정이다. kill -USR1 으로 실행하고 있었는데 여기서 다시 살아나지 않은것이 문제였다. kill 명령어가 잘못되었을리는 없고, 혹시 몰라 postrotate 에 연결된 task를 다음처럼 바꿔봤지만 여전히 문제였다.\n/bin/systemctl reload haproxy.service \u0026gt; /dev/null 2\u0026gt;/dev/null || true 의문사항 이 시점에서 두가지 의문점을 떠올렸는데\nhaproxy 는 rsyslog 로 (UDP) 로그를 전달하고 이를 저장하는 녀석은 rsyslog 이므로, logrotate 작업은 haproxy 가 아니라, rsyslog 가 재시작되어야 하는게 아닌가? 라는 것.\n그건 그렇고 왜 haproxy 는 재시작이 되지 않는가? 일단 haproxy 가 reload 되지 않는 원인부터 찾아봤다\nHaproxy 설정 살펴보기 사용하는 haproxy 버전은 1.8.12 버전이다.\n# haproxy -vv HA-Proxy version 1.8.12-8a200c7 2018/06/27 Copyright 2000-2018 Willy Tarreau \u0026lt;willy@haproxy.org\u0026gt; Build options : TARGET = linux2628 CPU = generic CC = gcc CFLAGS = -O2 -g -fno-strict-aliasing -Wdeclaration-after-statement -fwrapv -fno-strict-overflow -Wno-unused-label OPTIONS = USE_ZLIB=1 USE_OPENSSL=1 USE_PCRE=1 Default settings : maxconn = 2000, bufsize = 16384, maxrewrite = 1024, maxpollevents = 200 Built with OpenSSL version : OpenSSL 1.0.2k-fips 26 Jan 2017 Running on OpenSSL version : OpenSSL 1.0.2k-fips 26 Jan 2017 OpenSSL library supports TLS extensions : yes OpenSSL library supports SNI : yes OpenSSL library supports : SSLv3 TLSv1.0 TLSv1.1 TLSv1.2 Built with transparent proxy support using: IP_TRANSPARENT IPV6_TRANSPARENT IP_FREEBIND Encrypted password support via crypt(3): yes Built with multi-threading support. Built with PCRE version : 8.32 2012-11-30 Running on PCRE version : 8.32 2012-11-30 PCRE library supports JIT : no (USE_PCRE_JIT not set) Built with zlib version : 1.2.7 Running on zlib version : 1.2.7 Compression algorithms supported : identity(\u0026#34;identity\u0026#34;), deflate(\u0026#34;deflate\u0026#34;), raw-deflate(\u0026#34;deflate\u0026#34;), gzip(\u0026#34;gzip\u0026#34;) Built with network namespace support. Available polling systems : epoll : pref=300, test result OK poll : pref=200, test result OK select : pref=150, test result OK Total: 3 (3 usable), will use epoll. Available filters : [SPOE] spoe [COMP] compression [TRACE] trace 다시한번 sudo systemctl reload haproxy 를 입력해서 증상을 재현하고\nsudo journalctl --unit=haproxy 로 확인해보면 아래와 같이 실패한다.\n7월 26 15:20:13 haproxy-lb systemd[1]: haproxy.service: main process exited, code=killed, status=9/KILL 7월 26 15:20:13 haproxy-lb haproxy[35552]: Shutting down haproxy: [FAILED] 구글링을 시도해보았다. \u0026ldquo;haproxy reload not working\u0026rdquo; 이라는 키워드에서 뭔가가 여러 결과가 나타났다. 그러던중 메일링 리스트에서 관련 이슈를 찾을 수 있다. https://www.mail-archive.com/haproxy@formilux.org/\n제법 오래된 이슈인듯 한데, 이러한 경우를 \u0026ldquo;seamless reloads\u0026rdquo; 라고 지칭한다는 것을 알아내고 계속 리서칭해봤다. https://discourse.haproxy.org/t/seamless-reloads-dont-work-with-systemd/1954/16 에서의 사례가 참고가 많이 되었다. 그리고 공식사이트의 블로그를 확인하고 나서 문제점을 파악할 수 있었다.\n문제점 haproxy 를 seamless reload 하려면 haproxy -Ws 모드가 활성화된 master-worker 모드로 동작하도록 해야한다. 이를 위해서 빌드 할 때 \u0026lsquo;systemd\u0026rsquo; 를 지원하도록 빌드되어야 한다. 빌드 옵션은 haproxy -vv 에서 USE_SYSTEMD 가 활성화 되어 있는 것을 확인하면 된다. haproxy 공식 git repo 에서 systemd script 를 사용하지 않고 구식의 init 스크립트를 사용했다는 점 (어느 설치 블로그에서 복사해옴) 해결방법 Haproxy 에서 systemd 를 사용하여 reload 를 하기 위해서는 make 를 진행할 때 USE_SYSTEMD 옵션을 활성화 해서 빌드하자 나의 경우 make TARGET=linux2628 USE_OPENSSL=1 USE_PCRE=1 USE_ZLIB=1 USE_SYSTEMD=1 사용함. 공식 git repo 의 systemd script 를 사용하여 서비스 등록 @SBINDIR@ 은 \u0026ldquo;/usr/local/sbin/\u0026rdquo; 으로 대체해주었다. [Unit] Description=HAProxy Load Balancer After=network.target [Service] Environment=\u0026#34;CONFIG=/etc/haproxy/haproxy.cfg\u0026#34; \u0026#34;PIDFILE=/run/haproxy.pid\u0026#34; ExecStartPre=@SBINDIR@/haproxy -f $CONFIG -c -q ExecStart=@SBINDIR@/haproxy -Ws -f $CONFIG -p $PIDFILE ExecReload=@SBINDIR@/haproxy -f $CONFIG -c -q ExecReload=/bin/kill -USR2 $MAINPID KillMode=mixed Restart=always SuccessExitStatus=143 Type=notify # The following lines leverage SystemD\u0026#39;s sandboxing options to provide # defense in depth protection at the expense of restricting some flexibility # in your setup (e.g. placement of your configuration files) or possibly # reduced performance. See systemd.service(5) and systemd.exec(5) for further # information. # NoNewPrivileges=true # ProtectHome=true # If you want to use \u0026#39;ProtectSystem=strict\u0026#39; you should whitelist the PIDFILE, # any state files and any other files written using \u0026#39;ReadWritePaths\u0026#39; or # \u0026#39;RuntimeDirectory\u0026#39;. # ProtectSystem=true # ProtectKernelTunables=true # ProtectKernelModules=true # ProtectControlGroups=true # If your SystemD version supports them, you can add: @reboot, @swap, @sync # SystemCallFilter=~@cpu-emulation @keyring @module @obsolete @raw-io [Install] WantedBy=multi-user.target 추가 작업 logrotate 에서 haproxy 를 재시작하지 않고, rsyslog 를 재시작해주도록 수정해주었다.\n/var/log/haproxy/*log { daily rotate 90 create 0644 nobody nobody missingok notifempty compress sharedscripts postrotate /bin/systemctl restart rsyslog.service \u0026gt; /dev/null 2\u0026gt;/dev/null || true endscript } 정리 Haproxy 는 load balancer 로 사용하기 위해서 가급적이면 재시작하는 것을 지양하는 형태이다. 이 시점에서 logrotate 를 위해서 haproxy 를 재시작하는 것이 얼마나 엉뚱한 행동이었는지 알게되었다. haproxy 는 log 를 rsyslog 로 UDP 형태로 전송하기 때문에, 로그를 남기는 것은 rsyslog 이고, 실제로 logroate 에서는 rsyslog 를 재시작해주는게 맞다. haproxy 는 다음의 3가지 모드로 구동이 가능하다. 이중에서 내가 구동하는 모드는 -D 옵션으로 데몬 형태로 구동하고 있었는데 systemd 를 사용하여 reload 하기 위해서는 -W 모드 또는 -Ws 모드가 필요하다. 나의 경우에는 -Ws 모드를 사용하도록 바꾸기로 했다. 그리고 이를 위해서 USE_SYSTEMD 옵션을 활성화 할 필요가 있다. 남은 이야기 작업 이후 systemd reload haproxy 명령어를 통해서 haproxy 가 정상적으로 다시 리로드 된다. 이로써, configuration을 바꾸었거나, 인증서를 교체하는 작업등을 수행할 때 restart 하지 않고서도 graceful 하게 재시작 할 수 있게 되었다. 그런데 여기서 의문이 하나 추가되었는데 reload 될때 seamless, 그러니까 커넥션의 유실없이 정상적으로 haproxy 가 다시 로딩되는지 문제가 없는지, 확인해보고 싶었다. 그래서 실제로 테스트를 수행하면서 보완적업을 하게 되었는데, 이건 별도 포스트로 정리해보았다.\n참고 메일링 리스트에서 관련 이슈를 찾을 수 있다. 제법 오래된 이슈인듯. https://www.mail-archive.com/haproxy@formilux.org/ 이 이슈를 해결하면서 가장 많은 참고가 되었던 discussion https://discourse.haproxy.org/t/seamless-reloads-dont-work-with-systemd/1954/16 haproxy 공식 사이트에서 해결방법을 안내하고 있다. https://www.haproxy.com/blog/truly-seamless-reloads-with-haproxy-no-more-hacks/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/systemd-reload/",
	"title": "Systemd Reload",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/dos-attack/",
	"title": "Dos Attack",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/08/13/block-dos-attacks-using-haproxy-rate-limit/",
	"title": "Haproxy를 활용해서 dos 공격 방어하기",
	"tags": ["haproxy", "dos attack", "rate limit", "haproxy 1.8"],
	"description": "",
	"type": "post",
	"contents": "앞서 Haproxy 로 로드밸런서를 구성하는 방법을 살펴보았다. Haproxy를 로드밸런서로 운영하면서 DOS 공격이 유입되어 서비스 운영에 장애가 발생하였다. 이번에는 Haproxy 의 기능을 이용해서 단순한 패턴의 DOS 공격을 방어하는 방법을 살펴보았다.\nDOS 란? 먼저 DOS 공격의 정의를 살펴보았다.(DDOS 가 아니다, DOS다) 위키피디아 에 따르면 DOS 란 \u0026ldquo;Denial of Service attack\u0026rdquo;. 서비스 거부 공격으로 악의적 공격으로 해당 시스템의 자원을 부족하게 하여 원래 의도된 용도로 사용하지 못하게 하는 공격을 의미한다. 여기에서는 Haproxy를 사용하여 가장 단순한 패턴인 단일 IP에서 유입되는 과도한 request 를 방어하는 방법을 살펴보았다.\n구성 요건 Haproxy 의 rate limit 기능을 사용하여 단일 IP 에서 유입되는 과도한 request 를 막는다.\n내부 IP 대역은 제외한다. - vpc 내의 서버들이나, 내부 연동 서버들은 제한 대상에서 제외한다.\n특정 ULR 들은 제외한다. - static file 등을 위한 whitelist 로 관리되는 url 목록을 관리한다.\nHaproxy 설정하기 우선 제외할 IP 대역과, URL 리스트 파일을 정의하자.\n# ACL \u0026#34;vpc-network\u0026#34; 선언 acl vpc-network src 10.10.1.0/16 # ACL \u0026#34;allow_url\u0026#34; 선언 # whitelist url 목록 파일 지정 acl accept_url url_beg -i -f /etc/haproxy/accept_url.list accept_url.list 파일의 내용은 다음과 같다.\n# Allowd url /public /assets /resource /feed 이제 client ip 를 저장할 공간(sticky session)을 정의하자.\n# stickiness (Sticky Sessions)을 사용하여 10m 메모리에 10초동안 요청되는 client ip 를 저장한다. (1분 유지) # 이를 통한 ACL 제어는 다음의 컨트롤이 가능하다 # - conn_rate(10s) : 10초 동안 TCP 커넥션의 rate # - conn_cur : 동시 접속 TCP 커넥션 # - http_req_rate(10s) : 10초 동안의 HTTP 커넥션 rate # - http_err_rate(20s) : 20초 동안의 HTTP 커넥션 에러(비정상적인 커넥션 요청) # 여기서는 http_req_rate를 사용했다. stick-table type ip size 500m expire 1m store http_req_rate(10s) 정의한 stick table의 카운트를 올리려면 다음과 같이 지정하자\n# 카운트 table 유지 vpc network 접속이 아니고 AND 허용되는 url 가 아닐 때 ( true AND true 조건 형태) http-request track-sc1 src table www if !vpc-network !accept_url ACL \u0026lsquo;dos-attack\u0026rsquo; 정의\n# 위에서 정의한 stick-table 에 따라서 10초 동안 요청되는 HTTP connectoin rate 가 100이 넘으면 DOS 공격으로 판단. acl dos-attack src_http_req_rate(www) ge 100 DOS 공격으로 판단될 때 사용할 backend 연결\nuse_backend blocked_page if dos-attack !accept_url blocked_page backend 정의\nbackend blocked_page mode http # 벡엔드에서 연결될 서버가 없으므로 503 에러가 발생할 것이다. 이 때 보여줄 페이지는 아래와 같다. errorfile 503 /etc/haproxy/errors/blocked.http block 되었을 때 보여줄 에러 페이지\nstatus code 429는 too many connection 오류를 의미한다.\nHTTP/1.1 429 Too Many Requests Content-Type: text/html Retry-After: 600 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;ko\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Can not connect page\u0026lt;/title\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Nothing\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 완성된 Haproxy conf 위의 과정을 거치면 대략 아래와 같은 내용의 파일을 구성할 수 있다.\nfrontend www ... # ACL \u0026#34;vpc-network\u0026#34; 선언 acl vpc-network src 10.10.1.0/16 # ACL \u0026#34;allow_url\u0026#34; 선언 # whitelist url 목록 파일 지정 acl accept_url url_beg -i -f /etc/haproxy/accept_url.list # stickiness (Sticky Sessions)을 사용하여 10m 메모리에 10초동안 요청되는 client ip 를 저장한다. (1분 유지) # 이를 통한 ACL 제어는 다음의 컨트롤이 가능하다 # - conn_rate(10s) : 10초 동안 TCP 커넥션의 rate # - conn_cur : 동시 접속 TCP 커넥션 # - http_req_rate(10s) : 10초 동안의 HTTP 커넥션 rate # - http_err_rate(20s) : 20초 동안의 HTTP 커넥션 에러(비정상적인 커넥션 요청) # 여기서는 http_req_rate를 사용했다. stick-table type ip size 500m expire 1m store http_req_rate(10s) # 위에서 정의한 stick-table 에 따라서 10초 동안 요청되는 HTTP connectoin rate 가 100이 넘으면 DOS 공격으로 판단. acl dos-attack src_http_req_rate(www) ge 100 # 카운트 table 유지 vpc network 접속이 아니고 AND 허용되는 url 가 아닐 때 ( true AND true 조건 형태) http-request track-sc1 src table www if !vpc-network !accept_url use_backend blocked_page if dos-attack !accept_url default_backend web-svr backend blocked_page mode http # 벡엔드에서 연결될 서버가 없으므로 503 에러가 발생할 것이다. 이 때 보여줄 페이지는 아래와 같다. errorfile 503 /etc/haproxy/errors/blocked.http backend web-svr ...... 테스트 apache bench 를 사용해서 테스트 해보았다.\n$ ab -r -c 10 -n 2000 -l http://my.service.host/ This is ApacheBench, Version 2.3 \u0026lt;$Revision: 1796539 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking my.service.host (be patient) Completed 200 requests Completed 400 requests Completed 600 requests Completed 800 requests Completed 1000 requests Completed 1200 requests Completed 1400 requests Completed 1600 requests Completed 1800 requests Completed 2000 requests Finished 2000 requests Server Software: Apache Server Hostname: my.service.host Server Port: 80 Document Path: / Document Length: Variable Concurrency Level: 10 Time taken for tests: 2.991 seconds Complete requests: 2000 Failed requests: 0 Non-2xx responses: 1901 Total transferred: 24512750 bytes HTML transferred: 24332595 bytes Requests per second: 668.73 [#/sec] (mean) Time per request: 14.954 [ms] (mean) Time per request: 1.495 [ms] (mean, across all concurrent requests) Transfer rate: 8004.14 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 2 5 4.2 5 71 Processing: 6 10 7.3 8 73 Waiting: 5 9 5.5 7 73 Total: 9 15 8.4 13 80 Percentage of the requests served within a certain time (ms) 50% 13 66% 13 75% 14 80% 14 90% 16 95% 40 98% 45 99% 48 100% 80 (longest request) Non-2xx responses 라고 표시된 부분이 haproxy 에서 block 되어 429 response 응답을 수신한 부분이고, 로드 밸런서로 지정된 web-srv 서버들에도 access log 가 남지 않았다.\nrsyslog 로깅 변경 Haproxy 는 rsyslog 로 로그를 남기는데 여기서 dos attack 인 녀석들만 로그를 기록해 보자. /etc/rsyslog.d/haproxy-dos.conf 파일을 구성한다. 사전에 haproxy-info.log 가 존재하고, 로그에 dos 용 backend 인 \u0026lsquo;blocked_page\u0026rsquo; 라는 \u0026ldquo;backend name\u0026rdquo; 을 로깅하도록 커스텀 포맷이 지정되어 있다고 가정한다.\n# Provides UDP syslog reception $ModLoad imfile $template Haproxy, \u0026#34;%msg%\\n\u0026#34; $InputFilePollInterval 1 # UDP 로 메세지를 수신하지 않고, haproxy-info 파일을 watch 하는 형태로 입력받음 # 기존에 이미 haproxy-info.log 가 있다고 가정. $InputFileName /var/log/haproxy/haproxy-info.log $InputFileTag haproxy-dos: $InputFileStateFile haproxy-dos $InputFileFacility local0 $InputRunFileMonitor # blocked_page 라는 단어가 포함된 메세지를 haproxy-dos.log 에 다시 기록함 :syslogtag, isequal, \u0026#34;haproxy-dos:\u0026#34; { :msg, contains, \u0026#34;blocked_page\u0026#34; { local0.* /var/log/haproxy/haproxy-dos.log;Haproxy } stop } 적용 후기 기존에 nginx rate limit으로도 동일한 구성을 해보았는데, Haproxy 는 좀 더 유연하고, 더 디테일하게 제어가 가능했다. 여기서는 가장 기본적인 \u0026ldquo;단일 IP에서의 과도한 유입\u0026quot;만 막을 수 있었지만, 더 연구해서 다른 패턴도 막을 수 있도록 알아봐야겠다.(.. 언제?) 참고 https://blog.codecentric.de/en/2014/12/haproxy-http-header-rate-limiting/ https://gist.github.com/jeremyj/e964a951634f1997daea https://www.loadbalancer.org/blog/simple-denial-of-service-dos-attack-mitigation-using-haproxy-2/ https://www.haproxy.com/blog/use-a-load-balancer-as-a-first-row-of-defense-against-ddos/ "
},
{
	"permalink": "https://findstar.pe.kr/tags/rate-limit/",
	"title": "Rate Limit",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/07/27/install-haproxy/",
	"title": "Haproxy 설치해서 로드 밸런서로 활용하기",
	"tags": ["haproxy", "install", "haproxy 1.8"],
	"description": "",
	"type": "post",
	"contents": "Load Balancer 로 활용할 수 있는 Reverse Proxy 인 Haproxy 의 설치와 설정 방법을 정리해보았다.\nHaproxy 는 Load balancer 로 활용할 수 있으며, 다양한 설정이 가능하고, nginx reverse proxy 에 비해서 active health check가 가능하기 때문에 더 안정적으로 운영할 수 있다. 다만 설정이 조금 복잡해서 익숙해 지는데 시간이 걸렸다.\n설치 환경 haproxy 를 설치한 서버는 Centos 7.4 서버였다. yum 을 사용할것인지 아니면 컴파일을 해서 사용할지 결정해야 했는데 결국은 컴파일해서 설치하기로 했다. 그 이유는 yum 기본 repo 에 있는 haproxy 버전이 낮고, 대부분의 레퍼런스에서 컴파일해서 설치하고 있기에 이대로 진행했다. (다만, 컴파일에서 옵션을 잘못 지정해서 나중에 엄청난 삽질을 하게되었는데, 이건 따로 이야기 하기로 하고 설치 내역을 정리해보았다)\n필요한 yum 패키지들을 설치해주자.\nyum install gcc openssl openssl-devel pcre-static pcre-devel systemd-devel Haproxy 다운로드 \u0026amp; 컴파일 haproxy 소스 파일을 다운로드 받는다. 설치하는 현재 stable 버전은 1.8.12 버전이다.\nwget http://www.haproxy.org/download/1.8/src/haproxy-1.8.12.tar.gz tar zxvf haproxy-1.8.12.tar.gz 컴파일을 하자, 이때 옵션을 지정해줘야 한다. TARGET 정보는 README 파일에서 확인하자. SSL 인증서를 활성화 하기 위해서 USE_OPENSSL 를 enable 해주고, systemd 를 통해서 서비스를 컨트롤 하기 위해서 USE_SYSTEMD 도 활성화 해주었다.\nmake TARGET=linux2628 USE_OPENSSL=1 USE_PCRE=1 USE_ZLIB=1 USE_SYSTEMD=1 컴파일을 완료했다면 설치하자 /usr/local/sbin/haproxy 에 실행파일이 설치된다.\nmake install install -d \u0026#34;/usr/local/sbin\u0026#34; install haproxy \u0026#34;/usr/local/sbin\u0026#34; install -d \u0026#34;/usr/local/share/man\u0026#34;/man1 install -m 644 doc/haproxy.1 \u0026#34;/usr/local/share/man\u0026#34;/man1 install -d \u0026#34;/usr/local/doc/haproxy\u0026#34; for x in configuration management architecture peers-v2.0 cookie-options lua WURFL-device-detection proxy-protocol linux-syn-cookies network-namespaces DeviceAtlas-device-detection 51Degrees-device-detection netscaler-client-ip-insertion-protocol peers close-options SPOE intro; do \\ install -m 644 doc/$x.txt \u0026#34;/usr/local/doc/haproxy\u0026#34; ; \\ done 정상적으로 설치가 되었는지 확인하자\nsudo haproxy -v Haproxy systemd 등록 이제 systemd 에 등록하고 서비스를 컨트롤할 수 있도록 해주어야 한다. haproxy git repo 에서 제공하는 system file 을 확인하자. 해당 파일을 /etc/systemd/system/haproxy.service 에 저장하고 sudo systemd daemon-reload 실행하자.\nsudo wget \u0026#34;http://git.haproxy.org/?p=haproxy-1.8.git;a=blob_plain;f=contrib/systemd/haproxy.service.in\u0026#34; -O /etc/systemd/system/haproxy.service sudo systemd daemon-reload 기타 작업 haproxy 관련 디렉토리를 만들자.\nsudo mkdir -p /etc/haproxy sudo mkdir -p /var/log/haproxy sudo mkdir -p /etc/haproxy/certs sudo mkdir -p /etc/haproxy/errors/ haproxy 데몬을 구동할 haproxy 계정을 만들자\nsudo useradd -r haproxy 설정 파일 example 디렉토리의 haproxy config 을 참고해서 원하는 형태의 cfg 를 구성해야 한다. 이게 제일 힘든 부분이긴 한데 고생한 결과물을 기록했다.\n# 다음은 80 과 443 으로 haproxy 를 reverse proxy 형태로 구성하여 LB로 사용하는 설정이다. global daemon # 연결할 수 있는 최대 connection 을 지정한다. 이걸 안하면 기본값이 2000 으로 설정된다. maxconn 81920 # haproxy 프로세서를 구동할 user (gid/uid를 지정할 수도 있다.) user haproxy # ssl 을 구성할 때 key size를 지정한다. tune.ssl.default-dh-param 2048 # haproxy는 로그를 남기기 위해서 file io 를 직접 처리하지 않는다. rsyslog 로 UDP 전송을 한다. log 127.0.0.1 local0 # ciphers 설정 # WindowsXP SP3 IE7,8 호환이 필요하지 않을 경우 !3DES도 추가한다. ssl-default-bind-ciphers ECDH+AESGCM:ECDH+AES128:ECDH+AES256:DH+AES128:DH+AES256:DH+CAMELLIA128:DH+CAMELLIA256:DH+SEEDCBC:RSA:!aNULL:!MD5:!eNULL:!RC4 defaults # Haproxy 에서 log format 은 총 5가지 (default, tcp, http, clf, custom) 을 지원한다 # 1. 기본적인 log format # datetime host process_name[pid]: Connect from source_ip:source_port to dest_ip:dest_port (frontend_name/mode) # ex) Feb 6 12:12:09 localhost haproxy[14385]: Connect from 10.0.1.2:33312 to 10.0.3.31:8012 (www/HTTP) # # 2. `option tcplog` 가 지정된 경우 # datetime host process_name[pid]: client_ip:client_port [accept_date] frontend_name backend_name/server_name tw/tc/tx bytes_read termination_state actionn/feconn/beconn/srv_conn/retries srv_queue/backend_queue # ex) Feb 6 12:12:56 localhost haproxy[14387]: 10.0.1.2:33313 [06/Feb/2009:12:12:51.443] fnt bck/srv1 0/0/5007 212 -- 0/0/0/0/3 0/0 # # 3. `option httplog` 가 지정된 경우 # HTTP 프록시를 구성하는 경우에 제일 많이 사용되는 로그 포맷 # datetime host process_name[pid]: client_ip:client_port [request_date] frontend_name backend_name/server_name TR/Tw/Tc/Tr/Ta status_code bytes_read captured_request_cooie captured_response_cookie termination_state actionn/feconn/beconn/srv_conn/retries srv_queue/backend_queue {captured_request_headers} {captured_response_headers} \u0026#34;http_request\u0026#34; # ex) # Feb 6 12:14:14 localhost haproxy[14389]: 10.0.1.2:33317 [06/Feb/2009:12:14:14.655] http-in static/srv1 10/0/30/69/109 200 2750 - - ---- 1/1/1/1/0 0/0 {1wt.eu} {} \u0026#34;GET /index.html HTTP/1.1\u0026#34; # # 4. `log-format` 을 통해서 custom 구성을 한경우 # `log-format` 을 지정하면 커스텀 로그 포맷 구성이 가능함 # 자세한 변수는 http://cbonte.github.io/haproxy-dconv/1.8/configuration.html#8.2.4 참고 # # 5. clf 는 (Common Logging Format) # # 이설정파일에서는 custom log format 을 사용했다. log global mode http option httplog clf #Enable logging of null connections (헬스체크와 같이 시스템이 살아 있는지 확인하기 위해서 일정하게 접속하는 커넥션에 대한 로그 사용) option dontlognull #Enable skip logging normal connection log 일반적인 http status 200 로그는 남기지 않는다. (https://www.slideshare.net/haproxytech/haproxy-best-practice) 참고 option dontlog-normal #request-요청을 서버로 보낼 때 `X-Forwarded-For` 를 헤더에 추가한다 option forwardfor # 기본적으로 HAProxy 는 커넥션 유지 관점에서 keep-alive 모드로 동작을 하는데, 각각의 커넥션은 request-요청과 reponse-응답을 처리하고나서 # 새로운 request을 받기까지 connection idle 상태(유휴상태)로 양쪽이 연결되어 있다. # 이 동작모드를 변경하려면 \u0026#34;option http-server-close\u0026#34; \u0026#34;option forceclose\u0026#34; \u0026#34;option httpclose\u0026#34; \u0026#34;option http-tunnel\u0026#34; 의 옵션이 가능한데, # \u0026#34;option http-server-close\u0026#34; 는 클라이언트 사이드에서 HTTP keep-alive를 유지하고 파이프라이닝을 지원하면서 서버 사이드에 커넥션을 닫는 형태를 설정한다. # 이는 클라이언트 사이드에서 최저 수준의 응답지연을 제공하고 \u0026#34;option forceclose\u0026#34; 와 비슷하게 서버사이드에서 리소스를 재활용할 수 있게 되어 # backend 에서 빠르게 세션을 재사용할 수 있도록 해준다. option http-server-close timeout http-request 10s timeout client 20s timeout connect 4s timeout server 30s timeout http-keep-alive 10s # haproxy 의 상태정보를 확인할 수 있는 기능을 활성화 한다. \u0026#34;stats\u0026#34; # 이 포트는 외부에 노출되지 않도록 주의하자. # 접근 가능한 사용자를 제한하기 위해서 auth 를 추가했다. listen stats bind :9000 # Listen on localhost:9000 stats enable # Enable stats page stats realm Haproxy\\ Statistics # Title text for popup window stats uri /haproxy_stats # Stats URI stats auth admin1:password1 stats auth admin2:password2 frontend www # 기본적으로 80 번을 listen 한다. bind *:80 # https 지원을 위한 443 listen # cert 파일은 /etc/haproxy/certs 디렉토리에 넣는다. # 키 파일과 cert 파일을 하나로 합쳐서 넣으면 된다. bind *:443 ssl crt /etc/haproxy/certs maxconn 81920 # 압축 알고리즘 gzip 적용 compression algo gzip compression type text/plain application/json application/xml # ACL \u0026#34;deny_useragent\u0026#34; 선언 # 보안취약점 스캔 툴 같은 agent 들을 막기 위한 list 파일을 구성했다. # -i 옵션은 대소문자 구분하지 않음 # -f 는 파일에서 매칭 acl deny_useragent hdr_sub(user-agent) -i -f /etc/haproxy/deny_useragent.list # Private Ip 대역 확인용 ACL \u0026#34;private-network\u0026#34; 선언 # 같은 VPC 대역이라던가.. acl private-network src 10.10.0.0/16 http-request deny if deny_useragent # 모니터링을 위한 주소 설정 monitor-uri /monitor # 모니터링 주소는 같은 private ip 대역이 아니라면 실패 처리 monitor fail if !private-network # log format 을 위해서 header 정보를 capture 한다. custom format %hr 에서 사용되며 delimiter : \u0026#39;|\u0026#39;로 구분된다. capture request header Host len 128 capture request header User-Agent len 64 capture request header Referrer len 64 # 다음은 custom format 을 위한 지 # %ci : client ip # %trl : local date time # %HM : HTTP method (ex GET) # %HU : HTTP request URI (ex: /foo?bar=baz) # %HV : HTTP version (ex: HTTP/1.1) # %ST : Status Code # %B : bytes_read # %hr : header values (host, agent, referrer) 위에서 capture 한 값들. # %s : server_name # %b : backend_name # %TR : time to receive the full request from 1st byte # %Tw : total time in milliseconds spent waiting in the various queues # %Tc : total time in milliseconds spent waiting for the connection to establish to the final server, including retries # %Tr : total time in milliseconds spent waiting for the server to send a full HTTP response, not counting data # %Ta : time the request remained active in haproxy, which is the total time in milliseconds elapsed between the first byte of the request was received and the last byte of response was sent # %ac : total number of concurrent connections on the process when the session was logged # %fc : total number of concurrent connections on the frontend when the session was logged # %bc : total number of concurrent connections handled by the backend when the session was logged # %sc : total number of concurrent connections still active on the server when the session was logged # %rc : the number of connection retries experienced by this session when trying to connect to the server log-format \u0026#34;%ci\\ [%trl]\\ %HM\\ \\\u0026#34;%HU\\\u0026#34;\\ \\\u0026#34;%HV\\\u0026#34;\\ %ST\\ %B\\ %hr\\ %s\\ %b\\ %TR/%Tw/%Tc/%Tr/%Ta\\ %ac/%fc/%bc/%sc/%rc\u0026#34; # 기본적으로 사용할 backend 를 지정한다. default_backend web-svr backend web-svr # 로드 밸런싱을 라운드 로빈으로 지정한다. 이밖에도 leastconn 을 많이 지정하는것 같다. balance roundrobin # 연결할 서버들이 active health check를 지정한다. (nginx 와 가장큰 다른점이라고 생각된다. nginx 는 active health check 가 plus(유료)버전에서만 가능하다) option httpchk HEAD /_health HTTP/1.1\\r\\nHost:\\ localhost # 에러 파일 설정 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # backend 서버들로 request-요청 을 proxy 로 전달할 때 Protocol(https) 와 Port 정보를 함께 전달 # X-Forwarded-For 는 앞의 option 으로 전달됨 (option forwardfor) http-request set-header X-Forwarded-Port %[dst_port] http-request add-header X-Forwarded-Proto https if { ssl_fc } # 연결할 서버들의 리스트다. 3번 health check 가 실패하면 빼고, 2번 성공하면 다시 LB 대상에 포함시킨다. server my-web-server-1-host 10.10.192.1:80 check fall 3 rise 2 server my-web-server-2-host 10.10.192.2:80 check fall 3 rise 2 server my-web-server-3-host 10.10.192.3:80 check fall 3 rise 2 server my-web-server-4-host 10.10.192.4:80 check fall 3 rise 2 server my-web-server-5-host 10.10.192.5:80 check fall 3 rise 2 server my-web-server-6-host 10.10.192.6:80 check fall 3 rise 2 server my-web-server-7-host 10.10.192.7:80 check fall 3 rise 2 server my-web-server-8-host 10.10.192.8:80 check fall 3 rise 2 server my-web-server-9-host 10.10.192.9:80 check fall 3 rise 2 server my-web-server-10-host 10.10.192.10:80 check fall 3 rise 2 설정 파일이 완료되었다면 아래와 같이 이상이 없는지 체크할 수 있다.\nsudo haproxy -f /etc/haproxy/haproxy.cfg -c 구동 이제 systemd 를 사용해서 구동해보자\nsystemd start haproxy.service 재시작은 reload 하면된다. 만약 컴파일시에 USE_SYSTEMD 옵션을 주지 않았다면 재시작에 실패할 수 있다.\nsystemd reload haproxy.service 설치후 추가할일 rsyslog 에 haproxy 용 로그를 남기도록 설정한다. /etc/rsyslog.d/haproxy.conf 파일을 다음과 같이 구성한다.\n# Provides UDP syslog reception $ModLoad imudp $UDPServerRun 514 $template Haproxy, \u0026#34;%msg%\\n\u0026#34; #rsyslog 에는 rsyslog 가 메세지를 수신한 시각 및 데몬 이름같은 추가적인 정보가 prepend 되므로, message 만 출력하는 템플릿 지정 # 이를 haproxy-info.log 에만 적용한다. # 모든 haproxy 를 남기려면 다음을 주석해재, 단 access log 가 기록되므로, 양이 많다. #local0.* /var/log/haproxy/haproxy.log # local0.=info 는 haproxy 에서 에러로 처리된 이벤트들만 기록하게 됨 (포맷 적용) local0.=info /var/log/haproxy/haproxy-info.log;Haproxy # local0.notice 는 haproxy 가 재시작되는 경우와 같은 시스템 메세지를 기록하게됨 (포맷 미적용) local0.notice /var/log/haproxy/haproxy-allbutinfo.log logroate 에 haproxy 설정을 추가한다. /etc/logrotate.d/haproxy 파일을 다음과 같이 구성한다. haproxy 는 재시작할 필요가 없으므로 rsyslog 를 재시작해준다.\n/var/log/haproxy/*log { daily rotate 90 create 0644 nobody nobody missingok notifempty compress sharedscripts postrotate /bin/systemctl restart rsyslog.service \u0026gt; /dev/null 2\u0026gt;/dev/null || true endscript } 설치 후기 익숙해 지는데 시간이 걸리긴 했지만, 성능도 좋고 원하는 대부분의 기능이 제공되므로 만족스러웠다. 다만 설정과 관련된 방식이 친절하지 못하다고 느껴지는건 단순히 익숙하지 않음 때문은 아닌듯. 그리고 여전히 문서를 잘 읽어야 한다는 생각이 든다. 특히나 소스 압축 풀고 example 폴더 한번만 주의 깊게 봤으면 고생을 덜 했을 텐데 싶다. 참고 https://cbonte.github.io/haproxy-dconv/1.8/configuration.html http://git.haproxy.org/?p=haproxy-1.8.git;a=blob_plain;f=contrib/systemd/haproxy.service.in https://www.upcloud.com/support/haproxy-load-balancer-centos/ http://blog.whitelife.co.kr/321 https://medium.com/@jinro4/%EA%B0%9C%EB%B0%9C-haproxy-%EC%84%A4%EC%B9%98-%EB%B0%8F-%EA%B8%B0%EB%B3%B8-%EC%84%A4%EC%A0%95-f4623815622 http://sseungshin.tistory.com/77 "
},
{
	"permalink": "https://findstar.pe.kr/tags/install/",
	"title": "Install",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/elasticsearch/",
	"title": "ElasticSearch",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/07/14/elasticsearch-wildcard-to-ngram/",
	"title": "ElasticSearch 에서 wildcard 쿼리 대신 ngram을 활용하는 방법",
	"tags": ["ElasticSearch", "wildcard", "ngram", "Partial Matching", "match_phrase"],
	"description": "",
	"type": "post",
	"contents": "ElasticSearch를 사용하면서 DSL 을 구성할 때, RDBMS 의 like \u0026quot;%keyword%\u0026quot; 와 같은 쿼리를 대체하기 위해서 wildcard 를 사용하는 경우를 몇번 목격하였다. 이 경우 원하는 결과를 제대로 얻을 수도 없을 뿐더러, 성능의 문제가 발생하기 쉬운데, 이를 ngram 으로 대체하여 원하는 결과를 얻는 방법을 확인해 보았다.\n콘텐츠 검색의 경우 시의 내용을 DB 와 ElasticSearch 에 저장하고 쿼리를 통해서 원하는 시의 본문 전체 내용을 찾는 방법을 가정해 보자. 저장된 내용은 김소월의 진달래꽃 을 예시로 가정해봤다.\n진달래꽃 나 보기가 역겨워 가실 때에는 말없이 고이 보내 드리우리다 영변에 약산 진달래꽃 아름 따다 가실 길에 뿌리우리다 가시는 걸음 걸음 놓인 그 꽃을 사뿐히 즈려밟고 가시옵소서 나 보기가 역겨워 가실 때에는 죽어도 아니 눈물 흘리우리다 RDBMS 의 경우 먼저 RDBMS 의 like \u0026quot;%keyword%\u0026quot; 쿼리를 생각해 보자, 걸음 이라는 텍스트가 포함되어 있는 row 를 찾으려면 아래처럼 쿼리를 하게 되겠다. 이런 쿼리를 사용하는 순간 full text searching 을 하기 때문에 가급적이면 사용을 지양해야할 쿼리이긴 하지만, 어쩔 수 없이 사용한다고 가정하자.\nSELECT * FROM poems WHERE contents LIKE \u0026#39;%걸음%\u0026#39; id | contents 1 \u0026#34;진달래꽃 나 보기가 역겨워 가실 때에는 말없이 고이 보내 드리우리다 영변에 약산 진달래꽃 아름 따다 가실 길에 뿌리우리다 가시는 걸음 걸음 놓인 그 꽃을 사뿐히 즈려밟고 가시옵소서 나 보기가 역겨워 가실 때에는 죽어도 아니 눈물 흘리우리다\u0026#34; ElasticSearch 의 경우 RDBMS 에서 처럼 full text searching 이 가능할것으로 기대하고, wildcard 쿼리를 사용해보자.\ncurl -s http://my-elastic-cluster-host:9200/index-name/_search?pretty -XPOST -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;contents\u0026#34;: \u0026#34;*걸음*\u0026#34; } } } } }\u0026#39; 결과는 아래와 같은 형태가 된다.\n{ \u0026#34;took\u0026#34;: 230, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;index-name\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;poems\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1-1\u0026#34;, \u0026#34;_score\u0026#34;: null, \u0026#34;_source\u0026#34;: { \u0026#34;contents\u0026#34;: \u0026#34;진달래꽃 나 보기가 역겨워 가실 때에는 말없이 고이 보내 드리우리다 영변에 약산 진달래꽃 아름 따다 가실 길에 뿌리우리다 가시는 걸음 걸음 놓인 그 꽃을 사뿐히 즈려밟고 가시옵소서 나 보기가 역겨워 가실 때에는 죽어도 아니 눈물 흘리우리다\u0026#34; } } ] } } 원하는대로 진달래꽃을 잘 찾았다. 그럼 이번에는 키워드를 바꿔서 나 보기가 역겨워라는 문장에서 기가 라는 단어를 기준으로 검색해 보자.\nRDBMS SELECT * FROM poems WHERE contents LIKE \u0026#39;%기가%\u0026#39; 여전히 아래와 같이 잘 찾는다.\nid | contents 1 \u0026#34;진달래꽃 나 보기가 역겨워 가실 때에는 말없이 고이 보내 드리우리다 영변에 약산 진달래꽃 아름 따다 가실 길에 뿌리우리다 가시는 걸음 걸음 놓인 그 꽃을 사뿐히 즈려밟고 가시옵소서 나 보기가 역겨워 가실 때에는 죽어도 아니 눈물 흘리우리다\u0026#34; ElasticSearch curl -s http://my-elastic-cluster-host:9200/index-name/_search?pretty -XPOST -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: { \u0026#34;wildcard\u0026#34;: { \u0026#34;contents\u0026#34;: \u0026#34;*기가*\u0026#34; } } } } }\u0026#39; 그런데 이번에는 원하는 결과를 찾을 수가 없다.\n{ \u0026#34;took\u0026#34;: 230, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 0, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } 왜 그럴까? 이 차이는 바로 wildcard 쿼리가 term level query 이기 때문이다. ElasticSearch는 실제로 저장된 document 의 원문을 검사하는 것이 아니라, inverted index 의 목록 중에서 term(token) 중에 쿼리에서 질의한 keyword를 찾기 때문이다. (이건 아주 기본적인 사항이지만, 우리는 이를 금방 까먹는다\u0026hellip;)\n그럼 진달래꽃은 어떤 token 으로 구성되어 있는지 확인해 보자.\ncurl -s http://my-elastic-cluster-host:9200/index-name/_analyze?pretty -XPOST -d \u0026#39; \u0026#34;text\u0026#34;: \u0026#34;진달래꽃 나 보기가 역겨워 가실 때에는 말없이 고이 보내 드리우리다 영변에 약산 진달래꽃 아름 따다 가실 길에 뿌리우리다 가시는 걸음 걸음 놓인 그 꽃을 사뿐히 즈려밟고 가시옵소서 나 보기가 역겨워 가실 때에는 죽어도 아니 눈물 흘리우리다\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34; }\u0026#39; token 의 결과를 보면 \u0026ldquo;*기가*\u0026rdquo; 와 매칭되는 token 이 존재하지 않는다. 보기가 라는 단어는 다음과 같이 보기 의 단어(word)로 tokenize 되기 때문이다. (어떤 analyzer 를 사용하느냐는 여기서 언급하지 않겠다.)\n{ \u0026#34;tokens\u0026#34;: [ ....... { \u0026#34;token\u0026#34;: \u0026#34;보기\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;VX\u0026#34;, \u0026#34;position\u0026#34;: 7 }, ....... ] } 따라서 처음에 생각한 걸음 이라는 단어는 검색이 되지만, 기가 라는 단어는 검색이 되지 않는다.\n그렇다면 이를 어떻게 해결할 수 있을까? 이런 경우 ngram을 사용 하면 된다.\nmapping 에 nested 구조로 ngram 이라는 필드를 추가해보자\nmappings ...... \u0026#34;mappings\u0026#34;: { \u0026#34;poems\u0026#34;: { // type name \u0026#34;properties\u0026#34;: { \u0026#34;contents\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;ngram\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_customer_ngram_analyzer\u0026#34; } } } .... } } .... settings ...... \u0026#34;analyzer\u0026#34;: { \u0026#34;my_customer_ngram_analyzer\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;my_customer_ngram_tokenizer\u0026#34; } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;my_customer_ngram_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;max_gram\u0026#34;: \u0026#34;5\u0026#34; } } ...... 이제 새롭게 document를 추가하면 contents.ngram 이라는 필드에 token 이 다르게 나온다. 다시 analyzer 결과를 확인해 보자.\ncurl -s http://my-elastic-cluster-host:9200/index-name/_analyze?pretty -XPOST -d \u0026#39; \u0026#34;text\u0026#34;: \u0026#34;진달래꽃 나 보기가 역겨워 가실 때에는 말없이 고이 보내 드리우리다 영변에 약산 진달래꽃 아름 따다 가실 길에 뿌리우리다 가시는 걸음 걸음 놓인 그 꽃을 사뿐히 즈려밟고 가시옵소서 나 보기가 역겨워 가실 때에는 죽어도 아니 눈물 흘리우리다\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_customer_ngram_analyzer\u0026#34; }\u0026#39; 이번에는 단어를 하나하나 쪼개서 길이 2 ~ 길이 5사이의 token 으로 나뉘어진다. 보기가 라는 단어는 보기, 보기가, 기가 라는 세개의 토큰으로 나뉘어 졌다.\n{ \u0026#34;tokens\u0026#34;: [ ...... { \u0026#34;token\u0026#34;: \u0026#34;보기\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 2, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 7 }, { \u0026#34;token\u0026#34;: \u0026#34;보기가\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 7 }, { \u0026#34;token\u0026#34;: \u0026#34;기가\u0026#34;, \u0026#34;start_offset\u0026#34;: 1, \u0026#34;end_offset\u0026#34;: 3, \u0026#34;type\u0026#34;: \u0026#34;word\u0026#34;, \u0026#34;position\u0026#34;: 8 }, ...... } 이렇게 ngram 을 설정하면 min, max 의 설정에 따라서 나뉘는 기준이 달라진다. 사랑합니다 라는 단어를 ngram 의 length 에 따라서 나뉘어 보면 다음과 같다.\nLength 1 (unigram): [ 사, 랑, 합, 니, 다 ] Length 2 (bigram): [ 사랑, 랑합, 합니, 니다 ] Length 3 (trigram): [ 사랑합, 랑합니, 합니다 ] Length 4 (four-gram): [ 사랑합니, 랑합니 ] Length 5 (five-gram): [ 사랑합니다 ] 보기가 라는 단어는 단어의 길이가 3 이기 때문에 총 6개의 token 으로 나뉘어 졌다. 이렇게 단어의 일부분을 ngram 으로 잘라내서 tokenize 해서 매칭 시키는 방식을 partial matching 이라고 한다.\n이렇게 ngram 을 사용하도록 변경하였으니, 쿼리를 수정하자. wildcard 쿼리는 빼고, term 쿼리를 사용하자.\ncurl -s http://my-elastic-cluster-host:9200/index-name/_search?pretty -XPOST -d \u0026#39; { \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34; : [ {\u0026#34;term\u0026#34; : { \u0026#34;contents\u0026#34; : \u0026#34;기가\u0026#34; } }, {\u0026#34;match_phrase\u0026#34; : { \u0026#34;contents.ngram\u0026#34; : \u0026#34;기가\u0026#34; } } ], \u0026#34;minimum_should_match\u0026#34; : 1 } } }\u0026#39; 위의 쿼리는 contents 의 token 과 contents.ngram 의 token을 찾게 된다. 쿼리 결과는 아래와 같이 정상적으로 원하는 문서를 찾을 수 있다.\n{ \u0026#34;took\u0026#34;: 8, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;index-name\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;poems\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1-1\u0026#34;, \u0026#34;_score\u0026#34;: null, \u0026#34;_source\u0026#34;: { \u0026#34;contents\u0026#34;: \u0026#34;진달래꽃 나 보기가 역겨워 가실 때에는 말없이 고이 보내 드리우리다 영변에 약산 진달래꽃 아름 따다 가실 길에 뿌리우리다 가시는 걸음 걸음 놓인 그 꽃을 사뿐히 즈려밟고 가시옵소서 나 보기가 역겨워 가실 때에는 죽어도 아니 눈물 흘리우리다\u0026#34; } } ] } } 와일드카드 쿼리를 사용했을 때는 결과가 230 ms 였다면, ngram 을 사용하여 조정한 뒤에는 8 ms 가 걸렸다.\n로그 분석의 경우 nginx access log 를 ElasticSearch 를 저장하고 조회 \u0026amp; 분석하는 경우라면 URL 에 숫자와 알파벳이 조합된, 특수한 ID 값 같은 식별자가 붙는 경우가 많다. 예를 들면 ex) ?user=3FGAZS1032\u0026amp;key2=ssssss 라고 가정해보자.. 이 경우 user 가 3FGAZS1032 인 경우를 찾는다면 문제 없겠지만, user 값에 3FGAZS가 들어가는 경우를 찾는다면, 원하는 결과를 얻기가 어렵다. 마찬가지로 이 경우도 ngram 을 통한 tokenize 를 구성했다면 partial mathching 이 가능해져 원하는 문서를 찾을 수 있다.\n3FGAZS1032 라는 단어가 analyze 가 어떻게 되는지 살펴보자\ncurl -s http://my-log-es-cluster-host:9200/log-2018.07.14/_analyze?pretty -XPOST -d \u0026#39; { \u0026#34;text\u0026#34;: \u0026#34;3FGAZS1032\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;log_analyzer\u0026#34; } token 이 하나만 나온다.\n{ \u0026#34;tokens\u0026#34;: [ { \u0026#34;token\u0026#34;: \u0026#34;3FGAZS1032\u0026#34;, \u0026#34;start_offset\u0026#34;: 0, \u0026#34;end_offset\u0026#34;: 10, \u0026#34;type\u0026#34;: \u0026#34;\u0026lt;ALPHANUM\u0026gt;\u0026#34;, \u0026#34;position\u0026#34;: 0 } ] } 이 상황에서 \\*3FGAZS\\* 라는 와일드카드 쿼리를 사용할 수 있지만 응답속도가 느리다. 그렇지만 ngram 을 구성하면 훨씬 빠르다.\ncurl -s http://my-log-es-cluster-host:9200/log-2018.07.14/_search?pretty -XPOST -d \u0026#39; { \u0026#34;_source\u0026#34;: [\u0026#34;user\u0026#34;,\u0026#34;path\u0026#34;], \u0026#34;query\u0026#34; : { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ \u0026#34;term\u0026#34;: { \u0026#34;user.ngram\u0026#34;: \u0026#34;3FGAZS\u0026#34; }, \u0026#34;term\u0026#34;: { \u0026#34;path.keyword\u0026#34;: \u0026#34;/user/playground\u0026#34; } ] } } }\u0026#39; 결과가 1ms 가 나왔다.\n{ \u0026#34;took\u0026#34;: 1, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 1, \u0026#34;successful\u0026#34;: 1, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 39, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;log-2018.07.14\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;log\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;3078923\u0026#34;, \u0026#34;_score\u0026#34;: null, \u0026#34;_source\u0026#34;: { \u0026#34;user\u0026#34;: \u0026#34;3FGAZS1032\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/user/playground\u0026#34; } } ...... ] } } wildcard 대신 ngram 을 도입하여 얻은 효과 먼저 token 을 기준으로 wildcard 에서 못찾는 document를 찾을 수 있다. 반응 속도가 엄청 빨라진다. (1ms 라니..) 대신 token 의 갯수가 많아지기 때문에 inverted index 사이즈가 늘어난다. 결론 ElasticSearch 쿼리를 작성하면서 wildcard 를 사용할 때 RDBMS 의 like \u0026quot;%keyword%\u0026quot; 와 같이 사용하고자 한다면 ngram 을 사용한 tokenize 를 고려해보는게 좋다. wildcard 는 term(token) level query 이고, 이 차이점을 잘 알아 둘 필요가 있다. 더불어 ngram 과 match_phrase 쿼리를 사용하면 긴 단어나 문장을 검색 키워드로 조회하더라도 원하는 문서를 잘 찾을 수 있다(콘텐츠 검색의 경우).\n덧 붙이는 말 테스트와 더불어 강진우님님께서 관련 내용을 함께 고민해주고 도움을 주셨다.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/match_phrase/",
	"title": "Match_phrase",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/ngram/",
	"title": "Ngram",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/partial-matching/",
	"title": "Partial Matching",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/wildcard/",
	"title": "Wildcard",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/07/07/elasticsearch-reindex/",
	"title": "ElasticSearch 에서 reindex 을 활용하는 방법",
	"tags": ["ElasticSearch", "reindex"],
	"description": "",
	"type": "post",
	"contents": "그럴 것 같지 않지만, ElasticSearch 에서는 reindex를 수행할 일이 많이 발생한다. reindex를 실행할때 사용할 수 있는 옵션을 확인해 보았다.\nElasticSearch 를 사용하면서\n인덱스의 mapping 을 수정해야 하거나, 이전 버전 (2.X, 5.X - Rolling upgrade 를 사용한 버전 업그레이드를 지원하지 않는 ES) 에서 최신 버전으로 마이그레이션 해야 할 때, index의 이름 변경이 필요할 때 (rename) - 이경우는 주로 alias 를 잘 활용하지 않을 때 발생한다. reindex를 실행할 일은 생각보다 자주 발생한다.\nreindex 란 말 그대로 인덱싱을 새로 한다는 의미이다. 기존에 존재하는 index 에서 새로운 index로 데이터를 새롭게 색인하는 것을 의미한다.\n나의 경우에는 쿼리를 추가하는 과정에서 과정에서 aggregation 을 수행해야 하는데, mapping이 잘못 설정 되어 있다던가, 이전 데이터의 마이그레이션 과정에서 source 필드 일부를 제외해야 하거나 하는 등의 케이스에서 reindex 가 필요했다.\n기본적인 reindex 실행하기 다음은 가장 기본적인 reindex 실행방법이다. source 에 기존의 index, dest 에 새롭게 인덱싱할 index 이름을 지정한다.\ncurl -H \u0026#39;Content-Type: application/json\u0026#39; -XPOST http://my-elasticsearch-host:9200/_reindex -d `{ \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;old-index-name\u0026#34; }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;new-index-name\u0026#34; } }` 이렇게 하면 데이터의 양에 따라서 시간이 걸리긴 하지만 정상적으로 완료되면 아래와 같은 응답을 확인할 수 있다.\n{ \u0026#34;took\u0026#34; : 147, // 소요된 시간 (ms) \u0026#34;timed_out\u0026#34;: false, // timeout 이 발생했는지 여부 \u0026#34;created\u0026#34;: 120, // 새롭게 생성된 document 의 갯수 \u0026#34;updated\u0026#34;: 0, \u0026#34;deleted\u0026#34;: 0, \u0026#34;batches\u0026#34;: 1, \u0026#34;version_conflicts\u0026#34;: 0, \u0026#34;noops\u0026#34;: 0, \u0026#34;retries\u0026#34;: { \u0026#34;bulk\u0026#34;: 0, \u0026#34;search\u0026#34;: 0 }, \u0026#34;throttled_millis\u0026#34;: 0, \u0026#34;requests_per_second\u0026#34;: -1.0, \u0026#34;throttled_until_millis\u0026#34;: 0, \u0026#34;total\u0026#34;: 120, // 전체 처리된 document 갯수 \u0026#34;failures\u0026#34; : [ ] } 다른 ES 클러스터의 데이터를 reindex 하기 경우에 따라서는 신규 클러스터를 운영해야 해서, 크러스터 내부에서 새롭게 reindex 하는게 아니라, remote 의 ES 클러스터의 데이터를 reindex 해야 될 수도 있다. 이럴 때에는 \u0026ldquo;source\u0026rdquo; 에 \u0026ldquo;remote\u0026rdquo; 를 지정하면 된다.\ncurl -H \u0026#39;Content-Type: application/json\u0026#39; -XPOST http://my-elasticsearch-host:9200/_reindex -d `{ \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;old-index-name\u0026#34;, \u0026#34;remote\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;http://my-old-es-cluster-host:9200\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;pass\u0026#34; }, }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;new-index-name\u0026#34; } }` \u0026ldquo;remote\u0026rdquo; 의 index 를 reindex 하고자 한다면, 기존 클러스터의 host 가 whitelist 에 추가되어 있어야 한다. (elasticsearch.yml) reindex.remote.whitelist: \u0026#34;otherhost:9200, another:9200, 127.0.10.*:9200, localhost:*\u0026#34; 기존 index 의 일부 필드만 reindex 하는 방법 일부 필드만 reindex 하거나(includes), 불필요한 필드는 제외시킬 수도 있다. (excludes)\ncurl -H \u0026#39;Content-Type: application/json\u0026#39; -XPOST http://my-elasticsearch-host:9200/_reindex -d `{ \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;old-index-name\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;includes\u0026#34;: [\u0026#34;field1\u0026#34;, \u0026#34;field2\u0026#34;] } }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;new-index-name\u0026#34; } }` 전체 데이터가 아니라, 일부 데이터만 reindex 하는 방법 쿼리를 질의해서 기존 index 에서 일부 데이터만을 reindex 할 수도 있다. 일반적인 쿼리와 마찬가지로 size 를 지정할 수도 있다.\ncurl -H \u0026#39;Content-Type: application/json\u0026#39; -XPOST http://my-elasticsearch-host:9200/_reindex -d `{ \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;old-index-name\u0026#34;, \u0026#34;_source\u0026#34;: { \u0026#34;includes\u0026#34;: [\u0026#34;field1\u0026#34;, \u0026#34;field2\u0026#34;] }, \u0026#34;size\u0026#34;: 10000, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: [ { \u0026#34;match_phrase\u0026#34;: { \u0026#34;field1\u0026#34;: \u0026#34;search_keyword\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;datetime\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2018-07-05T05:05:54.000Z\u0026#34; } } } ] } } }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;new-index-name\u0026#34; } }` reindex 를 실행하면 한꺼번에 많은 데이터를 새로 인덱싱 해야 하기 때문에 ES 클러스터의 부하가 높아진다. 이미 부하가 높은 클러스터라면, 성능에 여유가 있는지 잘 살펴서 실행하도록 하자. 그리고 이왕이면 언제 필요할지 모르는 reindex 의 부담을 최소화 하기 위해서라도 index 는 잘 쪼개서 alias 로 관리하는 습관을 들이도록 하자.\n참고 https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html "
},
{
	"permalink": "https://findstar.pe.kr/tags/reindex/",
	"title": "Reindex",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/06/29/elasticsearch-template/",
	"title": "ElasticSearch 에서 template 을 활용하는 방법",
	"tags": ["ElasticSearch", "template", "mapping"],
	"description": "",
	"type": "post",
	"contents": "ElasticSearch를 로그 분석용으로 사용할 때 인덱스의 mapping은 template을 사용해서 생성되도록 설정하면 편리하다.\n로그 분석용으로 ElasticSearch 를 사용하면 주로 인덱스는 daily/weekly/monthly 중에 하나로 구성하는게 일반적이다. 동일한 유형의 인덱스를 계속해서 생성하게 되는데, 예를 들자면 nginx log의 인덱스를 다음과 같이 생성한다고 해보자.\nnginx-access-log-2018.06.20 nginx-access-log-2018.06.21 nginx-access-log-2018.06.22 nginx-access-log-2018.06.23 \u0026hellip; 위와 같은 인덱스를 자동으로 생성할 때 사용 할 mapping을 미리 지정할 수 있는데 이 기능이 바로 template 이다.\n템플릿 생성하기 다음은 nginx-access-log 라는 템플릿을 생성하는 것으로, 인덱스가 nginx-access-* 에 해당하는 패턴이라면, 지정된 mapping 으로 인덱스를 생성한다.\n## nginx-access-log 템플릿 생성 curl -X \u0026#34;PUT\u0026#34; \u0026#34;http://my-elasticsearch-server-host:9200/_template/nginx-access-log\u0026#34; \\ -H \u0026#39;Content-Type: application/json; charset=utf-8\u0026#39; \\ -d $\u0026#39;{ \u0026#34;index_patterns\u0026#34;: [ \u0026#34;nginx-access-*\u0026#34; ], \u0026#34;mappings\u0026#34;: { \u0026#34;log\u0026#34;: { // type name \u0026#34;properties\u0026#34;: { \u0026#34;ip\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;uri\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;datetime\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; } } } ........ } }\u0026#39; setting 설정하기 mapping 뿐만 아니라 setting 도 지정할 수 있다.\ncurl -X \u0026#34;PUT\u0026#34; \u0026#34;http://my-elasticsearch-server-host:9200/_template/new-template-name\u0026#34; \\ -d $\u0026#39;{ \u0026#34;index_patterns\u0026#34;: [ \u0026#34;nginx-access-*\u0026#34; ], \u0026#34;settings\u0026#34;: { // setting \u0026#34;number_of_shards\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;type1\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;host_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;EEE MMM dd HH:mm:ss Z YYYY\u0026#34; } } } } }\u0026#39; 여러개의 패턴 지정하기 인덱스 패턴을 여러개를 지정하고자 할 때는 배열 안에 계속 추가하면 된다.\ncurl -X \u0026#34;PUT\u0026#34; \u0026#34;http://my-elasticsearch-server-host:9200/_template/new-template-name\u0026#34; \\ -d $\u0026#39;{ \u0026#34;index_patterns\u0026#34;: [ \u0026#34;pattern1*\u0026#34;, \u0026#34;pattern2*\u0026#34;, \u0026#34;pattern3*\u0026#34;, \u0026#34;pattern4*\u0026#34; ], \u0026#34;settings\u0026#34;: { // setting \u0026#34;number_of_shards\u0026#34;: 1 }, \u0026#34;mappings\u0026#34;: { \u0026#34;type1\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;host_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;created_at\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;EEE MMM dd HH:mm:ss Z YYYY\u0026#34; } } } } }\u0026#39; 참고 https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html "
},
{
	"permalink": "https://findstar.pe.kr/tags/mapping/",
	"title": "Mapping",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/template/",
	"title": "Template",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/jekyll-to-hugo/",
	"title": "Jekyll to Hugo",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/06/25/jekyll-to-hugo-thoughts/",
	"title": "jekyll 에서 hugo 로 전환한 소감",
	"tags": ["hugo", "jekyll to hugo", "migration"],
	"description": "",
	"type": "post",
	"contents": "사용하던 github pages 기반 툴을 jekyll 에서 hugo 로 전환해보았다. 아래는 그 과정에서 느낀 점을 정리해보았다.\n전환하면서 느낀 점 hugo 전환한 이유 jekyll 에서 hugo 로 전환한 이유는 아래와 같다.\nhugo 가 반응속도가 훨~~씬 빠르다는 것. (jekyll 은 2 s, hugo 는 70ms ) theme 를 커스터마이징 하기가 손쉽다는 것. 매뉴얼이 잘 구성되어 있고, 심지어 영상으로 소개해준다는 것. 그리고 무엇보다 jekyll-paginate v2 를 쓰다가 github page 에서 동작을 안해서.. (사실 이게 제일 크다) hugo 에서 불편한 점 jekyll 에서는 plugin 으로 구현 가능한 것들이 직접 핸들링 해줘야 하는 부분들이 있었다. github page 가 native 로 연결되지 않아서 별도로 관리해주어야 한다는 점. 템플릿 문법이 상대적으로 낯설어서 적응하는데 애를 먹었다. (RTFM) 전환 작업하면서 알아야 했던 것들 theme 는 별도로 구성하는게 편하다\n매뉴얼에는 별도의 theme 프로젝트를 git submodule 로 관리하는걸 안내하고 있다. 내 경우는 아예 새로 만들었다.\n기본 템플릿\nhugo 에는 list 템플릿, single 템플릿 두 가지가 기본 템플릿이다. 테마를 구성한다면 요 차이를 명확하게 구분하자.\npagination 은 jekyll 이 더 편해보인다.\nseo, tag archive 같은 기능은 별도로 custom page 를 만들어서 쓰는게 편했다.\nshortcode 라는 별도의 custom 단축기능을 만들 수 있다. 내 경우는 CSS class 와 맞추기 위해서 아래처럼 구성해서 썼다.\n\u0026lt;figure class=\u0026#34;full-width caption\u0026#34; {{ with .Get \u0026#34;border\u0026#34; }} style=\u0026#34;border: 1px solid #ededed;\u0026#34; {{ end }}\u0026gt; \u0026lt;img src=\u0026#34;{{ .Get \u0026#34;src\u0026#34; }}\u0026#34; alt=\u0026#34;{{ .Get \u0026#34;alt\u0026#34; }}\u0026#34;/\u0026gt; {{ with .Get \u0026#34;caption\u0026#34; }} \u0026lt;figcaption class=\u0026#34;caption-text\u0026#34;\u0026gt;{{ . }}\u0026lt;/figcaption\u0026gt; {{ end }} \u0026lt;/figure\u0026gt; related post 를 구성하는 건 옵션이 많으니 입맛에 맞게 잘 맞춰야 한다.\n전체적인 소감 hugo는 github page 를 직접 지원하지 않아서 별도의 build \u0026amp; push 를 해줘야 하지만 이건 크게 문제되지 않는 것 같다. 오히려 빨라진 반응 속도가 더 쾌적한 느낌이 들고, 빠른 업데이트와 함께 매뉴얼이 잘 마련되어 있다는 점이 긍정적이라고 느껴진다. 찔끔찔끔 마이그레이션 하느라 시간은 2주 정도 걸렸는데 하고나니 잘 했다는 느낌이다. 이제는 테마는 그만좀 고치고 글을 좀 더 써야 할텐데..\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/migration/",
	"title": "Migration",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/limit_req_zone/",
	"title": "Limit_req_zone",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/nginx/",
	"title": "Nginx",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/06/24/nginx-rate-limiting/",
	"title": "nginx reverse proxy로 동일 IP 중복 요청 제한",
	"tags": ["nginx", "rate litmiting", "limit_req_zone"],
	"description": "",
	"type": "post",
	"contents": "Nginx 로 reverse proxy를 구성할 때 과도한 요청에 대한 제한을 두기 위해서 limit_req 모듈을 적용해보았다.\nnginx에서 지원하는 limit_req 모듈을 사용하면, 동일한 IP 에서 과도한 요청이 유입될 때, 이를 제한할 수 있다.\nLimit req 설정 limit_req_zone $binary_remote_addr zone=depend_rate_limit:10m rate=10r/s; # binary_remote_addr : 클라이언트의 IP를 기준으로 제한하겠다는 의미. # zone name : depend_rate_limit # share memory assign : 10M # rate : 10 request / second 하나씩 살펴보자면, limit_req_zone 이라는 것은 요청-request를 확인하고 이를 제한하기 위해서 특정한 영역(zone)을 선언한다는 의미이다. $binary_remote_addr 은 nginx 에서 기본적으로 제공하는 내장 변수로, 클라이언트의 IP를 기준으로 제한을 하겠다는 의미이다. zone=depend_dos 라는 것은 zone 의 이름을 설정하는 것으로, zone의 이름은 임의로 변경이 가능하다 30m 이라는 것은 zone에서 활용가능한 share memory size 로. 10M 정도면 충분하다. rate 는 요청-request 의 비율로, 여기에서는 초당 10개 이상의 요청-request 이 유입되면 제한을 하겠다는 의미이다. (r/s, r/m 가능) 이제 다음과 같이 적용하면 된다. http { limit_req_zone $binary_remote_addr zone=depend_rate_limit:10m rate=10r/s; ... server { ... location / { ... limit_req zone=depend_rate_limit burst=5; } burst 를 적용한 것은 rate(여기서는 10r/s) 이상의 request-요청에 대해서 5개 까지는 queue에 보관하도록 하고, 그 이상은 에러를 반환하게 한다는 의미이다.\n에러 반환 기본적으로 정의한 rate 를 넘어서는 request-요청에 대해서는 503 에러를 반환하는데. 이를 변경할 수 있다. 429 status code는 Too Many Requests를 의미한다. 참고\nlimit_req zone=depend_rate_limit burst=5 nodelay; limit_req_status 429; log level 설정 제한에 걸린 request-요청을 기록할 때 log level 을 설정할 수 있다.\nlimit_req zone=depend_rate_limit burst=5 nodelay; limit_req_status 429; limit_req_log_level error; 제한에서 제외할 IP 설정 실제로 limit_req 모듈을 적용하기 전에, 내부 네트워크 대역이나, 특정 IP에 대해서는 제한을 두지 않을 수 있다.\ngeo $apply_limit { default $binary_remote_addr; 10.10.0.0/16 \u0026#39;\u0026#39;; # 내부 네트워크 대역 10.10.*.* 은 access limit 사용안함 211.33.188.246 \u0026#39;\u0026#39;; # 외부의 특정 IP 211.33.188.246 는 access limit 사용안함 } ... ... limit_req_zone $apply_limit zone=depend_rate_limit:10m rate=10r/s; ... geo 모듈을 사용해서 client ip 를 확인해서 $apply_limit 이라는 변수를 새롭게 할당했다. 10.10.. 대역 이거나, (내부 네트워크 대역인 경우), 외부의 특정 211.33.188.246 인 경우에는 빈값이 지정된다.\n이렇게 하고 나서 limit_req_zone 에 정의한 $apply_limit 변수를 사용하면, 예외로한 IP 에 대해서는 접속제한이 동작하지 않는다.\n참고 http://nginx.org/en/docs/http/ngx_http_limit_req_module.html https://sarc.io/index.php/nginx/99-2014-03-18-14-30-00 https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429 "
},
{
	"permalink": "https://findstar.pe.kr/tags/rate-litmiting/",
	"title": "Rate Litmiting",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/06/03/change-alias-on-elasticsearch-index/",
	"title": "ElasticSearch 에서 Index Alias 변경하기",
	"tags": ["ElasticSearch", "index alias"],
	"description": "",
	"type": "post",
	"contents": "6월 3일 (일) TIL ElasticSearch 에서 index 의 alias 를 변경하는 방법을 알아보았다.\nAlias 확인 먼저 ElasticSearch 클러스터에 /_alias 로 접속하면 현재 생성된 인덱스 들과 연결된 alias 를 확인할 수 있다.\n{ \u0026#34;activity-log-index\u0026#34;: { \u0026#34;aliases\u0026#34;: { } }, \u0026#34;my-contents-index-v1\u0026#34;: { \u0026#34;aliases\u0026#34;: { \u0026#34;contents\u0026#34;: { } } }, \u0026#34;my-contents-index-v2\u0026#34;: { \u0026#34;aliases\u0026#34;: { } } } 현재 my-contents-index-v1 이라는 인덱스에 contents 라는 alias 가 부여되어 있다. 이를 my-contents-index-v2 으로 교체해보겠다.\ncurl -XPOST \u0026#39;http://my-elasticsearch-host:9200/_aliases?pretty\u0026#39; -d\u0026#39; { \u0026#34;actions\u0026#34; : [ { \u0026#34;remove\u0026#34; : { \u0026#34;index\u0026#34; : \u0026#34;my-contents-index-v1\u0026#34;, \u0026#34;alias\u0026#34; : \u0026#34;contents\u0026#34; } }, { \u0026#34;add\u0026#34; : { \u0026#34;index\u0026#34; : \u0026#34;my-contents-index-v2\u0026#34;, \u0026#34;alias\u0026#34; : \u0026#34;contents\u0026#34; } } ] }\u0026#39; 한번의 쿼리를 통해서 my-contents-index-v1 에서는 alias 가 삭제되고 my-contents-index-v2 에는 alias 가 추가되었다.\n이후 다시 /_alias 로 확인해보면 변경된 결과를 확인가능하다\n{ \u0026#34;activity-log-index\u0026#34;: { \u0026#34;aliases\u0026#34;: { } }, \u0026#34;my-contents-index-v1\u0026#34;: { \u0026#34;aliases\u0026#34;: { } }, \u0026#34;my-contents-index-v2\u0026#34;: { \u0026#34;aliases\u0026#34;: { \u0026#34;contents\u0026#34;: { } } } } "
},
{
	"permalink": "https://findstar.pe.kr/tags/index-alias/",
	"title": "Index Alias",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/til/",
	"title": "Today I Learned",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": "Today I Learned\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/filebeat/",
	"title": "Filebeat",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/logstash/",
	"title": "Logstash",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/05/28/install-and_configuration-filebeat-logstash/",
	"title": "logstash와 filebeat 설정하기",
	"tags": ["ElasticSearch", "logstash", "filebeat", "install"],
	"description": "",
	"type": "post",
	"contents": "Elasticsearch의 버전업을 지원하기 위해서 logstash 와 filebeat를 새롭게 설치하고 설정해보았다. 진행한 작업 내용을 정리 해보았다.\n서버 구성 먼저 서버 구성은 다음과 같다.\nElasticSearch 클러스터가 별도로 존재 LogStash 1대 다수의 웹서버에 각각 Filebeat 를 설치. filebeat filebeat 설치 filebeat 를 yum을 통해서 설치\nsudo rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch /etc/yum.repods.d/elastic.repo 추가\n[elastic-6.x] name=Elastic repository for 6.x packages baseurl=https://artifacts.elastic.co/packages/6.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum 을 통해서 설치\nyum install filebeat filebeat 설정 filebeat 에서는 json 형태로 logstash 에게 데이터를 전달하고, 이때 message 필드에 수집한 로그 파일의 데이터가 담겨진다. 수집하려는 log file 의 유형에 따라서 community beats를 사용할 수도 있지만, 나의 경우에는 Custom pattern의 로그 파일을 수집할 예정이라 logstash에서 pasring 하는 형태를 선택했다. (직접 beat를 만들 수도 있다.링크, 언어는 golang.)\n/etc/filebeat/filebeat.yml 파일 변경 filebeat.prospectors: # Each - is a prospector. Most options can be set at the prospector level, so # you can use different prospectors for various configurations. # Below are the prospector specific configurations. - type: log enabled: true paths: - /var/log/my_log_path/*.log fields: index_name: \"my_custom_file_index_name\" #----------Elasticsearch output--------------- 주석처리 # (beats 에서 바로 ES 로 데이터 전달하지 않음) #----------Logstash output ------------------- 주석해제 # (beats 에서 logstash 로 데이터 전달) output.logstash: # The Logstash hosts hosts: [\"my-logstash-server-host:5044\"] ... 수집할 log를 정의하고 fields 를 추가하였다. 추가한 fields는 logstash에 변수로 전달된다. 필요한 경우 여러개를 추가 할 수 있다. filebeat 실행 service filebeat start filebeat 데몬 로그 확인 tail -f /var/log/filebeat/filebeat filebeat 에서 file 을 다시 읽어 들어야 하는 경우 filebeat 는 파일을 어디까지 읽어 들였는지 메타 정보를 /var/lib/filebeat/registry 파일에 기록하고 있다.\n따라서 이 메타 정보를 강제로 reset 하려면 다음과 같이 하면 된다. (경험적으로 설치가 잘 되었는지 확인될 때까지는 메타 정보를 여러번 reset 해야했다.)\necho \u0026#34;[]\u0026#34; \u0026gt; /var/lib/filebeat/registry logstash logstash 설치 logstash 는 Java 가 준비된 환경에서 Download Link에서 다운받아서 설치했다. logstash plugin 설치 사용할 필터중에서 bundle 로 제공되지 않는 alter 설치 bin/logstash-plugin install logstash-filter-alter Validating logstash-filter-alter Installing logstash-filter-alter Installation successful logstash 에 filter 설정 grok, mutate, json, geoip, alter 필터를 설정했고 filebeat 에서 fields 로 넘겨받은 index_name을 사용했다.\ndate 필터는 기준 시각을 filebeat 에 의해서 파싱된 시각을 사용하지 않고, log 에 기록된 시각으로 지정하도록 한다.\ninput { beats { port =\u0026gt; \u0026#34;5044\u0026#34; } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{TIMESTAMP_ISO8601:time}\\t%{DATA:tag}\\t{ % {DATA:data} }\u0026#34; } } mutate { add_field =\u0026gt; { \u0026#34;json_data\u0026#34; =\u0026gt; \u0026#34;{ %{data}}\u0026#34; } } json { source =\u0026gt; \u0026#34;json_data\u0026#34; } if [fields][index_name] == \u0026#34;qna5\u0026#34; { geoip { source =\u0026gt; \u0026#34;remote_addr\u0026#34; } } alter { remove_field =\u0026gt; [ \u0026#34;data\u0026#34;, \u0026#34;json_data\u0026#34; ] } date { match =\u0026gt; [ \u0026#34;time\u0026#34;, \u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ssZZ\u0026#34; ] } } output { elasticsearch { hosts =\u0026gt; \u0026#34;my-elastcisearch-server-host:9200\u0026#34; index =\u0026gt; \u0026#34;%{[fields][index_name]}-%{+YYYY.MM.dd}\u0026#34; document_type =\u0026gt; \u0026#34;%{[fields][index_name]}\u0026#34; } } # debug 를 위해서는 아래의 출력으로 조정. #output { # stdout { codec =\u0026gt; json } #} logstash 실행 nohup logstash/bin/logstash -f logstash.conf \u0026gt; /var/log/logstash/logstash.out elasticsearch 확인 인덱스와 데이터가 제대로 들어오는지 확인하면 된다. 참고 https://www.elastic.co/guide/en/beats/filebeat/current/setup-repositories.html http://yongho1037.tistory.com/709 http://dgkim5360.tistory.com/entry/managing-multiple-heterogeneous-inputs-on-filebeat-and-logstash http://blog.plura.io/?p=3363 https://www.elastic.co/guide/en/beats/filebeat/current/configuration-filebeat-options.html#_literal_tags_literal https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns (grok 패턴 모음) https://grokdebug.herokuapp.com/ (grok 패턴 사용시 테스트) https://www.elastic.co/blog/geoip-in-the-elastic-stack (geoip 사용) "
},
{
	"permalink": "https://findstar.pe.kr/2018/05/13/upload-file-on-curl/",
	"title": "CURL 에서 파일 업로드 하기",
	"tags": ["curl", "file upload"],
	"description": "",
	"type": "post",
	"contents": "5월 13일 (수) TIL 작성한 API를 테스트 하기 위해서 Postman 이나 Paw를 주로 사용하는데, CURL을 사용해서 CLI에서 테스트해야 되는 경우도 종종 있다. 새롭게 API를 테스트 하던 중 CURL을 사용해서 파일 업로드 API를 테스트 하는 방법을 확인해 봤다.\n기본 사용법 먼저 파일을 업로드 하는 request를 위해서는 multipart/form-data 형식으로 보내야 하는데 이를 위해서 사용할 CURL 옵션은 -F(--form) 이다. (대문자!!) 그리고 파일의 path를 지정해서 보내면 된다.\n$ curl -F ‘file1=@/upload/file/path’ http://file.testApi.com 여러개 파일 전송 여러개의 파일을 보내야 하는 경우에는 -F 옵션을 연속해서 사용하면 된다.\n$ curl -F ‘file1=@/upload/file1/path’ -F ‘file2=@/upload/file2/path’ http://file.testApi.com 파일변수가 배열인 경우 가끔씩 수신하는 서버의 파일파라미터가 배열연 경우도 있다 이 경우는 아래처럼 하면 된다.\n$ curl -F ‘file[]=@/upload/file1/path’ -F ‘file[]=@/upload/file2/path’ http://file.testApi.com 다른 변수값 함께 전달 파일과 함께 다른 인자를 같이 넘겨줘야 되는 경우도 있는데 이럴 때는 path 형태가 아닌(@ 가 없는) 형태로 보내면 된다.\n$ curl -F ‘file1=@/upload/file/path’ -F \u0026#39;userId=1\u0026#39; -F \u0026#39;title=test\u0026#39; http://file.testApi.com filename 지정 파일인자말고, 원래의 파일이름을 명시하기를 원할 수도 있다.\n$ curl -F ‘file1=@/upload/file/path;filename=Profile1.png’ http://file.testApi.com 참고 : https://medium.com/@petehouston/upload-files-with-curl-93064dcccc76\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/file-upload/",
	"title": "File Upload",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/05/02/start-spring-boot-1/",
	"title": "Spring Boot 매뉴얼 뽀개기 1",
	"tags": ["study", "spring boot"],
	"description": "",
	"type": "post",
	"contents": "관리하던 Spring 프로젝트를 버전업과 함께 Gradle 및 Boot 기반으로 전환하고자 하는 이슈가 있어 회사 동료분과 함께 Spring Boot 를 차근차근 학습해보기로 했다. Spring Boot 를 좀 더 잘 이해해야겠다는 마음에 시작했는데, 야심차게 \u0026ldquo;Spring Boot 매뉴얼 뽀개기!\u0026rdquo; 라고 스터디 제목을 정했다. (과연..)\n스터디를 어떤 방식으로 진행할까 고민하다가 백기선님의 유튜브를 참고해서 영상과 함께 매뉴얼을 훑어 보자는 아이디어가 나왔고, 나쁘지 않겠다 싶어서 그렇게 하기로 했다. 일정은 한달 안에 필요한 기능들을 확인하는 걸로 정했다. 어차피 스터디 멤버는 완전 초심자가 아니기 때문에, 결국 대상은 : 자바 스프링을 사용해본 경험이 있으며, 프레임워크 및 개발 경험이 좀 있는 사람이 되었다. Getting Started를 기준으로 유튜브 영상은 알아서 챙겨보면 될 것 같고, 유튜브상에서는 잘 다루지 않는 Gradle 만 조금 더 서로 내용을 보강하기로 했다. (이렇게 말하지만, 사실 난 받아먹는 쪽.. 주도는 동료분께서..)\n오늘은 그 첫번째 기록이다.\n첫번째 날이므로, 기본적인 매뉴얼 페이지를 훑어보고, 목차를 확인하고, 한달간 진행할 분량을 대략 가늠해 보았다. 먼저 Getting Started 페이지의 목차인데, 목차를 보고 느낀건, 매뉴얼에 필요한건 왠만큼 다 있기 때문에, 매뉴얼만 잘 보면 대략적인 구동방식과 기능을 파악하는데 문제가 없겠다는 생각이었다. 그렇지만 다들 매뉴얼을 잘 확인 안하는게 문제다.(RTFM)\nI. Spring Boot Documentation II. Getting Started III. Using Spring Boot IV. Spring Boot features V. Spring Boot Actuator: Production-ready features VI. Deploying Spring Boot Applications VII. Spring Boot CLI VIII. Build tool plugins IX. ‘How-to’ guides X. Appendices I. Spring Boot Documentation 는 개괄적인 소개이다. 1. 이 문서에 대해서 설명 HTML, PDF, EPUB 으로 확인가능하다는 안내\n2. 도움이 필요할 때 다음을 참고하세요. spring.io stackoverflow github issue 3. 첫번째로 할일 소개, 시스템 필요사항, 설치하기를 확인하고 튜토리얼1, 튜토리얼2를 진행해보자. 예제1, 예제2 4. Spring Boot 와 동작하는 것들 Build System: 메이븐, 그래들, Ant, Starter - 요건 Boot에서 소개하는 것. Best practices : 코드 구조, @Configuration 어노테이션, @EnableAutoConfiguration 어노테이션, 빈과 의존성 주입 코드 실행방법 : IDE, 패키지, 메이븐, 그래들 패키징 : 실서버용 JAR 패키징 Spring Boot CLI 5. Spring Boot 의 기능들 핵심 기능 : Spring Application, External Configuration, Profiles, Logging 웹 어플리케이션 : MVC, Embedded Container SQL , NO-SQL Messaging : JMS, RabbitMQ, Kafka 등.. Testing Extending 6. 실서비스에서 활용하기 Endpoing 관리 HTTP / JMX Monitoring : Metrics, Audting, Tracing, Process 7. 기타 토픽 Boot Application 배포 : 클라우드. 빌드 툴 플러그인 : 메이븐, 그래들 II. Getting Started 을 참고해서 프로젝트를 실행해 본다. 8. Spring Boot 소개하기 Spring Boot 는 손쉽게 단독으로 실행가능(standalone)한 어플리케이션을 만들 수 있도록 도와줍니다. 또한 실행가능한 jar 형태로 패키징될 수 있고, 이를 도와주기 위한 CLI도 제공합니다.\n스프링 개발자를 위한 빠르고, 손쉬운 접근이 가능 경험을 제공하는 것을 목표로 합니다. 별다른 설정없이도 구동이 가능한 어플리케이션을 만들 수 있습니다. 작은 규모에서 부터 큰 프로젝트에 이르기 까지 수용할 수 있습니다. XML 설정을 위한 코드 생성이나 필요사항을 가지지 않습니다. 9. 시스템 요구사항 Spring Boot 2.0.1 버전은 Java 8 또는 9 그리고 스프링프레임워크 5.0.5 이상을 필요로 하고. 빌드툴은 메이븐 3.2+, 그래들4를 지원한다.\n10. 서블릿 : 다음의 embedded 서블릿을 제공한다. 서블릿 3.1 이상을 지원하는 컨테이너들을 지원한다\nTomcat 8.5 Jetty 9.4 Undertow 1.4 11. 설치하기 설치는 여러가지 방법이 있는데 일단 먼저 mavan 기반으로 시작해 보도록 한다.\n먼저 내가 사용하는 IntelliJ 에서 new project 를 선택하고 maven 프로젝트를 선택하자\n그 다음 groupid ex) com.example, artifactid ex) demo 를 지정하고\nNext 를 눌러 project name 과 프로젝트가 저장될 위치를 지정한다.\n이제 pom.xml 파일에 parent, dependencies, build 를 추가해서 다음처럼 구성하자.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;!-- Inherit defaults from Spring Boot --\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;!-- Add typical dependencies for a web application --\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!-- Package as an executable jar --\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; pom.xml 파일 에서 spring-boot-starter-parent 을 입력할 때 버전을 지정하면, 하위 ex) spring-boot-starter-web 에서도 동일한 버전을 따른다.\nIDE 의 View - Tool windows - Maven project view를 열어서 maven reimport 를 실행하자. maven 의존 패키지들을 다운받는다.\nsrc/main/java/com.example/ 위치에 Application.java 파일을 새롭게 작성하자\npackage com.example; import org.springframework.boot.*; import org.springframework.boot.autoconfigure.*; import org.springframework.web.bind.annotation.*; @RestController @EnableAutoConfiguration public class Application { @RequestMapping(\u0026#34;/\u0026#34;) String home() { return \u0026#34;Hello World!\u0026#34;; } public static void main(String[] args) throws Exception { SpringApplication.run(Application.class, args); } } maven project view 에서 run maven build 를 실행해서 어플리케이션을 실행시켜 보자 http://localhsot:8080 에서 Hello world 를 확인할 수 있다.\n어노테이션을 조금 살펴보면, @RequestMapping 어노테이션은 라우팅 정보를 제공한다.\n@RestController 어노테이션은 콜러에서 직접 결과 문자열을 돌려주도록 스프링에게 지시한다. 이 둘은 spring MVC 어노테이션으로 MVC 섹션에서 더 자세히 보도록 한다.\n@EnableAutoConfiguration 어노테이션은 추가한 jar를 기반으로 어떻게 스프링을 설정할 것인지 스프링 부트가 추측하도록 지시한다. spring-boot-stater-web은 톰캣, 스프링 MVC를 추가하기 때문에 자동-설정은 웹 어플리케이션을 개발한다고 예상하고, 그에 따라 스프링을 set up 한다.\n이제 이 어플리케이션을 패키징 해보자. maven project view 에서 Lifecycle \u0026gt; package 를 실행하면 실행가능한 jar 파일이 target 디렉토리 및에 생성된다.\nconsole 에서 java -jar target/demo-1.0-SNAPSHOT.jar 라고 입력해보자. maven project 에서 실행한 결과와 동일한 어플리케이션이 구동된다.\n일단 첫날은 요정도의 내용을 진행해보았고, 이후에 차근차근 다른 내용을 살펴볼 예정이다. 첫날에는 maven 으로 시작했지만 앞으로의 목표는 gradle을 추가적으로 적용해보도록 하겠다.\nTo be continue..\n참고 Spring Boot Getting Started "
},
{
	"permalink": "https://findstar.pe.kr/tags/study/",
	"title": "Study",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/ansible/",
	"title": "Ansible",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/04/04/ansible-awx-installation/",
	"title": "Ansible AWX 를 설치해보기",
	"tags": ["ansible", "awx", "installation", "ansible tower"],
	"description": "",
	"type": "post",
	"contents": "Ansible AWS 설치해보기 배경 ansible을 팀에서 사용하면서, 몇가지 이런게 있었으면 하는 점이 있었는데, 그 중하나가 바로 GUI 환경이다. 물론 CLI 콘솔 상에서 playbook을 실행하는데는 아무 문제가 없고 잘 사용하고 있지만, GUI에서도 보기 쉽게 playbook을 실행하고 또한 누가 언제 playbook을 실행했는지 기록되면 좋겠다는 바램이 있었다. Ansible Tower는 이러한 바램을 해소시켜 줄 수 있는 대안이라고 생각이 되지만, 라이센스 비용이 비싸기로 유명(?) 해서, 포기하고 있었는데. Redhat에서 Ansible Tower 의 오픈소스 버전으로 Ansible AWX를 발표했다! (대인배\u0026hellip;Redhat.)\n특징 AWX(Towner의 오픈소스 버전)는 stanalone으로 구동하며, job(playbook) 에 대한 수행 history, 사용자별 권한제어, GUI, 스케줄링을 통한 실행 기능등을 제공한다.\n접근 권한 제어 사용자 및 팀을 구성하고 권한을 설정하여 엑세스를 제한할 수 있다. 스케줄링 작업 일정을 예약하고 반복 옵션을 설정할 수 있다 . 가시성 동작중인 job 과 job의 상태, 내역을 확인할 수 있다. 인벤토리 대상 서버들의 구성 및 dynamic inventory 를 사용하여 클라우드에서 동적으로 생성되고 삭제되는 서버들의 list를 관리할 수 있다. 단일 시스템으로 구동되는데 사용된 기술은 다음과 같다\nDjango Angular JS 1.* Pgsql RabbitMQ memcached 초기 발표 때 부터 Github Repo를 보면서 버전업과 이슈들을 살펴보았는데, 초기에는 install guide가 부실해서 삽질이 많았다. 1.0.3 부터 docker를 통한 설치가 좀 더 간편해졌기 때문에, 이 버전부터는 그냥 docker 구성으로 설치를 진행했다. AWX에서는 완성된 docker-compose.yml 파일을 제공해주지 않기 때문에, 각종 설정을 구성하고, 자체적으로 제공되는 playbook을 실행하면 docker-compose 파일이 생성되는 구조이다. 다음은 Centos 7.4 에서 AWX의 설치 과정을 기록한 것이다.\n설치 필요한 package 설치 $ sudo yum -y install epel-release $ sudo yum -y install git gettext ansible docker nodejs npm gcc-c++ bzip2 $ sudo yum -y install python-docker-py docker 데몬 시작 $ sudo systemctl start docker $ sudo systemctl enable docker awx clone $ git clone https://github.com/ansible/awx.git $ cd awx/installer/ inventory 편집 (핵심!!!)\n설치하려는 환경이 proxy를 통해서 외부에 엑세스 한다면 : host_port, http_proxy, https_proxy, no_proxy 설정을 자체 환경에 맞게 변경한다. postgres_data_dir 위치 변경 : 초기에 /tmp 로 잡혀 있는데 이를 그대로 두고 설치하면, 처음에는 잘 뜨지만, OS 가 임시디렉토리를 정리해버리면 DB Data가 날아가 작업한 데이터를 잃어버리는 사태가.. use_docker_compose=true : installer playbook 을 실행하여 나의 환경에 맞춰진 docker-compose.yml 파일을 생성하는것을 enable 하는 옵션. 이후에는 docker-compose를 통해서 컨테이너를 제어하자. docker_compose_dir=/var/lib/awx : docker-compose를 사용하기로 했다면, docker-compose.yml 파일이 생성될 디렉토리를 지정해야한다. dockerhub_version : docker hub 에서 받아올 awx 버전이다. latest 으로 지정되어 있는데. 아직은 안정화가 덜되어 있는것 같아서 가갑적 태그를 지정해서 쓴다. (현재 최신은 Docker Hub에서 확인) 다음은 내가 설정한 내역이다.\ndockerhub_version=1.0.4.83 postgres_data_dir=/var/awx/pgdocker use_docker_compose=true docker_compose_dir=/var/lib/awx 이제 playbook을 실행해서 AWX를 설치한다. ansible-playbook -i inventory install.yml docker_compose_dir 로 이동해보면 docker-compose.yml 파일이 생성되었다. docker-compose up -d 이제 웹 브라우저로 접근해보자 초기 접속 계정은 admin/password 이다. 여기까지하면, AWX는 설치가 완료된것이다. 이제 project를 설정해보자.\ncredential 추가 항목 설정값 name Ansible Control Node Credential description playbook 을 실행하기 위해서 control node 에 접속하기 위한 credential organization Default type machine username ssh 계정명 password ssh 패스워드 PRIVILEGE ESCALATION sudo project 추가 항목 설정값 name Project Name description 프로젝트 설명 organization Default SCM type git SCM Url ansible playbook github repo 주소 SCM Branch SCM Update Options Clean, Update on Launch 체크 inventory 추가\nansible 을 CLI 에서 다룰 때는 hosts 파일을 지정하면 되었지만, awx 에서는 target node들을 db로 관리한다. 따라서 연결된 SCM 에서 hosts 파일을 읽어 오거나 (매번 playbook 이 실행되기 전에 hosts 내역을 업데이트 한다)/dynamic inventory(클라우드와 같이 target node 들이 유연하게 생성/삭제되어 변경되는 경우)/ 또는 수동으로 직접 관리할 수 있다..\njob template 추가\n항목 설정값 name 실행할 playbook 의 제목 description 실행할 playbook 의 설명 job type run (check 인경우에는 dry run 만 수행) inventory UI 상에서 추가한 inventory 연결 project Project Name playbook 프로젝트에 연결된 SCM(git)에서 playbook 리스트를 자동으로 불러와 그 중 하나를 선택한다. machen Credential Ansible Control Node Credential : playbook 을 실행하기 위한 control node 에 접속하기 위한 계정정보 Options Enable Privilges Escalation (sudo 필요시 체크) run job template\ntemplate 에서 등록한 job을 run(로켓 모양 아이콘 클릭) 하면 된다. 이렇게 되면 jobs 메뉴에 실행 이력이 추가된다.\nIdentity added: /tmp/awx_40_wPmxLi/credential_6 (/tmp/awx_40_wPmxLi/credential_6) Using /etc/ansible/ansible.cfg as config file PLAY [all] ********************************************************************* TASK [delete project directory before update] ********************************** skipping: [localhost] TASK [check repo using git] **************************************************** skipping: [localhost] TASK [update project using git] ************************************************ changed: [localhost] TASK [Set the git repository version] ****************************************** ok: [localhost] TASK [update project using hg] ************************************************* skipping: [localhost] TASK [Set the hg repository version] ******************************************* skipping: [localhost] TASK [parse hg version string properly] **************************************** skipping: [localhost] TASK [update project using svn] ************************************************ skipping: [localhost] TASK [Set the svn repository version] ****************************************** skipping: [localhost] TASK [parse subversion version string properly] ******************************** skipping: [localhost] TASK [Ensure the project directory is present] ********************************* skipping: [localhost] TASK [Fetch Insights Playbook(s)] ********************************************** skipping: [localhost] TASK [Save Insights Version] *************************************************** skipping: [localhost] TASK [Repository Version] ****************************************************** ok: [localhost] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Repository Version fafb252e642065bff7dfa4349adeade7e085825d\u0026#34; } TASK [Write Repository Version] ************************************************ changed: [localhost] PLAY [all] ********************************************************************* TASK [detect requirements.yml] ************************************************* skipping: [localhost] TASK [fetch galaxy roles from requirements.yml] ******************************** skipping: [localhost] PLAY RECAP ********************************************************************* localhost : ok=4 changed=2 unreachable=0 failed=0 이제 필요한 만큼 job template(playbook)을 연결하고 실행하면 된다. 요약하자면, awx clone =\u0026gt; inventory 파일 설정 =\u0026gt; docker-compose up 이다. 보다 자세한 설치는 install 가이드를 확인하자. 다음에는 Kubernetes 에서 설치해봐야 겠다.\n추가 사항 dynamic inventory 설정 : AWS or GCP or OpenStack 과 같이 인스턴스가 dynamic 하게 생성/삭제되는 경우에 target node를 특정할 수 없다. 이런경우 Cloud insfra 에서 지원되는 API를 통해서 inventory target node(hosts)를 질의해오는 방법이 있는데 이를 dynamic inventory라고 한다.\nAWX 는 standalone 으로 API 서버로 동작이 가능하다. 따라서 jenkins 나 다른 툴/시스템에서 API를 호출하여 job template을 실행할 수 있다. https://github.com/ansible/tower-cli 를 참고.\n참고 예제 - https://github.com/ansible/ansible-examples Best priatices - http://docs.ansible.com/ansible/latest/playbooks_best_practices.html Ansible Essential - https://www.ansible.com/blog/ansible-best-practices-essentials AWX 사용 - https://steemit.com/utopian-io/@adson/ansible-open-sources-ansible-tower-with-awx "
},
{
	"permalink": "https://findstar.pe.kr/tags/ansible-tower/",
	"title": "Ansible Tower",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/awx/",
	"title": "Awx",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/difference/",
	"title": "Difference",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/01/24/difference-between-null-and-empty-string-elasticsearch/",
	"title": "ElasticSearch 에서 null 과 empty string 의 차이",
	"tags": ["ElasticSearch", "null", "empty string", "difference"],
	"description": "",
	"type": "post",
	"contents": "1월 24일 (수) TIL ElasticSearch 에서 indexing 을 처리할 때, json 객체를 생성하면서 속성값이 때로는 null 또 어떨때는 빈 문자열 \u0026quot;\u0026quot; 로 채워넣을 때가 있었는데 보다 명확하게 이해하고 처리하기 위해서 null 값과 empty string(\u0026quot;\u0026quot;) 의 차이를 알아보았다.\n결론부터 말하자면 둘의 차이는 없다라고 이야기 할 수 있겠다.\n그 이유는 바로 inverted index 를 생각하면 알 수 있는 건데, null 과 empty string 은 둘다 analyzer 에 의해서 분석된 이후 inverted index를 하나이상 생성하지 못하기 떄문에 ES 입장에서는 둘의 차이가 없는 것이다.\n그렇다면\nnull 또는 empty string 과 다르게 아예 mapping field 가 없는 경우는 어떠한가? 이 또한 inverted index 가 생성되지 않기 때문에 동일하게 취급되는가? 이 경우에도 마찬가지다. inverted index 가 생성되지 않기 때문에, 두개는 동일하게 취급된다.\nexist 쿼리를 사용해도 마찬가지다. 다음의 5개 데이터가 있다고 가정해보자. { \u0026#34;tags\u0026#34; : [\u0026#34;search\u0026#34;] } // id 1 { \u0026#34;tags\u0026#34; : [\u0026#34;search\u0026#34;, \u0026#34;open_source\u0026#34;] } // id 2 { \u0026#34;other_field\u0026#34; : \u0026#34;some data\u0026#34; } // id 3 { \u0026#34;tags\u0026#34; : null } // id 4 { \u0026#34;tags\u0026#34; : [\u0026#34;search\u0026#34;, null] } // id 5 아래와 같이 질의를 하면,\n{ \u0026#34;query\u0026#34; : { \u0026#34;constant_score\u0026#34; : { \u0026#34;filter\u0026#34; : { \u0026#34;exists\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;tags\u0026#34; } } } } } 다음의 결과를 확인가능하다.\n\u0026#34;hits\u0026#34; : [ { \u0026#34;_id\u0026#34; : \u0026#34;1\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;tags\u0026#34; : [\u0026#34;search\u0026#34;] } }, { \u0026#34;_id\u0026#34; : \u0026#34;5\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;tags\u0026#34; : [\u0026#34;search\u0026#34;, null] } }, { \u0026#34;_id\u0026#34; : \u0026#34;2\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;tags\u0026#34; : [\u0026#34;search\u0026#34;, \u0026#34;open source\u0026#34;] } } ] 결과는 exist 쿼리를 질의해도 3, 4번 데이터가 hit 되지 않는다. mapping field 가 아예 존재하지 않는 경우와 null, empty string 인 경우 모두 동일하다.\n그렇다면, 속성값이 null 인 document를 DSL 을 통해서 찾을 수 있을까? 아래와 같이 질의하면,\n{ \u0026#34;query\u0026#34; : { \u0026#34;constant_score\u0026#34; : { \u0026#34;filter\u0026#34;: { \u0026#34;missing\u0026#34; : { \u0026#34;field\u0026#34; : \u0026#34;tags\u0026#34; } } } } } 다음의 결과를 확인가능하다\n\u0026#34;hits\u0026#34; : [ { \u0026#34;_id\u0026#34; : \u0026#34;3\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;other_field\u0026#34; : \u0026#34;some data\u0026#34; } }, { \u0026#34;_id\u0026#34; : \u0026#34;4\u0026#34;, \u0026#34;_score\u0026#34; : 1.0, \u0026#34;_source\u0026#34; : { \u0026#34;tags\u0026#34; : null } } ] mapping field 상에서 존재하지 않는 경우와, value 가 null 인 경우 모두를 찾는다. 따라서 아예 field가 없는 경우만 딱 지정해서 찾을 수는 없다.\n요약하자면, inverted index를 기준으로 모든 ES query 와 data를 생각하면 된다는 것이다. null, empty string(\u0026quot;\u0026quot;), missing field 모두 동일하다.\n참고 : https://www.elastic.co/guide/en/elasticsearch/guide/current/_dealing_with_null_values.html\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/empty-string/",
	"title": "Empty String",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/null/",
	"title": "Null",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/golang/",
	"title": "Golang",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/01/23/null-scan-for-mysql-struct-golang/",
	"title": "Golang 에서 Mysql 데이터를 Struct 에 Scan 할 때 Null 처리 경험기",
	"tags": ["Golang", "mysql", "struct", "json", "null"],
	"description": "",
	"type": "post",
	"contents": "1월 23일 (화) TIL 최근 ElasticSearch 에 bulk insert 를 하기 위한 golang 기반의 간단한 툴을 만들고 있다. mysql 에서 데이터를 가져온 다음에 이를 JSON으로 변환해서 ElasticSearch로 bulk insert 하는 구조인데, 이 과정에서 mysql rows 를 golang 에서 scan 하면서 알게된 내용을 정리해보았다.\n먼저 golang 툴을 만들면서 사용한 패키지는 아래와 같다.\n\u0026#34;gopkg.in/olivere/elastic.v5\u0026#34; \u0026#34;database/sql\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; \u0026#34;encoding/json\u0026#34; 처음 생성한 Post라는 이름의 struct는 다음과 같다.\ntype Post struct { PostId int64 `json:\u0026#34;postId\u0026#34;` AuthorId int64 `json:\u0026#34;authorId\u0026#34;` Category int64 `json:\u0026#34;categoryId\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Created string `json:\u0026#34;created\u0026#34;` Ip string `json:\u0026#34;-\u0026#34;` Ip1 int64 `json:\u0026#34;ip1\u0026#34;` Ip2 int64 `json:\u0026#34;ip2\u0026#34;` Ip3 int64 `json:\u0026#34;ip3\u0026#34;` Ip4 int64 `json:\u0026#34;ip4\u0026#34;` } 처음에는 쿼리를 해온 다음에 rows.Scan 와 같이 Post type 의 속성에 값을 넣으면 되겠지라고 생각했었는데, 실제로는 값이 null 인경우 에러가 발생한다 (여기서는 category 값이 null 인 경우가 존재했다.)\nsql: Scan error on column index 3: unsupported driver -\u0026gt; Scan pair: \u0026lt;nil\u0026gt; -\u0026gt; *int64 따라서 이경우는 sql 패키지에서 제공하는 sql.NullString을 사용해야 한다.\nNullString 은 다음과 같이 valid 를 확인해서 그 값을 이용해야 한다.\nif post.RawCategoryId.Valid { post.CategoryId = post.RawCategoryId.Int64 } 최종적으로 struct 는 다음처럼 구성했다.\ntype Post struct { PostId int64 `json:\u0026#34;postId\u0026#34;` AuthorId int64 `json:\u0026#34;authorId\u0026#34;` RawCategory sql.NullInt64 `json:\u0026#34;-\u0026#34;` Category int64 `json:\u0026#34;categoryId\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` Created string `json:\u0026#34;created\u0026#34;` Ip string `json:\u0026#34;-\u0026#34;` Ip1 int64 `json:\u0026#34;ip1\u0026#34;` Ip2 int64 `json:\u0026#34;ip2\u0026#34;` Ip3 int64 `json:\u0026#34;ip3\u0026#34;` Ip4 int64 `json:\u0026#34;ip4\u0026#34;` } sql 패키지에서 제공하는 타입은 당연하겠지만. NullString, NullBool, NullInt64, NullFloat64 가 있다.\n이후에 이 post struct 를 이용해서 JSON을 만들면 된다. 내 경우에는 추가적으로 IP를 ip1, ip2, ip3, ip4 로 나눴는데 이는 ES 쿼리를 편리하게 하기 위해서다.\n참고로 JSON으로 json.Marshal(post) 할 때에는 struct 에서 json:\u0026quot;-\u0026quot; 으로 선언한 속성은 Marshal 결과에 포함되지 않는다.\n참고 : https://medium.com/aubergine-solutions/how-i-handled-null-possible-values-from-database-rows-in-golang-521fb0ee267\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/json/",
	"title": "Json",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/mysql/",
	"title": "Mysql",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/struct/",
	"title": "Struct",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/01/19/understanding-query-on-elasticsearch/",
	"title": "ElasticSearch 에서 term, match, match_phrase 쿼리에 대한 이해",
	"tags": ["ElasticSearch", "term", "match", "match_phrase", "query DSL"],
	"description": "",
	"type": "post",
	"contents": "1월 19일 (금) TIL 작성한 ElasticSearch 쿼리를 테스트 해보다가 정확하게 이해하지 못한 상태고 작성하고 있는 키워드를 발견했다. 작성한 쿼리에는 term, match, match_phrase가 무분별하게 사용되고 있었는데, 다시금 문서를 확인해보고, 잊어먹기 전에 정리해봤다.\nterm 사전적 의미로 보자면 용어 쯤 되겠다. 해당 content 의 inverted index 에 저장되는 token 들 중에서 쿼리의 키워드와 일치하는 녀석이 있는지 찾아준다.\noriginal text 가 *\u0026ldquo;여러개의 물건들\u0026rdquo;*이고, 다음처럼 tokenize 된다면,\ncurl -XPOST \u0026#39;localhost:9200/my_index/_analyze?pretty\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;여러개의 물건들\u0026#34; } { \u0026#34;tokens\u0026#34; : [ { \u0026#34;token\u0026#34; : \u0026#34;여러\u0026#34;, \u0026#34;start_offset\u0026#34; : 0, \u0026#34;end_offset\u0026#34; : 2, \u0026#34;type\u0026#34; : \u0026#34;MM\u0026#34;, \u0026#34;position\u0026#34; : 0 }, { \u0026#34;token\u0026#34; : \u0026#34;개\u0026#34;, \u0026#34;start_offset\u0026#34; : 2, \u0026#34;end_offset\u0026#34; : 3, \u0026#34;type\u0026#34; : \u0026#34;NNB\u0026#34;, \u0026#34;position\u0026#34; : 1 }, { \u0026#34;token\u0026#34; : \u0026#34;물건\u0026#34;, \u0026#34;start_offset\u0026#34; : 5, \u0026#34;end_offset\u0026#34; : 7, \u0026#34;type\u0026#34; : \u0026#34;NNG\u0026#34;, \u0026#34;position\u0026#34; : 2 }, { \u0026#34;token\u0026#34; : \u0026#34;물건들\u0026#34;, \u0026#34;start_offset\u0026#34; : 5, \u0026#34;end_offset\u0026#34; : 8, \u0026#34;type\u0026#34; : \u0026#34;NNG\u0026#34;, \u0026#34;position\u0026#34; : 2 } ] } term 쿼리를 통해서 검색 결과에 포함하려면 다음의 4가지 쿼리가 가능하다.\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;여러\u0026#34; } } ] } } } { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;개\u0026#34; } } ] } } } { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;물건\u0026#34; } } ] } } } { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;물건들\u0026#34; } } ] } } } match term과 마찬가지로 inverted index 에 저장되는 token 들 중에서 일치하는 녀석이 있는지 찾아주는데, 차이점은 바로 검색하는 키워드를 analyze 한다는 것이다. 이 analyze 한 결과의 token 들 중에서 하나라도 일치하면 결과 document 에 포함된다.\n따라서 original text 가 위와 같을 때 가능한 쿼리는 다음과 같다.\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;여러개\u0026#34; } } ] } } } { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;여러사\u0026#34; } } ] } } } { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;나의 물건들\u0026#34; } } ] } } } 그 밖에 더 있을 수 있다\u0026hellip;\nmatch_phrase phrase 단어는 사전적 의미는 구 인데, 흔히 영어 문법 공부할 때 들어보았던 명사구, 부사, 전치사구\u0026hellip; 의 구 이다. 둘 또는 그 이상의 어절로 이루어져 한 덩어리로써 절이나 문장의 성분이 되는 동일한 말의 단위 라고 한다. 여기서 포인트는 둘 이상!, 이루어져! 이다. 즉 match 가 token 들 중에 일치하는 keyword 가 하나라도 존재한다면 결과 document 에 포함된다면, match_phrase 가 검색 match 처럼 keyword를 analyze 하는 것은 동일하나 그 결과 token 들이 모두 존재하고, 순서도 순차적으로 동일한 document 만을 검색 결과에 포함한다는 차이가 있다.\n따라서 match 와 같이 \u0026ldquo;나의 물건들\u0026rdquo; 이라는 검색 키워드를 넣는다면, 나 라는 token 이 들어 있지도 않고 순서도 맞지 않으니. 여러가지 물건들 이라는 document 는 결과에 포함되지 않는다.\n이 때 가능한 쿼리는 다음과 같다.\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match_phrase\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;여러개\u0026#34; } } ] } } } { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match_phrase\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;여러개의 물건들\u0026#34; } } ] } } } 순서만 바꿔도 안된다\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match_phrase\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;물건들 여러개\u0026#34; } } ] } } } 사용예 term 의 경우에는 문서의 tag 를 검색할 때 사용할 수 있겠다. 본문 검색에서는 match_phrase 를 사용하는게 match 보다는 적합하다고 생각된다. (물론 nested should 와 함께 더 넣어야한다.) 오늘도 또 한번 느낀다. RTFM. 문서를 잘 읽자.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/match/",
	"title": "Match",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/query-dsl/",
	"title": "Query DSL",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/term/",
	"title": "Term",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/01/17/understanding-whildcard-query-on-elasticsearch/",
	"title": "ElasticSearch 에서 wildcard 쿼리에 대한 이해",
	"tags": ["ElasticSearch", "wildcard", "query DSL", "inverted index"],
	"description": "",
	"type": "post",
	"contents": "1월 17일 (수) TIL ElasticSearch 에서 쿼리를 작성하던 중 wildcard 쿼리의 결과가 내가 생각했던 것 과는 달라서 내용을 정리해본다. wildcard query를 작성할 때 기대한 것은 RDBMS 의 like \u0026lsquo;%keyword%\u0026rsquo; 와 같은 형태가 가능할 것으로 기대했는데, 막상 쿼리 결과를 확인해 보니 원하는 형태가 아니었다.\noriginal text 가 *\u0026ldquo;여러개의 물건들\u0026rdquo;*이고, 내가 시도한 쿼리는 다음과 같았다.\n{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;wildcard\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;*여러개*\u0026#34; } } ] } }, \u0026#34;size\u0026#34;: 100 } 그런데 결과는 아래와 같이 hits count 가 0 이었다.\n{ \u0026#34;took\u0026#34;: 243, \u0026#34;timed_out\u0026#34;: false, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 0, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } 원인은 바로 wildcard query 가 term level query 이기 때문이다.\nterm level query 문서를 확인해 보자. (진작에 좀 읽었어야 하는데..)\nterm 즉 inverted index 를 기준으로 결과를 찾는다는 의미다. 다시 말해 analyzed 된 term keyword 가 있어야 하며, 문서를 색인할 때 토크나이징된 단어가 아니라면 inverted index에 들어 있지 않기 때문에, 아예 비교 대상에 포함되지 않는 것이다.\n내가 테스트한 custom analyzer의 토크나이징은 아래와 같았으니,\ncurl -XPOST \u0026#39;localhost:9200/my_index/_analyze?pretty\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -d\u0026#39; { \u0026#34;analyzer\u0026#34;: \u0026#34;my_custom_analyzer\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;여러개의 물건들\u0026#34; } { \u0026#34;tokens\u0026#34; : [ { \u0026#34;token\u0026#34; : \u0026#34;여러\u0026#34;, \u0026#34;start_offset\u0026#34; : 0, \u0026#34;end_offset\u0026#34; : 2, \u0026#34;type\u0026#34; : \u0026#34;MM\u0026#34;, \u0026#34;position\u0026#34; : 0 }, { \u0026#34;token\u0026#34; : \u0026#34;개\u0026#34;, \u0026#34;start_offset\u0026#34; : 2, \u0026#34;end_offset\u0026#34; : 3, \u0026#34;type\u0026#34; : \u0026#34;NNB\u0026#34;, \u0026#34;position\u0026#34; : 1 }, { \u0026#34;token\u0026#34; : \u0026#34;물건\u0026#34;, \u0026#34;start_offset\u0026#34; : 5, \u0026#34;end_offset\u0026#34; : 7, \u0026#34;type\u0026#34; : \u0026#34;NNG\u0026#34;, \u0026#34;position\u0026#34; : 2 }, { \u0026#34;token\u0026#34; : \u0026#34;물건들\u0026#34;, \u0026#34;start_offset\u0026#34; : 5, \u0026#34;end_offset\u0026#34; : 8, \u0026#34;type\u0026#34; : \u0026#34;NNG\u0026#34;, \u0026#34;position\u0026#34; : 2 } ] } 토크나이징된 결과에 \u0026ldquo;여러개\u0026rdquo; 라는 단어는 없기 때문에, 처음 질의한 wildcard 결과가 hits가 0 일 수 밖에 없었던 것이다.\n아 문서를 좀 더 잘 읽자.. 오늘의 교훈 \u0026ldquo;RTFM\u0026rdquo;\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/inverted-index/",
	"title": "Inverted Index",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/phpstorm/",
	"title": "Phpstorm",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/speed-up/",
	"title": "Speed Up",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/vm-options/",
	"title": "Vm Options",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/01/05/increse-heap-memory-on-phpstorm/",
	"title": "느려진 PHPStorm에서 Heap Memory를 늘리는 방법",
	"tags": ["phpstorm", "speed up", "vm options"],
	"description": "",
	"type": "post",
	"contents": "1월 05일 (금) TIL 이전에, phpstorm에서 사용하지 않는 플러그인을 비활성화 시켜서 약간의 속도 향상을 가져왔다면, 이제는 아예 Heap Memory Size를 늘려보기로 했다. IDE를 사용하다가 보면 열어둔 Tab이 많아지면서 슬슬 Heap memory size가 차기 시작하는데, 이건 예전에 Eclipse를 쓸 때 부터, Intellij, Webstorm, Phpstorm 가리지 않고 나타나는 증상이다. 커서가 렉 걸린 것 처럼 느리게 이동하기 시작하면, 우측 하단에 Memory Indicator를 바라보고는, 여기서 매번 Heap Memory Size 를 확인하고 클릭해주면서 한번씩 정리가 되는데, 그러고 나면 다시 괜찮아지고는 했다. 아예 Heap Memory Size 설정을 변경하기 위해서 설정을 변경해봤다.\n먼저 heap memory 를 정확하게 확인하기 위해서 memory indicator를 확인해보자.\n1 먼저 설정의 appearance 에서 window option의 show memory indicato를 켜자.\n2 그러면 에디터 우측 하단에 요렇게 메모리가 표시된다\n3 그런다음의 help 메뉴의 Edit Custom Vm Option을 선택하자\n4 이제 Custom Option을 지정하면 되는데 핵심은 Xmx 부분인데 maximum memory 이다. 이걸 머신의 ram 에 따라서 늘려주자 내가 쓰는 맥북의 RAM은 16G 인데 Xmx 를 처음에는 500M 에서 2G 로 변경해줬다.\n이제 IDE를 재시작하면 늘어난 memory 를 indicator 에서 확인가능하고, 버벅거림이 훨~~~씬 줄어들었다.\n"
},
{
	"permalink": "https://findstar.pe.kr/2018/01/04/disable-unused-plugins-phpstorm/",
	"title": "PHPStorm에서 사용하지 않는 플러그인 비활성화 시키기.",
	"tags": ["phpstorm", "speed up", "unused plugins"],
	"description": "",
	"type": "post",
	"contents": "1월 04일 (목) TIL phpstorm이 조금 버벅거리면서 느린 느낌이 들었다. 내 경우에는, JavaScript 와 php 가 같이 들어 있는 템플릿 파일을 수정하거나, 마크다운MD 파일을 수정할 때 그런 증상들이 심해졌다. 일단 사용하지 않는 플러그인들을 비활성화 시켜서 속도가 조금 개선되는지 확인해 보기로 했다.\n아래의 플러그인들은 사용하지 않음.\nASP : ASP 코딩 할일이 없어서 해제 Behat : BDD framework 인 Behat을 사용하지 않아서 해제 (이걸 uncheck 하면 연결된 codeception framework 도 해제된다) CoffeeScript : 커피스크립트 코딩 할일이 없어서 해제 CVS : subversion 이전에 활약하던 CVS 이다. 나는 git만 사용하므로 해제 Drupal support : Drupal 프레임워크를 사용하지 않으므로 해제 Gherikin : 스타트랙을 보긴했지만. 이건 그냥 위트용이다. 해제 Google App Engine Support : GAE 연동을 하지 않으므로 해제 (난 AWS\u0026hellip;) Haml : 사용하지 않아 해제 Handlebars/Mustache : 이건 사용하는 사람은 제법 있을 수 있지만, 나는 사용하지 않는 템플릿이라 해제 hg4idea : Mercurial version control system을 사용하지 않아 해제. Joomla : Joomla 프레임워크도 사용하지 않으므로 해제 Perforce : perforce VCS 사용하지 않으므로 해제 Phing : PHP 빌드툴인 Phing 이지만 사용하지 않으므로 해제(참고로 apache ant 기반의 빌드 툴이다.) PHPSpec : Behat 처럼 BDD framework 이지만 사용하지 않아 해제 Subversion : Git 만 쓴다.. subversion 도 안녕~ 해제. TextMate : TextMate 도 사용하지 않아 해제. Twig : Twig 템플릿도 사용하지 않아 해제 WordPress Support : Wordpress 도 사용하지 않는다. 해제. Vagrant : Vagrant 를 사용할 수 있지만, IDE 레벨에서 연동하지 않는다. 해제. tslint : Type Script Lint 사용하지 않아 해제. 사용하지 않는 플러그인들을 해제하고 apply 하면 IDE를 재시작하겠냐고 물어보는데 바로 재시작해줬다. 약간(?) 쾌적해진 듯 느껴지지만, 플라시보 인것 같기도.. 다음은 Heap Size를 늘리도록 VM Options 을 조정해봐야겠다.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/unused-plugins/",
	"title": "Unused Plugins",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/jetbrains/",
	"title": "Jetbrains",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/macro-key-mapping/",
	"title": "Macro Key Mapping",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2018/01/03/create-macro-shortcut-on-phpstorm/",
	"title": "PHPStorm 에서 매크로 단축키를 지정하는 방법",
	"tags": ["phpstorm", "jetbrains", "macro key mapping"],
	"description": "",
	"type": "post",
	"contents": "1월 03일 (수) TIL 참고 매뉴얼 : Jetbrains Recording Macros phpstorm의 버전을 올렸는데 지정해놓은 macro key mapping 이 이상해졌다. 지우고 새로 만들려니 macro 설정 하는 방법을 까먹어서 기억을 더듬어서 다시 한번 정리해본다.\n1 먼저 phpstorm 의 Edit -\u0026gt; Macros 를 살펴보자.\n2 다음으로 Macros 에서 Start Macro Recording 을 선택하자.\n3 그럼 아무런 표시 없이 커서만 깜빡 거릴텐데 우측 하단에 보면 Recodring 이라고 표시가 되고 있다.\n4 이제 자신이 저장하고 싶은 Custom 한 액션들을 수행하면 된다. 내 경우에는 Option + F1 (Select In)을 누른다음, 숫자 키 \u0026lsquo;1\u0026rsquo;을 눌러서 project view로 이동하는 액션을 취했다.\n이렇게 하면 원할 때 현재 편집중인 파일의 위치로 project view의 선택된 라인을 이동시킬 수 있다. autoscroll from source 라는 기능으로도 동일한 needs 를 해소 할 수 있지만 내가 원하는건, 항상이 아닌 특정 액션을 수행할 때만 이었기 때문에 macro 로 만들었다.\n5 그 다음에 다시 Macro 매뉴에서 Stop Macro Recording을 선택하자.\n6 레코딩이 진행되는 동안 수행한 액션들을 하나의 매크로로 만든다. 이제 이름을 지정하자\n7 이름까지 저장하고 나면, Edit -\u0026gt; Macros -\u0026gt; 아래쪽에 새로운 이름의 Macro 들을 확인할 수 있다.\n8 이제 Command + , 를 눌러 설정 창에서 keymap 을 누른 다음 macros 에 보면 저장된 매크로의 리스트를 확인할 수 있다. 여기서 내가 원하는 매크로에 키보드 단축키를 지정하면 끝!\n이제 마음껏 내가 원하는 대로 조합한 매크로를 단축키로 사용하면 된다.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/fluentd/",
	"title": "Fluentd",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/til/2018-01-02-local-test-for-fluentd-output/",
	"title": "Fluentd 를 로컬에서 테스트 해보는 방법",
	"tags": ["fluentd"],
	"description": "",
	"type": "post",
	"contents": "1월 02일 (화) TIL Fluentd fluentd (이하 td-agent)를 설치후에 conf를 설정하고 나면 동작을 잘 하는지 확인해야 되는 경우가 있는데 아래와 같이 테스트 해볼 수 있었다. /etc/td-agent/td-agent.conf 가 다음과 같을 때:\n\u0026lt;source\u0026gt; ..... ..... ..... \u0026lt;/source\u0026gt; # 결과 값에 hostname 을 추가로 덧붙임. \u0026lt;filter accesslog\u0026gt; @type record_transformer \u0026lt;record\u0026gt; source \u0026#34;#{Socket.gethostname}\u0026#34; path ${record[\u0026#34;host\u0026#34;]} \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; # Local 에서 테스트 해볼 때 사용할 수 있는 match \u0026lt;match accesslogs\u0026gt; type stdout \u0026lt;/match\u0026gt; $ td-agent -c /etc/td-agent/td-agent.conf\n위와 같이 입력하면 콘솔상에서 standard out을 통해서 match 되는 결과를 확인할 수 있다. 만약 여기에서 error 나 warning 을 확인한다면, td-agent.conf 파일의 source 와 filter를 적절하게 수정해서 다시 테스트 해서 설정을 맞추면 된다.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/iterm/",
	"title": "Iterm",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2017/01/03/itermocil/",
	"title": "itermocil을 사용해서 다수의 서버에 SSH 접속하기",
	"tags": ["ssh", "iterm", "macro"],
	"description": "SSH connect to multiple server by itermocil",
	"type": "post",
	"contents": "얼마전 작업하는 서비스의 서버의 OS 교체를 진행했다. 이 과정에서 사용하게된 itermocil 툴을 소개해본다.\n처음 계획으로는 Docker 기반의 배포 환경을 구성하려고 했는데, 시간이 모자라 진행하지는 못했고,\n예전 보다 조금 더 나은 수준의 배포환경을 구성하는 데 만족해야만 했다.\nCentos7 환경을 구성하였는데, 설치 한 뒤로도 자잘하게 환경설정과, 몇가지 권한설정을 추가적으로 진행했었다.\n이때 불편한 점이 하나 있었는데, 그것은 바로 한번에 다수의 서버에 SSH 접속하는 방법 이다.\n내가 사용하는 터미널 프로그램은 iterm 인데, Mac 에서 개발은 하는 사람치고 iterm을 사용하지 않는 사람을 볼 수가 없을 만큼 사랑받는 프로그램이다.\n물론 tmux 나 기타 방법으로도 다수의 서버에 쉽게 접속해 볼 수 있지만, 나는 alias 형태로 predefined 된 macro 같은 툴을 원했고,\n이과정에서 itermocil 이라는 툴을 발견했다.\nitermocil 소개 이름에서 알 수 있듯이 iterm 에서 사용하는 일종의 매크로라고 할 수 있다.\n\u0026ldquo;pre-configured layouts of windows and panes\u0026rdquo; 라고 되어 있는데 말 그대로 사전에 정의된 윈도우와 패널 레이아웃 관리자 이다.\n사용법도 간단한데\nitermocil layout-name 이렇게 하면 지정된 레이아웃에 정의된 명령어들을 수행하여 준다.\n설치방법 brew 를 통해서 설치하고 홈 디렉토리에 .itermocil 디렉토리를 만들어 준다.\n이후에 yml 포맷의 레이아웃 파일을 만들면 된다.\n처음에는 레이아웃파일이 없을 테니 샘플을 구성할 수 있게 지원하고 있다\n# brew 로 설치 $ brew install TomAnthony/brews/itermocil # 레이아웃을 저장하기 위한 .itermocil 디렉토리 생성 $ mkdir ~/.itermocil # sample 레이아웃 편집하기 $ itermocil --edit sample # sample 레이아웃 실행하기 $ itermocil sample 레이아웃 파일 구성방법 레이아웃 파일을 구성하는 방법을 다양하게 지원하고 있는데, 자세한건 매뉴얼에서 설명하고 있으니, 문서를 참고하길 바란다.\n내 경우에는 약 20대의 서버에 접속하기 위해서 다음과 같이 처리했다.\nwindows: - name: web-server root: ~/project/working-directory layout: tiled panes: - ssh findstar@service-web1.our-product.io - ssh findstar@service-web11.our-product.io - ssh findstar@service-web2.our-product.io - ssh findstar@service-web12.our-product.io - ssh findstar@service-web3.our-product.io - ssh findstar@service-web13.our-product.io - ssh findstar@service-web4.our-product.io - ssh findstar@service-web14.our-product.io - ssh findstar@service-web5.our-product.io - ssh findstar@service-web15.our-product.io - ssh findstar@service-web6.our-product.io - ssh findstar@service-web16.our-product.io - ssh findstar@service-web7.our-product.io - ssh findstar@service-web17.our-product.io - ssh findstar@service-web8.our-product.io - ssh findstar@service-web18.our-product.io - ssh findstar@service-web9.our-product.io - ssh findstar@service-web19.our-product.io - ssh findstar@service-web10.our-product.io - ssh findstar@service-web20.our-product.io 위에서 내가 사용한 레이아웃 속성은 tiled 인데, 이밖에도 다양한 레이아웃을 지원하니 문서를 참고하자.\n저렇게 구성한 파일을 web-server.yml로 저장하고 itermocil web-server 라고 입력하면 20대의 iterm panel 등장하는데, 제법 멋져보인다.\n물론 나와 같이 itermocil을 사용하여 다수의 서버에 접속하길 원하는 사람이 얼마나 되는지는 알 수 없지만,\n나로써는 유용하게 사용하여 기록으로 남겨놓는다.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/macro/",
	"title": "Macro",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/ssh/",
	"title": "Ssh",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/accesscontrol/",
	"title": "AccessControl",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2016/11/08/apache_2_4_custom_error_log_access_constrol/",
	"title": "apache 2.4에서 달라진 기능 2가지",
	"tags": ["apache2.4", "customLog", "errorLogFormat", "accessControl"],
	"description": "apache 2.4 changes errorLogFormat and access control",
	"type": "post",
	"contents": "apache 2.4버전을 사용하면서 이전버전과는 달라진 두가지 부분에 대해서 알게 되었다. 하나는 ErrorLog 에서 CustomLog format이 가능해졌다는 점과, AccessControl의 문법이 달라졌다는 것이다. Custom Error Log는 잘 쓰면 에러를 추적할 때 정말 유용하다.\n최근 웹서버를 CentOS 7 기반으로 재설치하면서 apache 2.4 버전을 사용해 보았다. 2.2 버전과는 다른 부분중에 2가지 부분을 살펴보았는데 다음과 같다.\nErrorLog 에서 Custom Log Format 이 가능해졌다. apache 2.4 에서는 Error Log 에 대해서 Format 설정이 가능하다. 사용 키워드는 ErrorLogFormat 이고 매뉴얼을 참고하자. link\n이전까지는 access log 에 대해서만 LogFormat 이 가능했는데 2.4부터 error log 에 대해서도 formating 이 설정 가능해졌다. 대부분의 웹서비스에서 error log 는 서비스의 다양한 오류를 추적하고, 상태를 확인하데는 주요하게 사용되기 때문에 이를 활용하면 디버깅이 훨씬 수월해진다.\n사용문법은 다음과 같고, 생각보다 어렵지 않다.\nErrorLogFormat \u0026ldquo;[%{u}t] [%m:%l] [host:%{Host}i , ip:%a] %7F: %E: %M ,\\ referer\\ %{Referer}i ,\\ User-Agent\\ %{User-Agent}i\u0026rdquo;\n여기서 사용한 Format 키워드들은 다음과 같다.\nFormat 키워드 설명 %{u}t 시간을 기록한다 이때 micro-second 까지 포함한다. micro-second 가 필요없다면 %t 만 쓰면 된다. %m 에러 로그를 기록하게된 apache 모듈의 이름을 기록한다. 기본적으로는 core가 기록되고, 에러를 기록하는 모듈명이(ex, authz_core )기록된다. %l 로그 레벨이 기록된다. %{Host}i 서버가 2개 이상의 virtual host 를 가지는 경우 이를 구분하기 위해서 추가했다. 정확하게는 ${name}i 의 형태인데 여기서 name은 Request header 의 name 이다. %a client 의 IP 를 기록한다. %F 문제가 발생한 소스 파일의 이름과 line number 를 기록한다. %E 에러 status code 가 기록된다. 대부분 구글링하게 되는 핵심 키워드. %M 실제 로그 메세지 내용이다. %{Referer}i Request 의 Referer 를 기록하도록 했다. %{User-Agent}i User-Agent 를 기록하도록 했다. %{name}i 의 경우에는 apache conf에서 SetEnvIf 등으로 설정한 파라미터도 기록할 수 있기 때문에 유용하게 사용할 수 있다.\n기타 보다 자세한 내용은 매뉴얼을 참고하면 된다.\nAccess Control 문법의 변경 기존의 apache 2.2 와 비교하여 access control 문법이 변경되었다. 물론 이전 2.2 버전까지 사용되던 문법을 완전히 사용하지 못하는 것은 아니고 mod_access_compat apache 모듈이 설치되어 있다면 사용가능하다. 다만 이경우 2.2 문법과 2.4 문법을 혼용하면, 원하는 대로 동작하지 않을 수 있기 때문에 이왕이면 한쪽으로 통일하는 것이 좋아보인다. 나의 경우에는 2.4 문법으로 통일했다.\n// 2.2 의 문법 \u0026lt;Location /server-status\u0026gt; SetHandler server-status Order deny,allow Deny from all Allow from 127.0.0.1 \u0026lt;/Location\u0026gt; \u0026lt;FilesMatch \u0026#34;^\\.ht\u0026#34;\u0026gt; Order allow,deny Deny from all Satisfy All \u0026lt;/FilesMatch\u0026gt; // 2.4의 문법 \u0026lt;Location /server-status\u0026gt; SetHandler server-status Require all denied Require ip 127.0.0.1 \u0026lt;/Location\u0026gt; \u0026lt;FilesMatch \u0026#34;^\\.ht\u0026#34;\u0026gt; Require all denied Satisfy All \u0026lt;/FilesMatch\u0026gt; 다음은 조금 더 복잡한 access control 이다.\n// 2.2 문법 #bingbot 을 막아보았다. BrowserMatch \u0026#34;bingbot\u0026#34; bingbot #192.168.10.10 IP를 막아보았다. SetEnvIfNoCase Remote_Addr 192.168.10.10 ddosIP \u0026lt;Location /\u0026gt; Order allow,deny Allow from all Deny from env=bingbot Deny from env=ddosIP \u0026lt;/Location\u0026gt; // 2.4 문법 #bingbot 을 막아보았다. BrowserMatch \u0026#34;bingbot\u0026#34; bingbot #192.168.10.10 IP를 막아보았다. SetEnvIfNoCase Remote_Addr 192.168.10.10 ddosIP \u0026lt;Location /\u0026gt; \u0026lt;RequireAny\u0026gt; \u0026lt;RequireAll\u0026gt; Require all granted Require not env bingbot Require not env ddosIP \u0026lt;/RequireAll\u0026gt; \u0026lt;/RequireAny\u0026gt; \u0026lt;/Location\u0026gt; 지금 당장은 apache 2.2 에서 사용하던 access control 문법을 바꾸지 않아도 되지만, 미래에는 없어질 것이기 때문에 시간이 난다면 고쳐주는 게 좋은것 같다.\n보다 자세한 문법은 매뉴얼을 참고하자.\n"
},
{
	"permalink": "https://findstar.pe.kr/tags/apache2.4/",
	"title": "Apache2.4",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/customlog/",
	"title": "CustomLog",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/errorlogformat/",
	"title": "ErrorLogFormat",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/composer/",
	"title": "Composer",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/speedup/",
	"title": "Speedup",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/tips/",
	"title": "Tips",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2016/11/06/composer-speed-up/",
	"title": "컴포저의 속도를 올리는 방법",
	"tags": ["tips", "composer", "speedup"],
	"description": "composer speed up",
	"type": "post",
	"contents": "PHP 의 의존성 관리도구인 컴포저 를 사용할 때, composer install 이나 composer update 시에 속도가 느려서 답답함을 느낄 때가 한두번이 아니다. 구글에서 composer speed up 으로 검색을 해보면 많은 사람들이 동일한 답답함을 느끼고 있다는 것을 확인할 수 있다.\n몇가지 방법을 통해서 조금 더 쾌적하게 사용할 수 있는 방법을 알아보자.\n컴포저의 속도를 개선할 수 있는 방법은 플러그인과, 커맨드 사용 패턴, 옵션 설정 등이 있는데 하나씩 살펴보자.\nPrestissimo Plugin 설치하기 prestissimo는 컴포저의 global 플러그인중 하나로 컴포저의 다양한 의존성 패키지들을 병령로 다운받을 수 있게 해주는 녀석이다.\n내부적으로 curl multi 옵션을 통해서 의존성 패키지들을 다운받도록 해서 속도 향상에 큰 기여를 하는 방식이다. 컴포저를 자주 사용한다면 설치하지 않을 이유가 없다!\n설치방법도 간단하다\n$ composer global require \u0026ldquo;hirak/prestissimo:^0.3\u0026rdquo;\n이렇게 하면 global composer 디렉토리에 설치가 된다.\ngithub에서 제작자는 벤치마크 결과가 288s -\u0026gt; 26s 로 줄어들었다고 말하고 있다.\n–prefer-dist 옵션 사용하기 composer update 또는 composer install 시에 –prepfer-dist 사용하면 패키지에 따라서 조금 더 나은 속도 향상을 기대할 수 있는데, 패키지가 dist 용도로 구성되어 있다면 소스를 일일이 받는 것 보다 빠르기 때문이다.\n좀더 부연 설명을 하자면 패키지를 다운로드 받을 때는 dist 와 source 두가지 방식이 있는데, dist는 안정화 버전, source 버그 픽스를 위한 용도라고 생각하면 된다.\n일반적으로는 -prefer-dist 옵션이 활성화 되어 있다.\nxdebug disable 시키기 php-xdebug 가 활성화 되어 있으면 컴포저의 속도가 느리다. 초기에는 이를 알수가 없었는데 최근에는 컴포저 자체에서 경고를 보여주는 것 같다.\n만약 php 설정이 cli 와 구분되어 있다면 cli 모드에서는 xdebug 를 활성화 시키지 않도록 하자.\ndns lookup 줄이기 속도에 얼마나 영향을 끼칠지는 의문이지만, packagist.com 의 dns lookup 속도는 느리다고 할 수 있다. (ping packagist.com 해보면 알 수 있다.) 따라서, /etc/hosts 에 packagist.com 의 ip를 추가하여 DNS lookup time 을 조금이나마 줄일 수 있다.\n87.98.253.214 packagist.com\n참고 컴포저 한글 매뉴얼 composer manual prestissimo plugin xdebug disable warning "
},
{
	"permalink": "https://findstar.pe.kr/tags/blog/",
	"title": "Blog",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/tags/jekyll/",
	"title": "Jekyll",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
},
{
	"permalink": "https://findstar.pe.kr/2016/11/04/start-new-blog/",
	"title": "블로그 플랫폼 이전 - 글쓰기에 도전하기",
	"tags": ["blog", "developer", "Jekyll"],
	"description": "blog migration from wordpress to jekyll",
	"type": "post",
	"contents": "사실 나는 개발자로 살아오면서 블로그에 공을 들이지는 않아왔었다. 딱히 글을 잘 쓰는 재주도 없었거니와, 무언가를 정리하는게 번거롭고 귀찮다고 생각했기 때문이다. 그동안에 작성한 블로그들은 순간순간 필요했던 정보들을 단순하게 저장해 놓는 용도였고, 그 자체도 정리되었다고 보기가 어려웠다. 지금에 와서 다시금 살펴보니 그다지 건질 내용이 없다는 생각이 드는게 사실이다. 스스로가 글 보다는 현장감과 순간의 뉘앙스가 전달되기 쉬운 말을 선호했고, 글 쓰는 것 자체가 나에게는 많은 에너지를 소모되는 일이었기 때문이다.\n하지만 최근에 와서는 조금씩 생각이 달라지기 시작했는데, 나 스스로가 알게된 사실이나 내용을 기억에만 의존하기에는 무리가 따르기 시작했고, 보다 체계적으로 정리하지 않으면, 효율성이 떨어진다고 느껴졌다. 그래서 이번에 새롭게 마음을 다잡고, 블로그를 새롭게 시작해 보기로 했다. 사실 년초에 블로그를 다시 해볼까 하는 막연한 생각을, 올해가 저물어 가는 11월에 와서야 행동으로 옮기기 시작한 것이다.\n블로그 플랫폼 이전에 있었던 블로그의 최신글은 무려 2014년도 글이다. 그 사이 스스로의 역할이나 업무 방식도 많이 바뀌었고, 사용하는 툴들도 달라졌다보니. 이왕 새롭게 할꺼 마크다운 방식으로 글을 작성해보는게 좋겠다 싶어서 Github Page 기반으로 시작해 보려고 한다. 그렇게 선택한 도구는 Jekyll! 주위의 많은 분들이 사용하고 있기도 했고, 레퍼런스가 많아 선택하게 되었다. 손에 익숙하지 않은 Ruby 기반이라는게 마음에 걸리긴 했지만, 이것보다 더 나은 대안이 없는 상태라서 일단 Jekyll 기반으로 사용하기로 했다.\n이전 포스트 마이그레이션 기존에 개설했던 블로그의 글들을 옮겨올까도 고민해봤지만, 마이그레이션 결과물이 썩 마음에 들지도 않고, 딱히 건질만한 내용도 없다고 생각되어(정리가 안되어 중구난방인 상태) 새롭게 작성하고 기회가 될 때마다 하나씩 다시 글을 작성해 보기로 했다. 처음 작성했던 블로그 글이 2010년 이고, 가장 최근글이 2014년이니. 거의 2년간 방치된 셈이다.\n지금까지 사용했던 플랫폼 SpringNote : 지금은 역사속으로 사라진 서비스 - 에디터는 좀 아깝다는 생각이 들었다. Tistory : 참 좋아하지만, 뭔가 내마음대로 더 꾸미고 싶다는 생각에 옮겨탔다. WordPress : 유용하게 사용해 왔지만, 내 마음에 들지 않았다. "
},
{
	"permalink": "https://findstar.pe.kr/about/",
	"title": "About",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": " 남들 보다 느려도 여유있기를 좋아하는. 즐겁고 재미난 일을 좋아하고, 멘땅에 헤딩은 그만하고 싶지만, 어쩌다 보니 멘땅에 헤딩을 많이하게 되는 케릭터.\n새로운걸 배우고 공부하는걸 좋아하지만, 더디게 나아가는 평범한 개발자. @findstar\nI want to fix the pothole that\u0026rsquo;s right in front of me before I fall in. Linus Torvalds\nSkills Web Java, PHP Spring, Laravel JavaScript, React K8S, AWS Interested ElasticSearch, Kafka Architecture, MSA Devops, Infra Experience Kakao corp : Contents Platform Mesh Korea : LastMile Delivery Business Xpressengine : Open Source Content Management System Seafuture : Maritime Business, Global Weather Tech SMARTON : Car IOT Business "
},
{
	"permalink": "https://findstar.pe.kr/search/",
	"title": "Search Result",
	"tags": [],
	"description": "",
	"type": "base",
	"contents": ""
}]